# GPU 层数设置指南

## 🎮 n_gpu_layers 参数说明

### 什么是 GPU 层数？

大语言模型（LLM）由多个 Transformer 层组成。`n_gpu_layers` 控制有多少层在 GPU 上运行，其余层在 CPU 上运行。

### 三种模式

#### 1. **All (Auto)** - 全部使用 GPU ⭐ 推荐
```
n_gpu_layers = -1
```

**特点**：
- ✅ 所有层都在 GPU 上运行
- ✅ 最快的推理速度
- ✅ 最佳性能
- ⚠️ 需要足够的 VRAM

**适用场景**：
- GPU 显存充足（8GB+）
- 追求最快速度
- 生产环境

**示例**：
- RTX 4090 (24GB) → 可以运行 13B Q8 模型
- RTX 3090 (24GB) → 可以运行 13B Q6 模型
- RTX 4060 Ti (16GB) → 可以运行 7B Q8 模型

#### 2. **CPU Only** - 仅使用 CPU
```
n_gpu_layers = 0
```

**特点**：
- ✅ 不占用 VRAM
- ✅ 适合没有 GPU 的环境
- ❌ 速度最慢（10-50x 慢于 GPU）

**适用场景**：
- 没有 GPU
- GPU 显存不足
- 测试和调试

**性能对比**：
- CPU: ~5-10 tokens/秒
- GPU: ~50-200 tokens/秒

#### 3. **Custom** - 自定义层数 🔧 高级
```
n_gpu_layers = custom_gpu_layers (例如: 35)
```

**特点**：
- ✅ 精确控制 GPU/CPU 分配
- ✅ 可以在显存不足时使用
- ⚠️ 需要了解模型结构

**适用场景**：
- GPU 显存不足以加载全部层
- 需要为其他应用保留 VRAM
- 多模型同时运行

**如何选择层数**：

| 模型大小 | 总层数 | 8GB VRAM | 12GB VRAM | 16GB VRAM | 24GB VRAM |
|---------|--------|----------|-----------|-----------|-----------|
| 3B-4B   | ~32    | 32 (All) | 32 (All)  | 32 (All)  | 32 (All)  |
| 7B-8B   | ~32    | 20-25    | 32 (All)  | 32 (All)  | 32 (All)  |
| 13B     | ~40    | 15-20    | 25-30     | 35-40     | 40 (All)  |
| 30B+    | ~60+   | 10-15    | 15-20     | 20-30     | 40-50     |

## 💡 custom_gpu_layers 的作用

### 什么时候需要自定义？

**场景 1: 显存不足**
```
问题: 加载 13B 模型时 VRAM 不足
解决: 设置 custom_gpu_layers = 30
结果: 30 层在 GPU，10 层在 CPU
```

**场景 2: 多模型运行**
```
问题: 同时运行文本模型和图像模型
解决: 文本模型 custom_gpu_layers = 25
结果: 为图像模型保留 VRAM
```

**场景 3: 性能调优**
```
问题: 想要平衡速度和显存使用
解决: 逐步增加 custom_gpu_layers
结果: 找到最佳平衡点
```

### 如何确定最佳值？

#### 方法 1: 试错法
```
1. 从 All (Auto) 开始
2. 如果 VRAM 不足，改为 Custom
3. 从总层数的 80% 开始（例如 32 层 → 25）
4. 逐步调整直到稳定
```

#### 方法 2: 监控法
```bash
# 监控 GPU 使用
watch -n 1 nvidia-smi

# 查看显存使用
nvidia-smi --query-gpu=memory.used --format=csv
```

#### 方法 3: 计算法
```
估算公式:
每层显存 ≈ 模型大小 / 总层数

例如: 7B Q8 模型 (7.5GB)
- 总层数: 32
- 每层: ~235MB
- 8GB VRAM: 可用 ~6GB → 25 层
```

## 📊 性能对比

### 速度测试（7B 模型）

| 配置 | GPU 层数 | 速度 (tokens/s) | VRAM 使用 |
|------|---------|----------------|----------|
| All (Auto) | 32 | 150 | 7.5GB |
| Custom | 25 | 120 | 6.0GB |
| Custom | 16 | 80 | 4.0GB |
| CPU Only | 0 | 8 | 0GB |

### 质量影响

⚠️ **重要**: GPU 层数**不影响**生成质量！

- ✅ 相同的输出结果
- ✅ 相同的准确性
- ❌ 仅影响速度

## 🎯 推荐配置

### 新手推荐
```
n_gpu_layers: All (Auto)
```
- 简单直接
- 自动优化
- 最佳性能

### 高级用户
```
n_gpu_layers: Custom
custom_gpu_layers: 根据需求调整
```
- 精确控制
- 优化显存使用
- 多任务场景

### 无 GPU 环境
```
n_gpu_layers: CPU Only
```
- 不需要 GPU
- 速度较慢但可用

## 🔧 实际示例

### 示例 1: RTX 4060 Ti (16GB)

**运行 Qwen3-8B Q8 (8.5GB)**
```
✅ 推荐: All (Auto)
   - 全部 32 层在 GPU
   - 速度: ~150 tokens/s
   - VRAM: 8.5GB
```

**运行 Qwen3-14B Q6 (12GB)**
```
✅ 推荐: Custom (35 层)
   - 35/40 层在 GPU
   - 速度: ~100 tokens/s
   - VRAM: 14GB
```

### 示例 2: RTX 3060 (12GB)

**运行 Qwen3-8B Q8 (8.5GB)**
```
✅ 推荐: All (Auto)
   - 速度: ~120 tokens/s
```

**运行 Qwen3-14B Q4 (8GB)**
```
✅ 推荐: All (Auto)
   - 速度: ~90 tokens/s
```

### 示例 3: 无 GPU / CPU Only

**任何模型**
```
✅ 推荐: CPU Only
   - 速度: ~5-10 tokens/s
   - 可以运行任何大小的模型（只要有足够 RAM）
```

## ❓ 常见问题

### Q: custom_gpu_layers 设置多少合适？

A: 
1. 先尝试 All (Auto)
2. 如果 VRAM 不足，设置为总层数的 70-80%
3. 逐步调整直到稳定

### Q: 设置错误会怎样？

A:
- 设置太高 → VRAM 不足，加载失败
- 设置太低 → 速度变慢，但仍可运行

### Q: 如何查看模型有多少层？

A:
```python
# 加载模型时会显示
print(f"Model layers: {model.n_layers}")
```

常见模型层数：
- 3B-4B: ~28-32 层
- 7B-8B: ~32-36 层
- 13B-14B: ~40-48 层
- 30B+: ~60+ 层

### Q: All (Auto) 和 Custom (99) 有区别吗？

A: 
- All (Auto) = -1 = 所有层
- Custom (99) = 最多 99 层（如果模型只有 32 层，则只用 32 层）
- 实际效果相同，但 All (Auto) 更简单

## 📚 总结

| 选项 | 适用场景 | 优点 | 缺点 |
|------|---------|------|------|
| **All (Auto)** | 显存充足 | 最快，简单 | 需要足够 VRAM |
| **CPU Only** | 无 GPU | 不需要 GPU | 非常慢 |
| **Custom** | 显存不足 | 灵活控制 | 需要手动调整 |

**推荐顺序**：
1. 🥇 先尝试 **All (Auto)**
2. 🥈 如果失败，用 **Custom** (70-80% 层数)
3. 🥉 最后才用 **CPU Only**

---

**记住**: GPU 层数只影响速度，不影响质量！ 🚀
