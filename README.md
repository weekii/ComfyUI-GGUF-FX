# ComfyUI-GGUF-FX

Complete GGUF model support for ComfyUI with three inference modes and auto-download capabilities.

## ğŸŒŸ Features

### Three Inference Modes

1. **GGUF Mode** - Local quantized models using llama-cpp-python
2. **Transformers Mode** - Full HuggingFace models
3. **Nexa SDK Mode** - Remote/local GGUF via Nexa SDK service (NEW)

### Nexa SDK Integration (NEW)

- âœ… **Auto-download models** from HuggingFace
- âœ… **Preset model list** with popular models
- âœ… **Dual mode support**: Remote service + Local GGUF files
- âœ… **Configurable API endpoints**
- âœ… **ComfyUI /models/LLM integration**
- âœ… **Thinking mode support** (DeepSeek-R1, Qwen3-Thinking)
- âœ… **Conversation history management**
- âœ… **Multi-endpoint support**

### Other Features

- ğŸ–¼ï¸ **Multi-image analysis** (up to 6 images)
- ğŸ¯ **System prompt presets**
- ğŸ“ **Unified output naming**: all text outputs as `context`
- ğŸ”„ **Automatic model caching**
- âš¡ **Device optimization** (CUDA, MPS, CPU)

## ğŸ“¦ Installation

### 1. Install ComfyUI Custom Node

```bash
cd ComfyUI/custom_nodes
git clone https://github.com/weekii/ComfyUI-GGUF-FX.git
cd ComfyUI-GGUF-FX
pip install -r requirements.txt
```

### 2. Install Nexa SDK (for Nexa mode)

```bash
pip install nexaai
```

### 3. Start Nexa SDK Service

```bash
# Start the service
nexa server

# Or specify a custom port
nexa server --port 11434
```

The service will be available at `http://127.0.0.1:11434`

## ğŸš€ Quick Start

### Using Nexa SDK Mode (Recommended)

#### 1. Basic Text Generation with Auto-Download

```
[Nexa Model Selector]
â”œâ”€ base_url: http://127.0.0.1:11434
â”œâ”€ models_dir: /workspace/ComfyUI/models/LLM
â”œâ”€ model_source: Remote (Nexa Service)
â””â”€ system_prompt: "You are a helpful assistant."
    â†“
[Nexa Text Generation]
â”œâ”€ preset_model: DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-HORROR-Max-GGUF:Q6_K
â”œâ”€ auto_download: True  âœ… Will download if not exists
â”œâ”€ prompt: "Hello, how are you?"
â””â”€ enable_thinking: False
    â†“
Output: context, thinking, raw_response
```

#### 2. Using Custom Model (HuggingFace URL)

```
[Nexa Text Generation]
â”œâ”€ preset_model: Custom (è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹)
â”œâ”€ custom_model: https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Uncensored-Fixed.Q8_0.gguf
â”œâ”€ auto_download: True
â””â”€ prompt: "Explain quantum computing"
```

The node will:
1. Parse the HuggingFace URL
2. Convert to model ID: `mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0`
3. Download the model if not exists
4. Load and run inference

#### 3. Using Local GGUF Files

```bash
# Copy your GGUF file to ComfyUI models directory
cp my-model.gguf /workspace/ComfyUI/models/LLM/
```

```
[Nexa Model Selector]
â”œâ”€ model_source: Local (GGUF File)
    â†“
[Nexa Text Generation]
â”œâ”€ preset_model: Custom (è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹)
â”œâ”€ custom_model: my-model.gguf
â””â”€ auto_download: False
```

### Using GGUF Mode (Local)

```
[Text Model Loader]
â”œâ”€ model: Select from dropdown
â””â”€ device: cuda/cpu/mps
    â†“
[Text Generation Node]
â”œâ”€ prompt: "Your prompt here"
â”œâ”€ max_tokens: 512
â””â”€ temperature: 0.7
    â†“
Output: context, thinking
```

### Using Transformers Mode

```
[Transformers Vision Model Loader]
â”œâ”€ model: Qwen/Qwen2-VL-7B-Instruct
â””â”€ device: cuda
    â†“
[Transformers Vision Generation]
â”œâ”€ image: Connect image
â””â”€ prompt: "Describe this image"
    â†“
Output: context
```

## ğŸ“‹ Available Nodes

### Nexa SDK Nodes

#### ğŸ”· Nexa Model Selector
Configure Nexa SDK service and model source.

**Parameters:**
- `base_url`: Nexa SDK service URL (default: `http://127.0.0.1:11434`)
- `models_dir`: Local models directory (default: `/workspace/ComfyUI/models/LLM`)
- `model_source`: Remote (Nexa Service) or Local (GGUF File)
- `refresh_models`: Refresh model list
- `system_prompt`: System prompt (optional)

**Outputs:**
- `model_config`: Configuration for Text Generation node
- `available_models`: List of available models

#### ğŸ”· Nexa Text Generation
Generate text using Nexa SDK with auto-download support.

**Parameters:**
- `model_config`: From Model Selector
- `preset_model`: Select from preset models or use custom
- `custom_model`: Custom model ID, HuggingFace URL, or local filename
- `auto_download`: Auto-download model if not exists (default: True)
- `prompt`: Input prompt
- `max_tokens`: Maximum tokens to generate (1-8192)
- `temperature`: Temperature (0.0-2.0)
- `top_p`: Top-p sampling (0.0-1.0)
- `top_k`: Top-k sampling (0-100)
- `repetition_penalty`: Repetition penalty (1.0-2.0)
- `enable_thinking`: Enable thinking mode
- `conversation_history`: JSON format conversation history (optional)

**Outputs:**
- `context`: Generated text (final answer)
- `thinking`: Thinking process (if enabled)
- `raw_response`: Raw API response

**Preset Models:**
1. `DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-HORROR-Max-GGUF:Q6_K`
2. `prithivMLmods/Qwen3-4B-2507-abliterated-GGUF:Q8_0`
3. `mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0`
4. `mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF:Q8_0`
5. `Triangle104/Josiefied-Qwen3-4B-abliterated-v2-Q8_0-GGUF`

**Supported Input Formats:**
- Model ID: `user/repo:quantization`
- HuggingFace URL: `https://huggingface.co/user/repo/blob/main/file.gguf`
- Local file: `model.gguf`

#### ğŸ”· Nexa Service Status
Check Nexa SDK service status and list models.

**Parameters:**
- `base_url`: Service URL
- `models_dir`: Local models directory
- `refresh`: Refresh model list

**Outputs:**
- `status`: Service status summary
- `remote_models`: Remote models list
- `local_models`: Local models list

### GGUF Nodes

- **Text Model Loader** - Load GGUF text models
- **Text Generation Node** - Generate text with GGUF models
- **Vision Model Loader (GGUF)** - Load GGUF vision models
- **Vision Description Node** - Generate image descriptions

### Transformers Nodes

- **Transformers Vision Model Loader** - Load HuggingFace vision models
- **Transformers Vision Generation** - Generate with Transformers models
- **Multi-Image Analysis** - Analyze multiple images (up to 6)

### Utility Nodes

- **System Prompt Config** - Configure system prompts with presets

## ğŸ¯ Use Cases

### 1. Auto-Download and Chat

```
[Nexa Model Selector]
â””â”€ system_prompt: "You are a creative writer."
    â†“
[Nexa Text Generation]
â”œâ”€ preset_model: mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF:Q8_0
â”œâ”€ auto_download: True  âœ… Downloads automatically
â”œâ”€ prompt: "Write a short story about AI"
â””â”€ max_tokens: 1024
```

### 2. Thinking Mode (DeepSeek-R1 Style)

```
[Nexa Text Generation]
â”œâ”€ preset_model: mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0
â”œâ”€ enable_thinking: True  âœ… Extracts thinking process
â””â”€ prompt: "Solve: What is 25 * 37?"
    â†“
Output:
â”œâ”€ context: "The answer is 925"
â””â”€ thinking: "<think>Let me calculate... 25 * 37 = 25 * 30 + 25 * 7...</think>"
```

### 3. Conversation with History

```python
# Build conversation history
history = [
    {"role": "user", "content": "What is Python?"},
    {"role": "assistant", "content": "Python is a programming language..."},
    {"role": "user", "content": "What are its main features?"}
]

# In ComfyUI node
[Nexa Text Generation]
â”œâ”€ conversation_history: json.dumps(history)
â””â”€ prompt: "Give me an example"
```

### 4. Multi-Image Analysis

```
[Multi-Image Analysis]
â”œâ”€ image_1: Connect image
â”œâ”€ image_2: Connect image
â”œâ”€ image_3: Connect image
â””â”€ prompt: "Compare these images"
    â†“
Output: Detailed comparison
```

### 5. Custom Endpoint

```
[Nexa Model Selector]
â”œâ”€ base_url: http://192.168.1.100:8080  # Custom server
â”œâ”€ models_dir: /custom/path/to/models
â””â”€ model_source: Remote (Nexa Service)
```

## ğŸ”§ Configuration

### Model Directory

Default: `/workspace/ComfyUI/models/LLM`

All GGUF models (Nexa SDK, GGUF mode) are stored in this directory.

### API Endpoint

Default: `http://127.0.0.1:11434`

Configurable in each Nexa node. Supports:
- Local: `http://127.0.0.1:11434`
- Custom port: `http://localhost:8080`
- Remote: `http://192.168.1.100:11434`

### Auto-Download

When enabled, the system will:
1. Check if model exists in Nexa service
2. If not, download from HuggingFace using `nexa pull`
3. Store in `/models/LLM` directory
4. Load and run inference

**Requirements:**
- Nexa SDK installed: `pip install nexaai`
- Internet connection for first download
- Sufficient disk space

## ğŸ’­ Thinking Mode

Supports automatic extraction of thinking process from models like DeepSeek-R1 and Qwen3-Thinking.

**Supported Tags:**
- `<think>...</think>` (DeepSeek-R1)
- `<thinking>...</thinking>`
- `[THINKING]...[/THINKING]`

**Usage:**
```
[Nexa Text Generation]
â”œâ”€ enable_thinking: True
â””â”€ prompt: "Explain your reasoning"
    â†“
Outputs:
â”œâ”€ context: Final answer (thinking tags removed)
â””â”€ thinking: Extracted thinking process
```

**Disable Thinking:**
Set `enable_thinking: False` or add `no_think` to system prompt.

## ğŸ“Š Model Comparison

| Mode | Pros | Cons | Use Case |
|------|------|------|----------|
| **Nexa SDK** | Auto-download, Remote service, Easy switching | Requires service running | Quick testing, Shared models |
| **GGUF** | Fast, Low memory, Offline | Manual download | Production, Offline use |
| **Transformers** | Full precision, Latest models | High memory, Slow | Research, Best quality |

## ğŸ› Troubleshooting

### Nexa Service Not Available

**Problem:** `âŒ Nexa SDK service is not available`

**Solution:**
1. Check if service is running: `curl http://127.0.0.1:11434/v1/models`
2. Start service: `nexa server`
3. Check firewall settings
4. Verify correct URL in node

### Model Download Failed

**Problem:** `âŒ Download failed`

**Solution:**
1. Check internet connection
2. Verify HuggingFace is accessible
3. Check disk space
4. Install Nexa SDK: `pip install nexaai`
5. Try manual download: `nexa pull user/repo:quantization`

### Local Model Not Found

**Problem:** `âŒ Local model not found`

**Solution:**
1. Check file exists: `ls /workspace/ComfyUI/models/LLM/`
2. Verify filename ends with `.gguf`
3. Check file permissions
4. Use absolute path if needed

### Out of Memory

**Problem:** CUDA out of memory

**Solution:**
1. Use smaller quantization (Q4_0 instead of Q8_0)
2. Reduce `max_tokens`
3. Use CPU mode
4. Close other applications

### Thinking Mode Not Working

**Problem:** Thinking output is empty

**Solution:**
1. Enable `enable_thinking: True`
2. Use a thinking-capable model (e.g., Qwen3-Thinking)
3. Check model output contains thinking tags
4. Remove `no_think` from system prompt

## ğŸ“ Directory Structure

```
ComfyUI-GGUF-FX/
â”œâ”€â”€ __init__.py                     # Node registration
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ node_definitions.py        # Node parameter definitions
â”‚   â””â”€â”€ paths.py                    # Path configuration
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ inference_engine.py        # GGUF inference engine
â”‚   â”œâ”€â”€ model_loader.py            # Model loader
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ nexa_engine.py         # Nexa SDK engine (with auto-download)
â”‚       â””â”€â”€ transformers_engine.py # Transformers engine
â”œâ”€â”€ nodes/
â”‚   â”œâ”€â”€ text_node.py               # GGUF text nodes
â”‚   â”œâ”€â”€ vision_node.py             # GGUF vision nodes
â”‚   â”œâ”€â”€ nexa_text_node.py          # Nexa SDK nodes (with presets)
â”‚   â”œâ”€â”€ vision_node_transformers.py # Transformers vision nodes
â”‚   â”œâ”€â”€ multi_image_node.py        # Multi-image analysis
â”‚   â””â”€â”€ system_prompt_node.py      # System prompt config
â””â”€â”€ utils/
    â”œâ”€â”€ downloader.py              # Model downloader
    â”œâ”€â”€ device_optimizer.py        # Device optimization
    â””â”€â”€ system_prompts.py          # System prompt presets
```

## ğŸ”„ Updates

### v2.1 (Latest)
- âœ… **Auto-download support** for Nexa SDK models
- âœ… **Preset model list** with 5 popular models
- âœ… **HuggingFace URL parsing** - paste URLs directly
- âœ… **Custom model input** - support model ID, URL, or filename
- âœ… **Auto-download toggle** - enable/disable per request

### v2.0
- âœ… Nexa SDK integration
- âœ… ComfyUI /models/LLM directory integration
- âœ… Configurable API endpoints
- âœ… Unified output naming (context)
- âœ… Dual mode support (Remote + Local)

### v1.0
- âœ… GGUF mode
- âœ… Transformers mode
- âœ… Multi-image analysis
- âœ… System prompt presets

## ğŸ“ Requirements

```txt
llama-cpp-python>=0.2.0
transformers>=4.30.0
torch>=2.0.0
Pillow>=9.0.0
requests>=2.25.0
nexaai  # For Nexa SDK mode
```

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ”— Links

- **GitHub**: https://github.com/weekii/ComfyUI-GGUF-FX
- **Nexa SDK**: https://github.com/NexaAI/nexa-sdk
- **ComfyUI**: https://github.com/comfyanonymous/ComfyUI

## ğŸ‘¤ Author

**weekii** <weekii2024@gmail.com>

## ğŸ™ Acknowledgments

- ComfyUI team for the amazing framework
- Nexa AI for the SDK
- HuggingFace for model hosting
- llama.cpp team for GGUF support

---

**Status**: âœ… Production Ready  
**Version**: 2.1  
**Last Updated**: 2025-10-29
