# ComfyUI-GGUF-FX

Complete GGUF model support for ComfyUI with local and Nexa SDK inference modes.

## ğŸŒŸ Features

### Two Inference Modes

1. **Text Generation (Local)** - Direct GGUF model loading using llama-cpp-python
2. **Nexa SDK** - Managed models via Nexa SDK service

### Key Features

- âœ… **Simple configuration** - Minimal parameters, maximum functionality
- âœ… **Auto model detection** - Nexa SDK automatically lists downloaded models
- âœ… **Thinking mode support** (DeepSeek-R1, Qwen3-Thinking)
- âœ… **Stop sequences** - Prevent over-generation
- âœ… **Paragraph merging** - Clean single-paragraph output
- âœ… **Detailed logging** - Debug-friendly console output
- âœ… **Device optimization** (CUDA, MPS, CPU)

## ğŸ“¦ Installation

### 1. Install ComfyUI Custom Node

```bash
cd ComfyUI/custom_nodes
git clone https://github.com/weekii/ComfyUI-GGUF-FX.git
cd ComfyUI-GGUF-FX
pip install -r requirements.txt
```

### 2. For Nexa SDK Mode (Optional)

```bash
# Install Nexa SDK
pip install nexaai

# Start Nexa service
nexa serve
```

The service will be available at `http://127.0.0.1:11434`

## ğŸš€ Quick Start

### Using Text Generation (Local GGUF)

**Recommended for local GGUF files**

```
[Text Model Loader]
â”œâ”€ model: Select your GGUF file
â””â”€ device: cuda/cpu/mps
    â†“
[Text Generation]
â”œâ”€ max_tokens: 256  â† Recommended for single paragraph
â”œâ”€ temperature: 0.7
â”œâ”€ top_p: 0.8
â”œâ”€ top_k: 40
â”œâ”€ repetition_penalty: 1.1
â”œâ”€ enable_thinking: False
â””â”€ prompt: "Your prompt here"
    â†“
Output: context, thinking
```

**Features:**
- âœ… Direct file access
- âœ… No service required
- âœ… Fast and simple
- âœ… Stop sequences prevent over-generation
- âœ… Automatic paragraph merging

### Using Nexa SDK Mode

**Recommended for Nexa SDK ecosystem**

#### Step 1: Download Model

```bash
# Download a model using Nexa CLI
nexa pull mradermacher/Huihui-Qwen3-4B-Instruct-2507-abliterated-GGUF:Q8_0 --model-type llm

# Check downloaded models
nexa list
```

#### Step 2: Use in ComfyUI

```
[Nexa Model Selector]
â”œâ”€ base_url: http://127.0.0.1:11434
â”œâ”€ refresh_models: â˜
â””â”€ system_prompt: (optional)
    â†“
[Nexa SDK Text Generation]
â”œâ”€ preset_model: Select from dropdown (auto-populated)
â”œâ”€ max_tokens: 256
â”œâ”€ temperature: 0.7
â””â”€ prompt: "Your prompt here"
    â†“
Output: context, thinking
```

**Features:**
- âœ… Centralized model management
- âœ… Auto-populated model list
- âœ… Supports `nexa pull` workflow

## ğŸ“‹ Available Nodes

### Text Generation Nodes (Local GGUF)

#### ğŸ”· Text Model Loader
Load GGUF models from `/workspace/ComfyUI/models/LLM/GGUF/`

**Parameters:**
- `model`: Select from available GGUF files
- `device`: cuda/cpu/mps
- `n_ctx`: Context window (default: 8192)
- `n_gpu_layers`: GPU layers (-1 for all)

**Output:**
- `model`: Model configuration

#### ğŸ”· Text Generation
Generate text with loaded GGUF model

**Parameters:**
- `model`: From Text Model Loader
- `max_tokens`: Maximum tokens (1-8192, **recommended: 256**)
- `temperature`: Temperature (0.0-2.0)
- `top_p`: Top-p sampling (0.0-1.0)
- `top_k`: Top-k sampling (0-100)
- `repetition_penalty`: Repetition penalty (1.0-2.0)
- `enable_thinking`: Enable thinking mode
- `prompt`: Input prompt (**at bottom for easy editing**)

**Outputs:**
- `context`: Generated text
- `thinking`: Thinking process (if enabled)

**Features:**
- âœ… Stop sequences: `["User:", "System:", "\n\n\n", "\n\n##", "\n\nNote:", "\n\nThis "]`
- âœ… Automatic paragraph merging for single-paragraph prompts
- âœ… Detailed console logging

### Nexa SDK Nodes

#### ğŸ”· Nexa Model Selector
Configure Nexa SDK service

**Parameters:**
- `base_url`: Service URL (default: `http://127.0.0.1:11434`)
- `refresh_models`: Refresh model list
- `system_prompt`: System prompt (optional)

**Output:**
- `model_config`: Configuration for Text Generation

#### ğŸ”· Nexa SDK Text Generation
Generate text using Nexa SDK

**Parameters:**
- `model_config`: From Model Selector
- `preset_model`: Select from dropdown (auto-populated from `nexa list`)
- `custom_model`: Custom model ID (format: `author/model:quant`)
- `auto_download`: Auto-download if missing
- `max_tokens`: Maximum tokens (**recommended: 256**)
- `temperature`, `top_p`, `top_k`, `repetition_penalty`: Generation parameters
- `enable_thinking`: Enable thinking mode
- `prompt`: Input prompt (**at bottom**)

**Outputs:**
- `context`: Generated text
- `thinking`: Thinking process (if enabled)

**Preset Models:**
- `DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-HORROR-Max-GGUF:Q6_K`
- `mradermacher/Huihui-Qwen3-4B-Instruct-2507-abliterated-GGUF:Q8_0`
- `prithivMLmods/Qwen3-4B-2507-abliterated-GGUF:Q8_0`

#### ğŸ”· Nexa Service Status
Check Nexa SDK service status

**Parameters:**
- `base_url`: Service URL
- `refresh`: Refresh model list

**Output:**
- `status`: Service status and model list

## ğŸ¯ Best Practices

### For Single-Paragraph Output

**System Prompt:**
```
You are an expert prompt generator. Output ONLY in English.

**CRITICAL: Output EXACTLY ONE continuous paragraph. Maximum 400 words.**
```

**Parameters:**
```
max_tokens: 256  â† Key setting!
temperature: 0.7
top_p: 0.8
top_k: 20
```

**Why max_tokens=256?**
- âœ… Prevents over-generation
- âœ… Model completes task without extra commentary
- âœ… Reduces from ~2700 chars (11 paragraphs) to ~1300 chars (1 paragraph)

### For Multi-Turn Conversations

Include history directly in prompt:
```
User: Hello
Assistant: Hi! How can I help?
User: Tell me a joke
```

No need for separate conversation history parameter.

## ğŸ’­ Thinking Mode

Automatically extracts thinking process from models like DeepSeek-R1 and Qwen3-Thinking.

**Supported Tags:**
- `<think>...</think>` (DeepSeek-R1, Qwen3)
- `<thinking>...</thinking>`
- `[THINKING]...[/THINKING]`

**Usage:**
```
[Text Generation]
â”œâ”€ enable_thinking: True
â””â”€ prompt: "Explain your reasoning"
    â†“
Outputs:
â”œâ”€ context: Final answer (thinking tags removed)
â””â”€ thinking: Extracted thinking process
```

**Disable Thinking:**
- Set `enable_thinking: False`
- Or add `no_think` to system prompt

## ğŸ“Š Mode Comparison

| Feature | Text Generation (Local) | Nexa SDK |
|---------|------------------------|----------|
| **Setup** | Copy GGUF file | `nexa pull` |
| **Service** | Not required | Requires `nexa serve` |
| **Model Management** | Manual | CLI (`nexa list`, `nexa pull`) |
| **Use Case** | Local files, production | Nexa ecosystem, shared models |
| **Speed** | Fast | Fast (via service) |
| **Flexibility** | Any GGUF file | Only `nexa pull` models |

**Recommendation:**
- Use **Text Generation** for local GGUF files
- Use **Nexa SDK** if you're already using Nexa ecosystem

## ğŸ› Troubleshooting

### Output Too Long (Multiple Paragraphs)

**Problem:** Model generates 11 paragraphs instead of 1

**Solution:**
1. **Reduce max_tokens** from 512 to 256
2. **Strengthen system prompt**: Add "EXACTLY ONE paragraph"
3. Stop sequences are already configured

### Nexa Service Not Available

**Problem:** `âŒ Nexa SDK service is not available`

**Solution:**
1. Start service: `nexa serve`
2. Check: `curl http://127.0.0.1:11434/v1/models`
3. Verify URL in node

### Model Not in Dropdown

**Problem:** Downloaded model doesn't appear in Nexa SDK dropdown

**Solution:**
1. Check: `nexa list`
2. Click "refresh_models" in Nexa Model Selector
3. Restart ComfyUI

### 0B Entries in `nexa list`

**Problem:** `nexa list` shows 0B entries

**Solution:**
```bash
# Clean up invalid entries
rm -rf ~/.cache/nexa.ai/nexa_sdk/models/local
rm -rf ~/.cache/nexa.ai/nexa_sdk/models/workspace
find ~/.cache/nexa.ai/nexa_sdk/models -name "*.lock" -delete

# Verify
nexa list
```

## ğŸ“ Directory Structure

```
ComfyUI-GGUF-FX/
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ NEXA_SDK_GUIDE.md              # Detailed Nexa SDK guide
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ __init__.py                     # Node registration
â”œâ”€â”€ config/
â”‚   â””â”€â”€ paths.py                    # Path configuration
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ inference_engine.py        # GGUF inference engine
â”‚   â”œâ”€â”€ model_loader.py            # Model loader
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ nexa_engine.py         # Nexa SDK engine
â”‚       â””â”€â”€ transformers_engine.py # Transformers engine
â”œâ”€â”€ nodes/
â”‚   â”œâ”€â”€ text_node.py               # Text Generation nodes
â”‚   â”œâ”€â”€ nexa_text_node.py          # Nexa SDK nodes
â”‚   â”œâ”€â”€ vision_node.py             # Vision nodes
â”‚   â””â”€â”€ system_prompt_node.py      # System prompt config
â””â”€â”€ utils/
    â”œâ”€â”€ device_optimizer.py        # Device optimization
    â””â”€â”€ system_prompts.py          # System prompt presets
```

## ğŸ”„ Recent Updates

### v2.2 (2025-10-29)
- âœ… **Simplified Nexa Model Selector** - Removed unused `models_dir` and `model_source`
- âœ… **Removed unused outputs** - Cleaner node interface
- âœ… **Moved prompt to bottom** - Better UX for long prompts
- âœ… **Removed conversation_history** - Use prompt directly
- âœ… **Stop sequences** - Prevent over-generation
- âœ… **Paragraph merging** - Clean single-paragraph output
- âœ… **Dynamic model list** - Auto-populated from Nexa SDK API
- âœ… **Detailed logging** - Debug-friendly console output

### v2.1
- âœ… Nexa SDK integration
- âœ… Preset model list
- âœ… Thinking mode support

### v2.0
- âœ… GGUF mode with llama-cpp-python
- âœ… ComfyUI /models/LLM integration

## ğŸ“ Requirements

```txt
llama-cpp-python>=0.2.0
transformers>=4.30.0
torch>=2.0.0
Pillow>=9.0.0
requests>=2.25.0
nexaai  # Optional, for Nexa SDK mode
```

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ”— Links

- **GitHub**: https://github.com/weekii/ComfyUI-GGUF-FX
- **Nexa SDK**: https://github.com/NexaAI/nexa-sdk
- **Nexa SDK Guide**: [NEXA_SDK_GUIDE.md](NEXA_SDK_GUIDE.md)
- **ComfyUI**: https://github.com/comfyanonymous/ComfyUI

## ğŸ‘¤ Author

**weekii** <weekii2024@gmail.com>

## ğŸ™ Acknowledgments

- ComfyUI team for the amazing framework
- Nexa AI for the SDK
- HuggingFace for model hosting
- llama.cpp team for GGUF support

---

**Status**: âœ… Production Ready  
**Version**: 2.2  
**Last Updated**: 2025-10-29
