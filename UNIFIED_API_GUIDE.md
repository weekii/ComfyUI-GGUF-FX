# ç»Ÿä¸€ API æ–‡æœ¬ç”ŸæˆæŒ‡å—

## æ¦‚è¿°

ç»Ÿä¸€æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿæ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
1. **Local (GGUF)** - ç›´æ¥åŠ è½½æœ¬åœ° GGUF æ–‡ä»¶
2. **Ollama API** - é€šè¿‡ Ollama æœåŠ¡è°ƒç”¨æ¨¡å‹
3. **Nexa SDK API** - é€šè¿‡ Nexa SDK æœåŠ¡è°ƒç”¨æ¨¡å‹

## èŠ‚ç‚¹

### ğŸ”· Unified Text Model Selector

ç»Ÿä¸€çš„æ¨¡å‹é€‰æ‹©å™¨ï¼Œæ”¯æŒæœ¬åœ°å’Œè¿œç¨‹æ¨¡å¼ã€‚

**å‚æ•°**ï¼š

**Mode**: `Local (GGUF)` / `Remote (API)`

**Local æ¨¡å¼å‚æ•°**ï¼š
- `local_model`: é€‰æ‹© GGUF æ–‡ä»¶
- `n_ctx`: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé»˜è®¤: 8192ï¼‰
- `n_gpu_layers`: GPU å±‚æ•°ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨ï¼‰

**Remote æ¨¡å¼å‚æ•°**ï¼š
- `base_url`: API æœåŠ¡åœ°å€
- `api_type`: `Ollama` / `Nexa SDK` / `OpenAI Compatible`
- `remote_model`: æ¨¡å‹åç§°ï¼ˆç•™ç©ºè‡ªåŠ¨è·å–ç¬¬ä¸€ä¸ªï¼‰
- `refresh_models`: åˆ·æ–°æ¨¡å‹åˆ—è¡¨

**é€šç”¨å‚æ•°**ï¼š
- `system_prompt`: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰

**è¾“å‡º**ï¼š
- `model_config`: æ¨¡å‹é…ç½®ï¼ˆä¼ ç»™ Text Generation èŠ‚ç‚¹ï¼‰

### ğŸ”· Unified Text Generation

ç»Ÿä¸€çš„æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹ï¼Œè‡ªåŠ¨é€‚é…æœ¬åœ°/è¿œç¨‹æ¨¡å¼ã€‚

**å‚æ•°**ï¼š
- `model_config`: ä» Model Selector
- `max_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆæ¨è: 256ï¼‰
- `temperature`: æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰
- `top_p`: Top-p é‡‡æ ·ï¼ˆ0.0-1.0ï¼‰
- `top_k`: Top-k é‡‡æ ·ï¼ˆ0-100ï¼‰
- `repetition_penalty`: é‡å¤æƒ©ç½šï¼ˆ1.0-2.0ï¼‰
- `enable_thinking`: å¯ç”¨æ€è€ƒæ¨¡å¼
- `prompt`: è¾“å…¥æç¤ºè¯

**è¾“å‡º**ï¼š
- `context`: ç”Ÿæˆçš„æ–‡æœ¬
- `thinking`: æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æœ¬åœ° GGUF æ–‡ä»¶

**é€‚ç”¨äº**ï¼š
- å·²æœ‰ GGUF æ–‡ä»¶
- ä¸éœ€è¦é¢å¤–æœåŠ¡
- å¿«é€Ÿæµ‹è¯•

**é…ç½®**ï¼š
```
[Unified Text Model Selector]
â”œâ”€ mode: Local (GGUF)
â”œâ”€ local_model: Huihui-Qwen3-4B-Instruct-2507-abliterated.Q8_0.gguf
â”œâ”€ n_ctx: 8192
â””â”€ n_gpu_layers: -1
    â†“
[Unified Text Generation]
â””â”€ prompt: "Hello"
```

### åœºæ™¯ 2: Ollama API

**é€‚ç”¨äº**ï¼š
- ä½¿ç”¨ Ollama ç”Ÿæ€
- éœ€è¦ Ollama çš„æ¨¡å‹ç®¡ç†
- å¤šä¸ªåº”ç”¨å…±äº«æ¨¡å‹

**å‡†å¤‡å·¥ä½œ**ï¼š

1. å®‰è£… Ollamaï¼š
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

2. åˆ›å»º Modelfileï¼š
```bash
cat > Modelfile << 'EOF'
FROM /workspace/ComfyUI/models/LLM/GGUF/your-model.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
EOF
```

3. åˆ›å»ºæ¨¡å‹ï¼š
```bash
ollama create my-model -f Modelfile
```

4. å¯åŠ¨æœåŠ¡ï¼š
```bash
ollama serve
# æˆ–ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£
OLLAMA_HOST=127.0.0.1:11435 ollama serve
```

**é…ç½®**ï¼š
```
[Unified Text Model Selector]
â”œâ”€ mode: Remote (API)
â”œâ”€ base_url: http://127.0.0.1:11434
â”œâ”€ api_type: Ollama
â””â”€ remote_model: my-model:latest
    â†“
[Unified Text Generation]
â””â”€ prompt: "Hello"
```

### åœºæ™¯ 3: Nexa SDK API

**é€‚ç”¨äº**ï¼š
- ä½¿ç”¨ Nexa SDK ç”Ÿæ€
- éœ€è¦ Nexa çš„æ¨¡å‹ç®¡ç†
- æ ‡å‡†åŒ–çš„æ¨¡å‹æ ¼å¼

**å‡†å¤‡å·¥ä½œ**ï¼š

1. å®‰è£… Nexa SDKï¼š
```bash
pip install nexaai
```

2. ä¸‹è½½æ¨¡å‹ï¼š
```bash
nexa pull mradermacher/Huihui-Qwen3-4B-Instruct-2507-abliterated-GGUF:Q8_0 --model-type llm
```

3. å¯åŠ¨æœåŠ¡ï¼š
```bash
nexa serve
```

**é…ç½®**ï¼š
```
[Unified Text Model Selector]
â”œâ”€ mode: Remote (API)
â”œâ”€ base_url: http://127.0.0.1:11434
â”œâ”€ api_type: Nexa SDK
â””â”€ remote_model: (è‡ªåŠ¨è·å–)
    â†“
[Unified Text Generation]
â””â”€ prompt: "Hello"
```

## API å…¼å®¹æ€§

### OpenAI å…¼å®¹æ ¼å¼

Ollama å’Œ Nexa SDK éƒ½ä½¿ç”¨ OpenAI å…¼å®¹çš„ API æ ¼å¼ï¼š

**ç«¯ç‚¹**ï¼š
- æ¨¡å‹åˆ—è¡¨ï¼š`GET /v1/models`
- èŠå¤©è¡¥å…¨ï¼š`POST /v1/chat/completions`

**è¯·æ±‚æ ¼å¼**ï¼š
```json
{
  "model": "model-name",
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}
```

**å“åº”æ ¼å¼**ï¼š
```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "Hi there!"
      }
    }
  ]
}
```

### å‚æ•°å·®å¼‚

| å‚æ•° | Ollama | Nexa SDK / OpenAI |
|------|--------|-------------------|
| é‡å¤æƒ©ç½š | `repeat_penalty` | `repetition_penalty` |

ç»Ÿä¸€ API å¼•æ“ä¼šè‡ªåŠ¨è½¬æ¢è¿™äº›å‚æ•°ã€‚

## å¯¹æ¯”

| ç‰¹æ€§ | Local GGUF | Ollama | Nexa SDK |
|------|-----------|--------|----------|
| **æœåŠ¡ä¾èµ–** | æ—  | Ollama æœåŠ¡ | Nexa æœåŠ¡ |
| **æ¨¡å‹ç®¡ç†** | æ‰‹åŠ¨ | `ollama` CLI | `nexa` CLI |
| **æ¨¡å‹æ ¼å¼** | ä»»æ„ GGUF | Modelfile | author/model:quant |
| **å¯åŠ¨é€Ÿåº¦** | å¿« | ä¸­ | ä¸­ |
| **é€‚ç”¨åœºæ™¯** | æœ¬åœ°æ–‡ä»¶ | Ollama ç”Ÿæ€ | Nexa ç”Ÿæ€ |

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: æœåŠ¡ä¸å¯ç”¨

**ç—‡çŠ¶**ï¼š`âš ï¸ service is not available`

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥æœåŠ¡
curl http://127.0.0.1:11434/v1/models

# Ollama
ollama serve

# Nexa SDK
nexa serve
```

### é—®é¢˜ 2: æ¨¡å‹åˆ—è¡¨ä¸ºç©º

**ç—‡çŠ¶**ï¼š`âš ï¸ No models found`

**Ollama è§£å†³**ï¼š
```bash
# åˆ›å»ºæ¨¡å‹
ollama create model-name -f Modelfile

# æŸ¥çœ‹æ¨¡å‹
ollama list
```

**Nexa SDK è§£å†³**ï¼š
```bash
# ä¸‹è½½æ¨¡å‹
nexa pull model-name --model-type llm

# æŸ¥çœ‹æ¨¡å‹
nexa list
```

### é—®é¢˜ 3: ç«¯å£å†²çª

**ç—‡çŠ¶**ï¼š`bind: address already in use`

**è§£å†³**ï¼š
```bash
# ä½¿ç”¨ä¸åŒç«¯å£
OLLAMA_HOST=127.0.0.1:11435 ollama serve

# åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨å¯¹åº”ç«¯å£
base_url: http://127.0.0.1:11435
```

## æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
# API å¯¹æ¯”æµ‹è¯•
python3 tests/test_ollama_nexa_comparison.py

# èŠ‚ç‚¹é›†æˆæµ‹è¯•
python3 tests/test_unified_text_node.py
```

## æœ€ä½³å®è·µ

1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨ Local GGUF æ¨¡å¼ï¼Œå¿«é€Ÿè¿­ä»£
2. **ç”Ÿäº§ç¯å¢ƒ**ï¼šä½¿ç”¨ Ollama/Nexa SDKï¼Œç»Ÿä¸€ç®¡ç†
3. **å¤šç”¨æˆ·**ï¼šä½¿ç”¨ API æ¨¡å¼ï¼Œå…±äº«æ¨¡å‹èµ„æº
4. **ç¦»çº¿ä½¿ç”¨**ï¼šä½¿ç”¨ Local GGUF æ¨¡å¼

## ç¤ºä¾‹å·¥ä½œæµ

### ç®€å•å¯¹è¯

```
[Unified Text Model Selector]
â”œâ”€ mode: Remote (API)
â”œâ”€ api_type: Ollama
â””â”€ system_prompt: "You are a helpful assistant."
    â†“
[Unified Text Generation]
â”œâ”€ prompt: "Hello, how are you?"
â””â”€ max_tokens: 100
    â†“
Output: "I'm doing well, thank you! How can I help you today?"
```

### æ€è€ƒæ¨¡å¼

```
[Unified Text Model Selector]
â”œâ”€ mode: Local (GGUF)
â””â”€ local_model: Qwen3-Thinking-model.gguf
    â†“
[Unified Text Generation]
â”œâ”€ prompt: "Solve: 25 * 37 = ?"
â”œâ”€ enable_thinking: True
â””â”€ max_tokens: 256
    â†“
Outputs:
â”œâ”€ context: "The answer is 925"
â””â”€ thinking: "<think>Let me calculate... 25 * 37 = 25 * 30 + 25 * 7...</think>"
```

## æŠ€æœ¯ç»†èŠ‚

### UnifiedAPIEngine

æ ¸å¿ƒ API å¼•æ“ï¼Œä½äº `core/inference/unified_api_engine.py`ã€‚

**ç‰¹æ€§**ï¼š
- è‡ªåŠ¨æ£€æµ‹ API ç±»å‹
- å‚æ•°è‡ªåŠ¨è½¬æ¢
- é”™è¯¯å¤„ç†å’Œé‡è¯•
- è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—

**ä½¿ç”¨**ï¼š
```python
from core.inference.unified_api_engine import get_unified_api_engine

engine = get_unified_api_engine("http://127.0.0.1:11434", "ollama")
response = engine.chat_completion(
    model="model-name",
    messages=[{"role": "user", "content": "Hello"}],
    temperature=0.7,
    max_tokens=512
)
```

## ç›¸å…³æ–‡æ¡£

- [README.md](README.md) - ä¸»è¦æ–‡æ¡£
- [NEXA_SDK_GUIDE.md](NEXA_SDK_GUIDE.md) - Nexa SDK è¯¦ç»†æŒ‡å—
- [tests/](tests/) - æµ‹è¯•è„šæœ¬

---

**ç‰ˆæœ¬**: 2.3  
**æ›´æ–°æ—¥æœŸ**: 2025-10-29
