"""
Nexa SDK Inference Engine - ä½¿ç”¨ Nexa SDK æœåŠ¡è¿›è¡Œæ¨ç†
é€šè¿‡ HTTP API è°ƒç”¨æœ¬åœ° Nexa SDK æœåŠ¡
æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„ç®¡ç†ã€è‡ªåŠ¨ä¸‹è½½å’Œä¸ ComfyUI çš„ /models/LLM ç›®å½•é›†æˆ
"""

import requests
import os
import time
from typing import Dict, List, Optional, Any


class NexaInferenceEngine:
    """Nexa SDK æ¨ç†å¼•æ“ï¼ˆé€šè¿‡ HTTP APIï¼‰"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:11434", models_dir: Optional[str] = None):
        """
        åˆå§‹åŒ– Nexa æ¨ç†å¼•æ“
        
        Args:
            base_url: Nexa SDK æœåŠ¡çš„åŸºç¡€ URL
            models_dir: æœ¬åœ°æ¨¡å‹ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ ComfyUI çš„ /models/LLMï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.models_endpoint = f"{self.base_url}/v1/models"
        self.chat_endpoint = f"{self.base_url}/v1/chat/completions"
        self.completions_endpoint = f"{self.base_url}/v1/completions"
        
        # æ¨¡å‹ç›®å½•é…ç½®
        self.models_dir = models_dir
        
        # ç¼“å­˜å¯ç”¨æ¨¡å‹åˆ—è¡¨
        self._available_models = None
    
    def set_models_dir(self, models_dir: str):
        """
        è®¾ç½®æ¨¡å‹ç›®å½•
        
        Args:
            models_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        """
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)
        print(f"ğŸ“ Nexa SDK models directory set to: {models_dir}")
    
    def get_models_dir(self) -> Optional[str]:
        """
        è·å–å½“å‰é…ç½®çš„æ¨¡å‹ç›®å½•
        
        Returns:
            æ¨¡å‹ç›®å½•è·¯å¾„
        """
        return self.models_dir
    
    def get_local_models(self) -> List[str]:
        """
        è·å–æœ¬åœ°å·²ä¸‹è½½çš„ GGUF æ¨¡å‹åˆ—è¡¨
        
        Returns:
            æœ¬åœ°æ¨¡å‹æ–‡ä»¶åˆ—è¡¨
        """
        if not self.models_dir or not os.path.exists(self.models_dir):
            return []
        
        models = []
        for item in os.listdir(self.models_dir):
            if item.endswith('.gguf'):
                models.append(item)
        
        return sorted(models)
    
    def get_model_path(self, model_name: str) -> str:
        """
        è·å–æ¨¡å‹çš„å®Œæ•´è·¯å¾„
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–æ–‡ä»¶å
        
        Returns:
            æ¨¡å‹çš„å®Œæ•´è·¯å¾„
        """
        if not self.models_dir:
            raise ValueError("Models directory not set. Call set_models_dir() first.")
        
        # å¦‚æœå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œç›´æ¥è¿”å›
        if os.path.isabs(model_name):
            return model_name
        
        # å¦åˆ™æ‹¼æ¥åˆ°æ¨¡å‹ç›®å½•
        return os.path.join(self.models_dir, os.path.basename(model_name))
    
    def wait_for_model_ready(self, model_id: str, max_wait: int = 30, check_interval: int = 2) -> bool:
        """
        ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆ
        
        Args:
            model_id: æ¨¡å‹ ID
            max_wait: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        
        Returns:
            æ¨¡å‹æ˜¯å¦å°±ç»ª
        """
        print(f"â³ Waiting for model to be ready...")
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            # åˆ·æ–°æ¨¡å‹åˆ—è¡¨
            models = self.get_available_models(force_refresh=True)
            
            if model_id in models:
                print(f"âœ… Model is ready!")
                return True
            
            time.sleep(check_interval)
            elapsed = int(time.time() - start_time)
            print(f"   Still waiting... ({elapsed}s)")
        
        print(f"âš ï¸  Timeout waiting for model")
        return False
    
    def load_model_via_api(self, model_id: str) -> bool:
        """
        é€šè¿‡ Nexa SDK API åŠ è½½æ¨¡å‹ï¼ˆæœåŠ¡ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
        
        Args:
            model_id: æ¨¡å‹ ID
        
        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            print(f"ğŸ“¥ Loading model via Nexa SDK service: {model_id}")
            
            # å°è¯•ç›´æ¥è°ƒç”¨ chat APIï¼ŒæœåŠ¡ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
            # å‘é€ä¸€ä¸ªç©ºè¯·æ±‚æ¥è§¦å‘æ¨¡å‹åŠ è½½
            payload = {
                "model": model_id,
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1,
            }
            
            response = requests.post(
                self.chat_endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œç»™ä¸‹è½½æ—¶é—´
            )
            
            # å¦‚æœè¿”å› 200ï¼Œæ¨¡å‹å·²åŠ è½½
            if response.status_code == 200:
                print(f"âœ… Model loaded successfully")
                self._available_models = None  # åˆ·æ–°ç¼“å­˜
                return True
            
            # å¦‚æœè¿”å› 400ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œç­‰å¾…ä¸€ä¸‹
            elif response.status_code == 400:
                print(f"â³ Model is loading, waiting for it to be ready...")
                # ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆ
                if self.wait_for_model_ready(model_id, max_wait=60):
                    return True
                else:
                    print(f"âš ï¸  Model loading timeout")
                    return False
            else:
                print(f"âš ï¸  Model load returned status {response.status_code}")
                return False
                
        except requests.exceptions.Timeout:
            print(f"âš ï¸  Model loading timeout (may still be downloading in background)")
            # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿå°è¯•ç­‰å¾…æ¨¡å‹å‡ºç°åœ¨åˆ—è¡¨ä¸­
            return self.wait_for_model_ready(model_id, max_wait=30)
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            return False
    
    def ensure_model_available(self, model_id: str, auto_download: bool = True) -> bool:
        """
        ç¡®ä¿æ¨¡å‹å¯ç”¨ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™é€šè¿‡æœåŠ¡åŠ è½½ï¼‰
        
        Args:
            model_id: æ¨¡å‹ ID
            auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½
        
        Returns:
            æ¨¡å‹æ˜¯å¦å¯ç”¨
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶
        if model_id.endswith('.gguf'):
            model_path = self.get_model_path(model_id)
            if os.path.exists(model_path):
                print(f"âœ… Local model found: {model_path}")
                return True
            
            if auto_download:
                print(f"âš ï¸  Local model not found, will try to use remote")
                return False
            
            return False
        
        # æ£€æŸ¥è¿œç¨‹æœåŠ¡ä¸­æ˜¯å¦æœ‰è¯¥æ¨¡å‹
        available_models = self.get_available_models(force_refresh=True)
        if model_id in available_models:
            print(f"âœ… Model available in Nexa service: {model_id}")
            return True
        
        # å¦‚æœä¸åœ¨æœåŠ¡ä¸­ï¼Œå°è¯•é€šè¿‡ API åŠ è½½ï¼ˆæœåŠ¡ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
        if auto_download:
            print(f"ğŸ“¥ Model not in service, attempting to load...")
            return self.load_model_via_api(model_id)
        
        return False
    
    def get_available_models(self, force_refresh: bool = False) -> List[str]:
        """
        è·å– Nexa SDK æœåŠ¡ä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Returns:
            æ¨¡å‹ ID åˆ—è¡¨
        """
        if self._available_models is None or force_refresh:
            try:
                response = requests.get(self.models_endpoint, timeout=5)
                response.raise_for_status()
                data = response.json()
                
                # æå–æ¨¡å‹ ID
                self._available_models = [model['id'] for model in data.get('data', [])]
                if not force_refresh:  # åªåœ¨éå¼ºåˆ¶åˆ·æ–°æ—¶æ‰“å°
                    print(f"âœ… Found {len(self._available_models)} models in Nexa SDK service")
                
            except Exception as e:
                if not force_refresh:
                    print(f"âŒ Failed to fetch models from Nexa SDK: {e}")
                self._available_models = []
        
        return self._available_models
    
    def is_service_available(self) -> bool:
        """
        æ£€æŸ¥ Nexa SDK æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(self.models_endpoint, timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        stream: bool = False,
        auto_download: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨ API
        
        Args:
            model: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            stream: æ˜¯å¦æµå¼è¾“å‡º
            auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            API å“åº”
        """
        # å¤„ç†æ¨¡å‹è·¯å¾„
        original_model = model
        
        # å¦‚æœæ˜¯æœ¬åœ° GGUF æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if model.endswith('.gguf'):
            if os.path.isabs(model):
                # ç»å¯¹è·¯å¾„ï¼šNexa SDK ä¸æ”¯æŒï¼Œéœ€è¦æå–æ¨¡å‹å
                model_filename = os.path.basename(model)
                print(f"âš ï¸  Nexa SDK doesn't support absolute paths")
                print(f"   Using filename: {model_filename}")
                model = model_filename
            elif not os.path.isabs(model):
                # ç›¸å¯¹è·¯å¾„/æ–‡ä»¶åï¼šä¿æŒä¸å˜
                if self.models_dir:
                    full_path = self.get_model_path(model)
                    print(f"ğŸ“ Local GGUF file: {full_path}")
                    # ä½†å‘é€ç»™ API æ—¶åªç”¨æ–‡ä»¶å
                    model = os.path.basename(model)
        else:
            # è¿œç¨‹æ¨¡å‹ï¼šç¡®ä¿æ¨¡å‹å¯ç”¨
            if auto_download:
                if not self.ensure_model_available(model, auto_download=True):
                    raise RuntimeError(f"Failed to load model: {model}")
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": stream,
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if top_k is not None:
            payload["top_k"] = top_k
        if repetition_penalty is not None:
            payload["repetition_penalty"] = repetition_penalty
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        payload.update(kwargs)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” API Request:")
        print(f"   Endpoint: {self.chat_endpoint}")
        print(f"   Model: {payload['model']}")
        print(f"   Messages: {len(payload['messages'])} messages")
        
        try:
            response = requests.post(
                self.chat_endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            if response.status_code != 200:
                print(f"âŒ API Error {response.status_code}:")
                print(f"   Response: {response.text[:500]}")
            
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            raise RuntimeError("Request timeout. The model might be too slow or the service is overloaded.")
        except requests.exceptions.RequestException as e:
            error_msg = f"API request failed: {e}"
            # å°è¯•è·å–å“åº”å†…å®¹
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.text
                    error_msg += f"\nResponse: {error_detail[:500]}"
                except:
                    pass
            raise RuntimeError(error_msg)
    
    def text_completion(
        self,
        model: str,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        auto_download: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ–‡æœ¬è¡¥å…¨ API
        
        Args:
            model: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            prompt: è¾“å…¥æç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            API å“åº”
        """
        # å¤„ç†æ¨¡å‹è·¯å¾„
        original_model = model
        
        # å¦‚æœæ˜¯æœ¬åœ° GGUF æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if model.endswith('.gguf'):
            if os.path.isabs(model):
                # ç»å¯¹è·¯å¾„ï¼šNexa SDK ä¸æ”¯æŒï¼Œéœ€è¦æå–æ¨¡å‹å
                model_filename = os.path.basename(model)
                print(f"âš ï¸  Nexa SDK doesn't support absolute paths")
                print(f"   Using filename: {model_filename}")
                model = model_filename
            elif not os.path.isabs(model):
                # ç›¸å¯¹è·¯å¾„/æ–‡ä»¶åï¼šä¿æŒä¸å˜
                if self.models_dir:
                    full_path = self.get_model_path(model)
                    print(f"ğŸ“ Local GGUF file: {full_path}")
                    # ä½†å‘é€ç»™ API æ—¶åªç”¨æ–‡ä»¶å
                    model = os.path.basename(model)
        else:
            # è¿œç¨‹æ¨¡å‹ï¼šç¡®ä¿æ¨¡å‹å¯ç”¨
            if auto_download:
                if not self.ensure_model_available(model, auto_download=True):
                    raise RuntimeError(f"Failed to load model: {model}")
        
        payload = {
            "model": model,
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if top_k is not None:
            payload["top_k"] = top_k
        if repetition_penalty is not None:
            payload["repetition_penalty"] = repetition_penalty
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        payload.update(kwargs)
        
        try:
            response = requests.post(
                self.completions_endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=120
            )
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            raise RuntimeError("Request timeout. The model might be too slow or the service is overloaded.")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"API request failed: {e}")
    
    def generate_text(
        self,
        model: str,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        auto_download: bool = True,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆç®€åŒ–æ¥å£ï¼‰
        
        Args:
            model: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            prompt: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        # è°ƒç”¨èŠå¤©è¡¥å…¨ API
        response = self.chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            auto_download=auto_download,
            **kwargs
        )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        try:
            return response['choices'][0]['message']['content']
        except (KeyError, IndexError) as e:
            raise RuntimeError(f"Failed to parse response: {e}")
    
    def get_model_info(self, model: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹ ID
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        try:
            response = requests.get(self.models_endpoint, timeout=5)
            response.raise_for_status()
            data = response.json()
            
            for model_info in data.get('data', []):
                if model_info['id'] == model:
                    return model_info
            
            return None
        
        except Exception as e:
            print(f"âŒ Failed to get model info: {e}")
            return None


# å…¨å±€å¼•æ“å®ä¾‹å­—å…¸ï¼ˆæŒ‰ base_url åŒºåˆ†ï¼‰
_global_nexa_engines = {}


def get_nexa_engine(base_url: str = "http://127.0.0.1:11434", models_dir: Optional[str] = None) -> NexaInferenceEngine:
    """
    è·å– Nexa æ¨ç†å¼•æ“å®ä¾‹ï¼ˆæ”¯æŒå¤šä¸ªä¸åŒçš„æœåŠ¡åœ°å€ï¼‰
    
    Args:
        base_url: Nexa SDK æœåŠ¡çš„åŸºç¡€ URL
        models_dir: æœ¬åœ°æ¨¡å‹ç›®å½•
    
    Returns:
        NexaInferenceEngine å®ä¾‹
    """
    global _global_nexa_engines
    
    # ä½¿ç”¨ base_url ä½œä¸º key
    if base_url not in _global_nexa_engines:
        _global_nexa_engines[base_url] = NexaInferenceEngine(base_url, models_dir)
    
    # æ›´æ–°æ¨¡å‹ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼‰
    if models_dir:
        _global_nexa_engines[base_url].set_models_dir(models_dir)
    
    return _global_nexa_engines[base_url]
