"""
Nexa SDK Inference Engine - ä½¿ç”¨ Nexa SDK æœåŠ¡è¿›è¡Œæ¨ç†
é€šè¿‡ HTTP API è°ƒç”¨æœ¬åœ° Nexa SDK æœåŠ¡
æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„ç®¡ç†ï¼Œä¸ ComfyUI çš„ /models/LLM ç›®å½•é›†æˆ
"""

import requests
import os
from typing import Dict, List, Optional, Any


class NexaInferenceEngine:
    """Nexa SDK æ¨ç†å¼•æ“ï¼ˆé€šè¿‡ HTTP APIï¼‰"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:11434", models_dir: Optional[str] = None):
        """
        åˆå§‹åŒ– Nexa æ¨ç†å¼•æ“
        
        Args:
            base_url: Nexa SDK æœåŠ¡çš„åŸºç¡€ URL
            models_dir: æœ¬åœ°æ¨¡å‹ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ ComfyUI çš„ /models/LLMï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.models_endpoint = f"{self.base_url}/v1/models"
        self.chat_endpoint = f"{self.base_url}/v1/chat/completions"
        self.completions_endpoint = f"{self.base_url}/v1/completions"
        
        # æ¨¡å‹ç›®å½•é…ç½®
        self.models_dir = models_dir
        
        # ç¼“å­˜å¯ç”¨æ¨¡å‹åˆ—è¡¨
        self._available_models = None
    
    def set_models_dir(self, models_dir: str):
        """
        è®¾ç½®æ¨¡å‹ç›®å½•
        
        Args:
            models_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        """
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)
        print(f"ğŸ“ Nexa SDK models directory set to: {models_dir}")
    
    def get_models_dir(self) -> Optional[str]:
        """
        è·å–å½“å‰é…ç½®çš„æ¨¡å‹ç›®å½•
        
        Returns:
            æ¨¡å‹ç›®å½•è·¯å¾„
        """
        return self.models_dir
    
    def get_local_models(self) -> List[str]:
        """
        è·å–æœ¬åœ°å·²ä¸‹è½½çš„ GGUF æ¨¡å‹åˆ—è¡¨
        
        Returns:
            æœ¬åœ°æ¨¡å‹æ–‡ä»¶åˆ—è¡¨
        """
        if not self.models_dir or not os.path.exists(self.models_dir):
            return []
        
        models = []
        for item in os.listdir(self.models_dir):
            if item.endswith('.gguf'):
                models.append(item)
        
        return sorted(models)
    
    def get_model_path(self, model_name: str) -> str:
        """
        è·å–æ¨¡å‹çš„å®Œæ•´è·¯å¾„
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–æ–‡ä»¶å
        
        Returns:
            æ¨¡å‹çš„å®Œæ•´è·¯å¾„
        """
        if not self.models_dir:
            raise ValueError("Models directory not set. Call set_models_dir() first.")
        
        # å¦‚æœå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œç›´æ¥è¿”å›
        if os.path.isabs(model_name):
            return model_name
        
        # å¦åˆ™æ‹¼æ¥åˆ°æ¨¡å‹ç›®å½•
        return os.path.join(self.models_dir, os.path.basename(model_name))
    
    def get_available_models(self, force_refresh: bool = False) -> List[str]:
        """
        è·å– Nexa SDK æœåŠ¡ä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Returns:
            æ¨¡å‹ ID åˆ—è¡¨
        """
        if self._available_models is None or force_refresh:
            try:
                response = requests.get(self.models_endpoint, timeout=5)
                response.raise_for_status()
                data = response.json()
                
                # æå–æ¨¡å‹ ID
                self._available_models = [model['id'] for model in data.get('data', [])]
                print(f"âœ… Found {len(self._available_models)} models in Nexa SDK service")
                
            except Exception as e:
                print(f"âŒ Failed to fetch models from Nexa SDK: {e}")
                self._available_models = []
        
        return self._available_models
    
    def is_service_available(self) -> bool:
        """
        æ£€æŸ¥ Nexa SDK æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(self.models_endpoint, timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨ API
        
        Args:
            model: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            API å“åº”
        """
        # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶åï¼ˆ.ggufï¼‰ï¼Œè½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
        if model.endswith('.gguf') and not os.path.isabs(model):
            if self.models_dir:
                model = self.get_model_path(model)
                print(f"ğŸ“ Using local model: {model}")
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": stream,
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if top_k is not None:
            payload["top_k"] = top_k
        if repetition_penalty is not None:
            payload["repetition_penalty"] = repetition_penalty
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        payload.update(kwargs)
        
        try:
            response = requests.post(
                self.chat_endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            raise RuntimeError("Request timeout. The model might be too slow or the service is overloaded.")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"API request failed: {e}")
    
    def text_completion(
        self,
        model: str,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ–‡æœ¬è¡¥å…¨ API
        
        Args:
            model: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            prompt: è¾“å…¥æç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            API å“åº”
        """
        # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶åï¼ˆ.ggufï¼‰ï¼Œè½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
        if model.endswith('.gguf') and not os.path.isabs(model):
            if self.models_dir:
                model = self.get_model_path(model)
                print(f"ğŸ“ Using local model: {model}")
        
        payload = {
            "model": model,
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if top_k is not None:
            payload["top_k"] = top_k
        if repetition_penalty is not None:
            payload["repetition_penalty"] = repetition_penalty
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        payload.update(kwargs)
        
        try:
            response = requests.post(
                self.completions_endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=120
            )
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            raise RuntimeError("Request timeout. The model might be too slow or the service is overloaded.")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"API request failed: {e}")
    
    def generate_text(
        self,
        model: str,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆç®€åŒ–æ¥å£ï¼‰
        
        Args:
            model: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            prompt: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        # è°ƒç”¨èŠå¤©è¡¥å…¨ API
        response = self.chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            **kwargs
        )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        try:
            return response['choices'][0]['message']['content']
        except (KeyError, IndexError) as e:
            raise RuntimeError(f"Failed to parse response: {e}")
    
    def get_model_info(self, model: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹ ID
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        try:
            response = requests.get(self.models_endpoint, timeout=5)
            response.raise_for_status()
            data = response.json()
            
            for model_info in data.get('data', []):
                if model_info['id'] == model:
                    return model_info
            
            return None
        
        except Exception as e:
            print(f"âŒ Failed to get model info: {e}")
            return None


# å…¨å±€å¼•æ“å®ä¾‹å­—å…¸ï¼ˆæŒ‰ base_url åŒºåˆ†ï¼‰
_global_nexa_engines = {}


def get_nexa_engine(base_url: str = "http://127.0.0.1:11434", models_dir: Optional[str] = None) -> NexaInferenceEngine:
    """
    è·å– Nexa æ¨ç†å¼•æ“å®ä¾‹ï¼ˆæ”¯æŒå¤šä¸ªä¸åŒçš„æœåŠ¡åœ°å€ï¼‰
    
    Args:
        base_url: Nexa SDK æœåŠ¡çš„åŸºç¡€ URL
        models_dir: æœ¬åœ°æ¨¡å‹ç›®å½•
    
    Returns:
        NexaInferenceEngine å®ä¾‹
    """
    global _global_nexa_engines
    
    # ä½¿ç”¨ base_url ä½œä¸º key
    if base_url not in _global_nexa_engines:
        _global_nexa_engines[base_url] = NexaInferenceEngine(base_url, models_dir)
    
    # æ›´æ–°æ¨¡å‹ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼‰
    if models_dir:
        _global_nexa_engines[base_url].set_models_dir(models_dir)
    
    return _global_nexa_engines[base_url]
