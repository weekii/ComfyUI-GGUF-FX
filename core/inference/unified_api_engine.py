"""
ç»Ÿä¸€çš„ API å¼•æ“
æ”¯æŒ Nexa SDKã€Ollamaã€OpenAI å…¼å®¹çš„ API
"""

import requests
from typing import List, Dict, Any, Optional


class UnifiedAPIEngine:
    """ç»Ÿä¸€çš„ API å¼•æ“ï¼Œæ”¯æŒå¤šç§ API åç«¯"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:11434", api_type: str = "ollama"):
        """
        åˆå§‹åŒ– API å¼•æ“
        
        Args:
            base_url: API æœåŠ¡åœ°å€
            api_type: API ç±»å‹ (ollama, nexa, openai)
        """
        self.base_url = base_url.rstrip('/')
        self.api_type = api_type.lower()
        
        # è®¾ç½®ç«¯ç‚¹
        if self.api_type in ["ollama", "nexa"]:
            self.chat_endpoint = f"{self.base_url}/v1/chat/completions"
            self.models_endpoint = f"{self.base_url}/v1/models"
        elif self.api_type == "openai":
            self.chat_endpoint = f"{self.base_url}/v1/chat/completions"
            self.models_endpoint = f"{self.base_url}/v1/models"
        else:
            # é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼
            self.chat_endpoint = f"{self.base_url}/v1/chat/completions"
            self.models_endpoint = f"{self.base_url}/v1/models"
        
        self._available_models = None
    
    def is_service_available(self) -> bool:
        """
        æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(self.models_endpoint, timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self, force_refresh: bool = False) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Returns:
            æ¨¡å‹ ID åˆ—è¡¨
        """
        if self._available_models is None or force_refresh:
            try:
                response = requests.get(self.models_endpoint, timeout=5)
                response.raise_for_status()
                data = response.json()
                
                # æå–æ¨¡å‹ ID
                if 'data' in data:
                    # OpenAI æ ¼å¼
                    self._available_models = [model['id'] for model in data.get('data', [])]
                elif 'models' in data:
                    # Ollama æ ¼å¼
                    self._available_models = [model['name'] for model in data.get('models', [])]
                else:
                    self._available_models = []
                
                if not force_refresh:
                    print(f"âœ… Found {len(self._available_models)} models from {self.api_type} service")
                
            except Exception as e:
                if not force_refresh:
                    print(f"âŒ Failed to fetch models: {e}")
                self._available_models = []
        
        return self._available_models
    
    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ Chat Completion API
        
        Args:
            model: æ¨¡å‹ ID
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: Top-p é‡‡æ ·
            top_k: Top-k é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            API å“åº”
        """
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": stream,
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°ï¼ˆä¸åŒ API æ”¯æŒä¸åŒå‚æ•°ï¼‰
        if top_k is not None:
            payload["top_k"] = top_k
        if repetition_penalty is not None:
            if self.api_type == "ollama":
                payload["repeat_penalty"] = repetition_penalty
            else:
                payload["repetition_penalty"] = repetition_penalty
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        payload.update(kwargs)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” API Request:")
        print(f"   Type: {self.api_type}")
        print(f"   Endpoint: {self.chat_endpoint}")
        print(f"   Model: {model}")
        print(f"   Messages: {len(messages)} messages")
        
        try:
            response = requests.post(
                self.chat_endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            if response.status_code != 200:
                print(f"âŒ API Error {response.status_code}:")
                print(f"   Response: {response.text[:500]}")
            
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            raise RuntimeError("Request timeout. The model might be too slow or the service is overloaded.")
        except requests.exceptions.RequestException as e:
            error_msg = f"API request failed: {e}"
            # å°è¯•è·å–å“åº”å†…å®¹
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.text
                    error_msg += f"\nResponse: {error_detail[:500]}"
                except:
                    pass
            raise RuntimeError(error_msg)


# å…¨å±€å¼•æ“å®ä¾‹ç¼“å­˜
_engine_cache = {}


def get_unified_api_engine(base_url: str = "http://127.0.0.1:11434", api_type: str = "ollama") -> UnifiedAPIEngine:
    """
    è·å–æˆ–åˆ›å»º API å¼•æ“å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        base_url: API æœåŠ¡åœ°å€
        api_type: API ç±»å‹
    
    Returns:
        UnifiedAPIEngine å®ä¾‹
    """
    cache_key = f"{base_url}:{api_type}"
    
    if cache_key not in _engine_cache:
        _engine_cache[cache_key] = UnifiedAPIEngine(base_url, api_type)
    
    return _engine_cache[cache_key]
