"""
Inference Engine - è´Ÿè´£æ¨¡å‹æ¨ç†å’Œç”Ÿæˆ
"""

from typing import Dict, List, Optional, Any
import numpy as np


class InferenceEngine:
    """GGUF æ¨¡å‹æ¨ç†å¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨ç†å¼•æ“"""
        self.loaded_models: Dict[str, Any] = {}
        self.model_contexts: Dict[str, Any] = {}
    
    def load_model(self, model_path: str, **kwargs) -> bool:
        """
        åŠ è½½æ¨¡å‹åˆ°å†…å­˜
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            **kwargs: é¢å¤–çš„åŠ è½½å‚æ•°
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            from llama_cpp import Llama
            from llama_cpp.llama_chat_format import Llava15ChatHandler
            
            # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
            if model_path in self.loaded_models:
                print(f"âœ… Model already loaded: {model_path}")
                return True
            
            # åŠ è½½æ¨¡å‹
            n_ctx = kwargs.get('n_ctx', 8192)
            n_gpu_layers = kwargs.get('n_gpu_layers', -1)
            verbose = kwargs.get('verbose', False)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è§†è§‰æ¨¡å‹
            mmproj_path = kwargs.get('mmproj_path')
            
            if mmproj_path:
                # è§†è§‰è¯­è¨€æ¨¡å‹
                chat_handler = Llava15ChatHandler(clip_model_path=mmproj_path, verbose=verbose)
                llm = Llama(
                    model_path=model_path,
                    chat_handler=chat_handler,
                    n_ctx=n_ctx,
                    n_gpu_layers=n_gpu_layers,
                    verbose=verbose,
                    logits_all=True
                )
            else:
                # çº¯æ–‡æœ¬æ¨¡å‹
                llm = Llama(
                    model_path=model_path,
                    n_ctx=n_ctx,
                    n_gpu_layers=n_gpu_layers,
                    verbose=verbose
                )
            
            self.loaded_models[model_path] = llm
            print(f"âœ… Model loaded successfully: {model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load model {model_path}: {e}")
            return False
    
    def unload_model(self, model_path: str):
        """
        å¸è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        if model_path in self.loaded_models:
            del self.loaded_models[model_path]
            if model_path in self.model_contexts:
                del self.model_contexts[model_path]
            print(f"ğŸ—‘ï¸  Model unloaded: {model_path}")
    
    def generate_text(
        self,
        model_path: str,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if model_path not in self.loaded_models:
            raise ValueError(f"Model not loaded: {model_path}")
        
        llm = self.loaded_models[model_path]
        
        try:
            output = llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                echo=False,
                **kwargs
            )
            
            return output['choices'][0]['text']
        
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            return f"Error: {str(e)}"
    
    def generate_with_image(
        self,
        model_path: str,
        image_data: Any,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨å›¾åƒç”Ÿæˆæ–‡æœ¬ï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            image_data: å›¾åƒæ•°æ®
            prompt: æ–‡æœ¬æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if model_path not in self.loaded_models:
            raise ValueError(f"Model not loaded: {model_path}")
        
        llm = self.loaded_models[model_path]
        
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": image_data}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            output = llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                **kwargs
            )
            
            return output['choices'][0]['message']['content']
        
        except Exception as e:
            print(f"âŒ Vision generation failed: {e}")
            return f"Error: {str(e)}"
    
    def is_model_loaded(self, model_path: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return model_path in self.loaded_models
    
    def get_loaded_models(self) -> List[str]:
        """è·å–æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹è·¯å¾„"""
        return list(self.loaded_models.keys())
    
    def clear_all(self):
        """æ¸…é™¤æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹"""
        self.loaded_models.clear()
        self.model_contexts.clear()
        print("ğŸ—‘ï¸  All models unloaded")
