# å¿«é€Ÿå‚è€ƒ - llama-cpp-python CUDA å®‰è£…

## ğŸš€ ä¸€é”®å®‰è£…

```bash
wget https://raw.githubusercontent.com/weekii/ComfyUI-GGUF-FX/main/setup_llama_cpp_cuda.sh
chmod +x setup_llama_cpp_cuda.sh
./setup_llama_cpp_cuda.sh
```

## ğŸ“‹ è„šæœ¬åŠŸèƒ½

âœ… è‡ªåŠ¨æ£€æµ‹ Pythonã€CUDAã€GPU  
âœ… è‡ªåŠ¨è¯†åˆ« GPU å‹å·å’Œè®¡ç®—èƒ½åŠ›  
âœ… ä»æºç ç¼–è¯‘ CUDA ç‰ˆæœ¬  
âœ… ä¿®å¤ä¾èµ–å†²çª  
âœ… éªŒè¯å®‰è£…  
âœ… åˆ›å»ºæµ‹è¯•è„šæœ¬  

## ğŸ¯ æ”¯æŒçš„ GPU

| GPU | è®¡ç®—èƒ½åŠ› | æ€§èƒ½æå‡ |
|-----|---------|---------|
| H100 | 9.0 | 30-60x |
| A100 | 8.0 | 25-50x |
| RTX 4090 | 8.9 | 20-40x |
| RTX 3090 | 8.6 | 20-35x |

## âœ… éªŒè¯å®‰è£…

```bash
python3 /tmp/test_llama_cpp_cuda.py
```

## ğŸ”§ åœ¨ ComfyUI ä¸­ä½¿ç”¨

```
[Text Model Loader]
â”œâ”€ device: GPU  âœ… é€‰æ‹© GPU
â””â”€ n_ctx: 8192
```

## ğŸ“Š ç›‘æ§ GPU

```bash
watch -n 1 nvidia-smi
```

## ğŸ“ æ–‡ä»¶ä½ç½®

- **å®‰è£…è„šæœ¬**: `/workspace/setup_llama_cpp_cuda.sh`
- **æµ‹è¯•è„šæœ¬**: `/tmp/test_llama_cpp_cuda.py`
- **å®‰è£…æ—¥å¿—**: `/tmp/llama_cpp_install.log`
- **å®Œæ•´æ–‡æ¡£**: `/workspace/VASTAI_GPU_SETUP_GUIDE.md`

## ğŸ› å¸¸è§é—®é¢˜

### GPU æœªä½¿ç”¨ï¼Ÿ
1. ç¡®è®¤ `device: GPU`
2. é‡å¯ ComfyUI
3. æ£€æŸ¥æ—¥å¿—

### ç¼–è¯‘å¤±è´¥ï¼Ÿ
```bash
export CUDA_HOME=/usr/local/cuda
./setup_llama_cpp_cuda.sh
```

### ä¾èµ–å†²çªï¼Ÿ
```bash
pip install 'numpy<2.0,>=1.20' --force-reinstall
```

## ğŸ“ æ”¯æŒ

- GitHub: https://github.com/weekii/ComfyUI-GGUF-FX
- æ–‡æ¡£: https://llama-cpp-python.readthedocs.io/

---

**å¿«é€Ÿã€ç®€å•ã€è‡ªåŠ¨åŒ–ï¼** ğŸ‰
