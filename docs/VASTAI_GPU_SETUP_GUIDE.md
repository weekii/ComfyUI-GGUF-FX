# Vast.ai GPU ä¸»æœº llama-cpp-python CUDA å®‰è£…æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### ä¸€é”®å®‰è£…

```bash
# ä¸‹è½½è„šæœ¬
wget https://raw.githubusercontent.com/weekii/ComfyUI-GGUF-FX/main/setup_llama_cpp_cuda.sh

# æˆ–è€…å¦‚æœä½ å·²ç»æœ‰è„šæœ¬
chmod +x setup_llama_cpp_cuda.sh

# è¿è¡Œå®‰è£…
./setup_llama_cpp_cuda.sh
```

## è„šæœ¬åŠŸèƒ½

### è‡ªåŠ¨æ£€æµ‹

- âœ… **Python ç‰ˆæœ¬** - è‡ªåŠ¨æ£€æµ‹ Python 3.x
- âœ… **è™šæ‹Ÿç¯å¢ƒ** - è‡ªåŠ¨æ£€æµ‹å¹¶æ¿€æ´» `/venv/main` æˆ–å½“å‰ç¯å¢ƒ
- âœ… **CUDA ç‰ˆæœ¬** - ä» nvidia-smi è¯»å–
- âœ… **GPU å‹å·** - è‡ªåŠ¨è¯†åˆ«å¹¶è®¾ç½®è®¡ç®—èƒ½åŠ›
  - H100: 9.0
  - A100: 8.0
  - RTX 4090/4080: 8.9
  - RTX 3090/3080: 8.6
- âœ… **CUDA è·¯å¾„** - è‡ªåŠ¨æŸ¥æ‰¾ `/usr/local/cuda*`

### è‡ªåŠ¨å®‰è£…

1. å¸è½½ç°æœ‰ llama-cpp-python
2. ä»æºç ç¼–è¯‘ CUDA ç‰ˆæœ¬
3. é’ˆå¯¹ GPU ä¼˜åŒ–ç¼–è¯‘å‚æ•°
4. ä¿®å¤ numpy ç‰ˆæœ¬å†²çª
5. éªŒè¯ CUDA æ”¯æŒ
6. åˆ›å»ºæµ‹è¯•è„šæœ¬

### å®‰å…¨ç‰¹æ€§

- æ£€æŸ¥ç°æœ‰å®‰è£…ï¼Œé¿å…é‡å¤
- é‡åˆ°é”™è¯¯è‡ªåŠ¨é€€å‡º
- ä¿å­˜å®‰è£…æ—¥å¿—
- å½©è‰²è¾“å‡ºï¼Œæ˜“äºæŸ¥çœ‹

## æ”¯æŒçš„ GPU

### å·²æµ‹è¯•

- âœ… NVIDIA H100 (è®¡ç®—èƒ½åŠ› 9.0)
- âœ… NVIDIA A100 (è®¡ç®—èƒ½åŠ› 8.0)
- âœ… RTX 4090 (è®¡ç®—èƒ½åŠ› 8.9)
- âœ… RTX 3090 (è®¡ç®—èƒ½åŠ› 8.6)

### ç†è®ºæ”¯æŒ

æ‰€æœ‰æ”¯æŒ CUDA çš„ NVIDIA GPUï¼ˆè®¡ç®—èƒ½åŠ› >= 6.0ï¼‰

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ–°çš„ Vast.ai å®ä¾‹

```bash
# 1. SSH ç™»å½•åˆ° Vast.ai å®ä¾‹
ssh -p PORT root@IP

# 2. ä¸‹è½½è„šæœ¬
wget https://raw.githubusercontent.com/weekii/ComfyUI-GGUF-FX/main/setup_llama_cpp_cuda.sh

# 3. è¿è¡Œ
chmod +x setup_llama_cpp_cuda.sh
./setup_llama_cpp_cuda.sh

# 4. å®Œæˆï¼
```

### åœºæ™¯ 2: å·²æœ‰ ComfyUI ç¯å¢ƒ

```bash
# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœæœ‰ï¼‰
source /venv/main/bin/activate

# 2. è¿è¡Œè„šæœ¬
./setup_llama_cpp_cuda.sh

# 3. é‡å¯ ComfyUI
cd /workspace/ComfyUI
python3 main.py
```

### åœºæ™¯ 3: æ›´æ¢ GPU å‹å·

```bash
# å¦‚æœæ›´æ¢äº† GPUï¼ˆå¦‚ä» A100 æ¢åˆ° H100ï¼‰
# é‡æ–°è¿è¡Œè„šæœ¬å³å¯ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹æ–° GPU
./setup_llama_cpp_cuda.sh
```

## éªŒè¯å®‰è£…

### æ–¹æ³• 1: ä½¿ç”¨æµ‹è¯•è„šæœ¬

```bash
python3 /tmp/test_llama_cpp_cuda.py
```

**é¢„æœŸè¾“å‡º**:
```
============================================================
llama-cpp-python CUDA æµ‹è¯•
============================================================

âœ… llama-cpp-python å·²å®‰è£…

åº“è·¯å¾„: /venv/main/lib/python3.12/site-packages/llama_cpp/lib

âœ… CUDA åº“:
   - libggml-cuda.so (130.8 MB)

âœ… PyTorch CUDA: True
   CUDA ç‰ˆæœ¬: 12.6
   GPU: NVIDIA H100 80GB HBM3

============================================================
æµ‹è¯•å®Œæˆï¼
============================================================
```

### æ–¹æ³• 2: åœ¨ ComfyUI ä¸­æµ‹è¯•

1. **åŠ è½½æ¨¡å‹**
   ```
   [Text Model Loader]
   â”œâ”€ model: é€‰æ‹© GGUF æ¨¡å‹
   â”œâ”€ device: GPU  âœ…
   â””â”€ n_ctx: 8192
   ```

2. **ç›‘æ§ GPU**
   ```bash
   watch -n 1 nvidia-smi
   ```

3. **ç”Ÿæˆæ–‡æœ¬**
   ```
   [Text Generation]
   â”œâ”€ prompt: "Write a story"
   â””â”€ max_tokens: 512
   ```

**é¢„æœŸç»“æœ**:
- GPU å†…å­˜ä½¿ç”¨å¢åŠ 
- GPU åˆ©ç”¨ç‡ 30-90%
- ç”Ÿæˆé€Ÿåº¦: 100-300+ tokens/s

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: ç¼–è¯‘å¤±è´¥

**é”™è¯¯**: `CUDA not found`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ‰‹åŠ¨è®¾ç½® CUDA è·¯å¾„
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# é‡æ–°è¿è¡Œè„šæœ¬
./setup_llama_cpp_cuda.sh
```

### é—®é¢˜ 2: æœªæ£€æµ‹åˆ° GPU

**é”™è¯¯**: `æœªæ£€æµ‹åˆ° NVIDIA GPU`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥é©±åŠ¨
nvidia-smi

# å¦‚æœå¤±è´¥ï¼Œé‡å¯å®ä¾‹æˆ–è”ç³» Vast.ai æ”¯æŒ
```

### é—®é¢˜ 3: è™šæ‹Ÿç¯å¢ƒé—®é¢˜

**é”™è¯¯**: `æœªæ£€æµ‹åˆ°è™šæ‹Ÿç¯å¢ƒ`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ‰‹åŠ¨æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source /venv/main/bin/activate

# æˆ–åˆ›å»ºæ–°çš„è™šæ‹Ÿç¯å¢ƒ
python3 -m venv /venv/main
source /venv/main/bin/activate

# é‡æ–°è¿è¡Œè„šæœ¬
./setup_llama_cpp_cuda.sh
```

### é—®é¢˜ 4: GPU æœªä½¿ç”¨

**ç—‡çŠ¶**: ComfyUI æ—¥å¿—æ˜¾ç¤º "ğŸ’» Using CPU only"

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®è®¤ `device` è®¾ç½®ä¸º `GPU`
2. é‡å¯ ComfyUI
3. æ£€æŸ¥æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯

### é—®é¢˜ 5: ä¾èµ–å†²çª

**è­¦å‘Š**: `numpy version incompatible`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è„šæœ¬ä¼šè‡ªåŠ¨ä¿®å¤ï¼Œå¦‚æœä»æœ‰é—®é¢˜ï¼š
pip install 'numpy<2.0,>=1.20' --force-reinstall
```

## æ€§èƒ½åŸºå‡†

### H100 GPU

| æ¨¡å‹å¤§å° | CPU | H100 GPU | åŠ é€Ÿæ¯” |
|---------|-----|----------|--------|
| 4B Q4_K_M | 5-8 tokens/s | 150-250 tokens/s | 30-40x |
| 7B Q4_K_M | 3-5 tokens/s | 100-180 tokens/s | 30-50x |
| 13B Q4_K_M | 2-3 tokens/s | 60-120 tokens/s | 30-60x |

### A100 GPU

| æ¨¡å‹å¤§å° | CPU | A100 GPU | åŠ é€Ÿæ¯” |
|---------|-----|----------|--------|
| 4B Q4_K_M | 5-8 tokens/s | 120-200 tokens/s | 25-35x |
| 7B Q4_K_M | 3-5 tokens/s | 80-150 tokens/s | 25-40x |
| 13B Q4_K_M | 2-3 tokens/s | 50-100 tokens/s | 25-50x |

### RTX 4090

| æ¨¡å‹å¤§å° | CPU | RTX 4090 | åŠ é€Ÿæ¯” |
|---------|-----|----------|--------|
| 4B Q4_K_M | 5-8 tokens/s | 100-180 tokens/s | 20-30x |
| 7B Q4_K_M | 3-5 tokens/s | 70-130 tokens/s | 20-35x |
| 13B Q4_K_M | 2-3 tokens/s | 40-80 tokens/s | 20-40x |

## æ–‡ä»¶è¯´æ˜

### è„šæœ¬ç”Ÿæˆçš„æ–‡ä»¶

- `/tmp/llama_cpp_install.log` - å®‰è£…æ—¥å¿—
- `/tmp/test_llama_cpp_cuda.py` - æµ‹è¯•è„šæœ¬

### è„šæœ¬ä½ç½®

æ¨èä¿å­˜åœ¨ï¼š
- `/workspace/setup_llama_cpp_cuda.sh` - å·¥ä½œç›®å½•
- æˆ– GitHub ä»“åº“ä¸­ä¾¿äºä¸‹è½½

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è®¡ç®—èƒ½åŠ›

å¦‚æœè„šæœ¬æœªæ­£ç¡®è¯†åˆ«ä½ çš„ GPUï¼š

```bash
# ç¼–è¾‘è„šæœ¬ï¼Œæ‰¾åˆ° COMPUTE_CAP è®¾ç½®
# æˆ–è€…æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
export COMPUTE_CAP="80"  # ä½ çš„ GPU è®¡ç®—èƒ½åŠ›
./setup_llama_cpp_cuda.sh
```

### è‡ªå®šä¹‰ CUDA è·¯å¾„

```bash
export CUDA_HOME=/path/to/cuda
./setup_llama_cpp_cuda.sh
```

### é™é»˜å®‰è£…

```bash
# è·³è¿‡ç¡®è®¤æç¤º
yes | ./setup_llama_cpp_cuda.sh
```

## ä¸ ComfyUI-GGUF-FX é›†æˆ

æ­¤è„šæœ¬ä¸“ä¸º ComfyUI-GGUF-FX è®¾è®¡ï¼Œå®‰è£…åï¼š

1. **Text Model Loader** èŠ‚ç‚¹æ”¯æŒ GPU åŠ é€Ÿ
2. **Vision Description** èŠ‚ç‚¹æ”¯æŒ GPU åŠ é€Ÿ
3. æ‰€æœ‰ GGUF æ¨¡å‹æ¨ç†éƒ½ä¼šä½¿ç”¨ GPU

## æ›´æ–°å’Œç»´æŠ¤

### æ›´æ–° llama-cpp-python

```bash
# é‡æ–°è¿è¡Œè„šæœ¬å³å¯
./setup_llama_cpp_cuda.sh
```

### æ£€æŸ¥æ›´æ–°

```bash
# æ£€æŸ¥æœ€æ–°ç‰ˆæœ¬
pip search llama-cpp-python

# æˆ–è®¿é—®
# https://github.com/abetlen/llama-cpp-python
```

## æ”¯æŒ

### é—®é¢˜åé¦ˆ

- GitHub Issues: https://github.com/weekii/ComfyUI-GGUF-FX/issues
- è„šæœ¬é—®é¢˜è¯·é™„ä¸Š `/tmp/llama_cpp_install.log`

### ç›¸å…³èµ„æº

- llama-cpp-python å®˜æ–¹æ–‡æ¡£: https://llama-cpp-python.readthedocs.io/
- llama.cpp é¡¹ç›®: https://github.com/ggerganov/llama.cpp
- ComfyUI-GGUF-FX: https://github.com/weekii/ComfyUI-GGUF-FX

## è®¸å¯è¯

æ­¤è„šæœ¬éµå¾ª MIT è®¸å¯è¯ï¼Œå¯è‡ªç”±ä½¿ç”¨å’Œä¿®æ”¹ã€‚

---

**ç‰ˆæœ¬**: 1.0  
**æ—¥æœŸ**: 2025-10-29  
**ä½œè€…**: ComfyUI-GGUF-FX  
**æµ‹è¯•ç¯å¢ƒ**: Vast.ai H100, A100, RTX 4090
