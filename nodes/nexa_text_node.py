"""
Nexa SDK Text Node - ä½¿ç”¨ Nexa SDK æœåŠ¡çš„æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹
æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„ç®¡ç†ã€è‡ªåŠ¨ä¸‹è½½å’Œä¸ ComfyUI çš„ /models/LLM ç›®å½•é›†æˆ
"""

import re
import os
from typing import Tuple

# å°è¯•å¯¼å…¥è·¯å¾„é…ç½®
try:
    from ..config.paths import PathConfig
    HAS_PATH_CONFIG = True
except:
    HAS_PATH_CONFIG = False
    print("âš ï¸  PathConfig not available, using default paths")

from ..core.inference.nexa_engine import get_nexa_engine


# Nexa SDK é¢„è®¾æ¨¡å‹åˆ—è¡¨
# æ ¼å¼: author/model-name:quant
# ä½¿ç”¨å‰éœ€è¦å…ˆè¿è¡Œ: nexa pull <model-name>
PRESET_MODELS = [
    "Custom (è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹ ID)",
    "DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-HORROR-Max-GGUF:Q6_K",
    "mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF:Q8_0",
    "prithivMLmods/Qwen3-4B-2507-abliterated-GGUF:Q8_0",
    "mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0",
    "mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF:Q8_0",
]

# HuggingFace URL åˆ°æ¨¡å‹ ID çš„æ˜ å°„
HUGGINGFACE_URL_MAPPING = {
    "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/blob/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q8_0.gguf": 
        "prithivMLmods/Qwen3-4B-2507-abliterated-GGUF:Q8_0",
    
    "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Uncensored-Fixed.Q8_0.gguf":
        "mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0",
    
    "https://huggingface.co/mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF/blob/main/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B.Q8_0.gguf":
        "mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF:Q8_0",
    
    "https://huggingface.co/Triangle104/Josiefied-Qwen3-4B-abliterated-v2-Q8_0-GGUF/blob/main/josiefied-qwen3-4b-abliterated-v2-q8_0.gguf":
        "Triangle104/Josiefied-Qwen3-4B-abliterated-v2-Q8_0-GGUF",
}


def parse_model_input(model_input: str) -> str:
    """
    è§£ææ¨¡å‹è¾“å…¥ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. æ¨¡å‹ ID: "user/repo:quantization"
    2. HuggingFace URL
    3. æœ¬åœ°æ–‡ä»¶å: "model.gguf"
    
    Returns:
        æ ‡å‡†åŒ–çš„æ¨¡å‹æ ‡è¯†ç¬¦
    """
    model_input = model_input.strip()
    
    # å¦‚æœæ˜¯ HuggingFace URLï¼Œè½¬æ¢ä¸ºæ¨¡å‹ ID
    if model_input.startswith("https://huggingface.co/"):
        if model_input in HUGGINGFACE_URL_MAPPING:
            return HUGGINGFACE_URL_MAPPING[model_input]
        
        # å°è¯•ä» URL ä¸­æå–æ¨¡å‹ä¿¡æ¯
        # æ ¼å¼: https://huggingface.co/user/repo/blob/main/file.gguf
        # æˆ–: https://huggingface.co/user/repo/resolve/main/file.gguf
        parts = model_input.replace("https://huggingface.co/", "").split("/")
        if len(parts) >= 2:
            user = parts[0]
            repo = parts[1]
            
            # æå–é‡åŒ–ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(parts) >= 4:
                filename = parts[-1]
                # ä»æ–‡ä»¶åæå–é‡åŒ–ç±»å‹ï¼Œå¦‚ Q8_0, Q6_K ç­‰
                import re
                quant_match = re.search(r'\.(Q\d+_[0K]|Q\d+)', filename, re.IGNORECASE)
                if quant_match:
                    quant = quant_match.group(1).upper()
                    return f"{user}/{repo}:{quant}"
            
            return f"{user}/{repo}"
    
    # ç›´æ¥è¿”å›ï¼ˆæ¨¡å‹ ID æˆ–æœ¬åœ°æ–‡ä»¶åï¼‰
    return model_input


class NexaModelSelector:
    """Nexa SDK æ¨¡å‹é€‰æ‹©å™¨"""
    
    @classmethod
    def INPUT_TYPES(cls):
        # é»˜è®¤ API ç«¯ç‚¹
        default_base_url = "http://127.0.0.1:11434"
        
        # è·å– LLM æ¨¡å‹ç›®å½•
        if HAS_PATH_CONFIG:
            default_models_dir = PathConfig.get_llm_models_path()
        else:
            import folder_paths
            default_models_dir = os.path.join(folder_paths.models_dir, "LLM", "GGUF")
            os.makedirs(default_models_dir, exist_ok=True)
        
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": default_base_url,
                    "tooltip": "Nexa SDK æœåŠ¡åœ°å€ï¼ˆå¯é…ç½®ï¼‰"
                }),
                "models_dir": ("STRING", {
                    "default": default_models_dir,
                    "tooltip": "æœ¬åœ°æ¨¡å‹ç›®å½•ï¼ˆGGUF æ–‡ä»¶å­˜æ”¾ä½ç½®ï¼‰"
                }),
                "model_source": (["Remote (Nexa Service)", "Local (GGUF File)"], {
                    "default": "Remote (Nexa Service)",
                    "tooltip": "æ¨¡å‹æ¥æºï¼šè¿œç¨‹æœåŠ¡æˆ–æœ¬åœ°æ–‡ä»¶"
                }),
                "refresh_models": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
                }),
            },
            "optional": {
                "system_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("NEXA_MODEL", "STRING")
    RETURN_NAMES = ("model_config", "available_models")
    FUNCTION = "select_model"
    CATEGORY = "GGUF-VisionLM/Nexa"
    OUTPUT_NODE = True
    
    def select_model(
        self, 
        base_url: str, 
        models_dir: str,
        model_source: str,
        refresh_models: bool = False,
        system_prompt: str = ""
    ):
        """é€‰æ‹©æ¨¡å‹å¹¶è¿”å›é…ç½®"""
        
        # åˆ›å»ºæˆ–è·å–å¼•æ“
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        is_available = engine.is_service_available()
        
        if not is_available:
            error_msg = f"âš ï¸  Nexa SDK service is not available at {base_url}"
            print(error_msg)
            print("   Please make sure the service is running.")
            
            # å³ä½¿æœåŠ¡ä¸å¯ç”¨ï¼Œä¹Ÿè¿”å›é…ç½®ï¼ˆç”¨äºæœ¬åœ°æ¨¡å‹ï¼‰
            config = {
                "base_url": base_url,
                "models_dir": models_dir,
                "model_source": model_source,
                "system_prompt": system_prompt,
                "engine_type": "nexa",
                "service_available": False
            }
            return (config, error_msg)
        
        # è·å–å¯ç”¨æ¨¡å‹
        available_models_list = []
        
        if model_source == "Remote (Nexa Service)":
            # è¿œç¨‹æ¨¡å‹
            remote_models = engine.get_available_models(force_refresh=refresh_models)
            available_models_list.extend([f"[Remote] {m}" for m in remote_models])
        else:
            # æœ¬åœ°æ¨¡å‹
            local_models = engine.get_local_models()
            available_models_list.extend([f"[Local] {m}" for m in local_models])
        
        # æ ¼å¼åŒ–è¾“å‡º
        if available_models_list:
            models_text = "\n".join(available_models_list)
            print(f"âœ… Found {len(available_models_list)} models")
        else:
            models_text = "âš ï¸  No models found"
            print(models_text)
        
        # åˆ›å»ºé…ç½®
        config = {
            "base_url": base_url,
            "models_dir": models_dir,
            "model_source": model_source,
            "system_prompt": system_prompt,
            "engine_type": "nexa",
            "service_available": True,
            "available_models": available_models_list
        }
        
        print(f"âœ… Nexa SDK configured")
        print(f"   Service URL: {base_url}")
        print(f"   Models Dir: {models_dir}")
        print(f"   Source: {model_source}")
        
        return (config, models_text)


class NexaTextGeneration:
    """Nexa SDK æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_config": ("NEXA_MODEL", {
                    "tooltip": "Nexa æ¨¡å‹é…ç½®ï¼ˆæ¥è‡ª Model Selectorï¼‰"
                }),
                "preset_model": (PRESET_MODELS, {
                    "default": PRESET_MODELS[0],
                    "tooltip": "é¢„è®¾æ¨¡å‹åˆ—è¡¨ï¼ˆé€‰æ‹©æˆ–ä½¿ç”¨è‡ªå®šä¹‰ï¼‰"
                }),
                "custom_model": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "è‡ªå®šä¹‰æ¨¡å‹ IDï¼ˆæ ¼å¼: author/model:quantï¼‰"
                }),
                "auto_download": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ nexa pullï¼‰"
                }),
                "prompt": ("STRING", {
                    "default": "Hello, how are you?",
                    "multiline": True,
                    "tooltip": "è¾“å…¥æç¤ºè¯"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 8192,
                    "step": 1,
                    "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p é‡‡æ ·"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "Top-k é‡‡æ ·ï¼ˆ0 è¡¨ç¤ºç¦ç”¨ï¼‰"
                }),
                "repetition_penalty": ("FLOAT", {
                    "default": 1.1,
                    "min": 1.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "é‡å¤æƒ©ç½š"
                }),
                "enable_thinking": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆæ”¯æŒ DeepSeek-R1, Qwen3-Thinking ç­‰æ¨¡å‹ï¼‰"
                }),
            },
            "optional": {
                "conversation_history": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "å¯¹è¯å†å²ï¼ˆJSON æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("context", "thinking", "raw_response")
    FUNCTION = "generate"
    CATEGORY = "GGUF-VisionLM/Nexa"
    OUTPUT_NODE = True
    
    @staticmethod
    def _extract_thinking(text: str, enable_thinking: bool) -> Tuple[str, str]:
        """
        ä»è¾“å‡ºä¸­æå–æ€è€ƒå†…å®¹
        
        æ”¯æŒå¤šç§æ ¼å¼:
        1. <think>...</think> (Qwen3, DeepSeek-R1)
        2. <thinking>...</thinking>
        3. [THINKING]...[/THINKING]
        
        æ— è®º enable_thinking æ˜¯å¦å¯ç”¨ï¼Œéƒ½ä¼šç§»é™¤ think æ ‡ç­¾ã€‚
        å½“ enable_thinking=False æ—¶ï¼Œä¸è¿”å› thinking å†…å®¹ã€‚
        
        Returns:
            (final_output, thinking_content)
        """
        # æ¨¡å¼ 1: <think>...</think>
        think_pattern = r'<think>(.*?)</think>'
        matches = re.findall(think_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches) if enable_thinking else ""
            # ç§»é™¤æ€è€ƒæ ‡ç­¾ï¼Œä¿ç•™æœ€ç»ˆç­”æ¡ˆ
            final_output = re.sub(think_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ¨¡å¼ 2: <thinking>...</thinking>
        thinking_pattern = r'<thinking>(.*?)</thinking>'
        matches = re.findall(thinking_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches) if enable_thinking else ""
            final_output = re.sub(thinking_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ¨¡å¼ 3: [THINKING]...[/THINKING]
        bracket_pattern = r'\[THINKING\](.*?)\[/THINKING\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches) if enable_thinking else ""
            final_output = re.sub(bracket_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ²¡æœ‰æ‰¾åˆ°æ€è€ƒæ ‡è®°ï¼Œè¿”å›åŸæ–‡
        return text, ""
    
    def generate(
        self,
        model_config,
        preset_model: str,
        custom_model: str,
        auto_download: bool,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 40,
        repetition_penalty: float = 1.1,
        enable_thinking: bool = False,
        conversation_history: str = ""
    ):
        """ç”Ÿæˆæ–‡æœ¬"""
        
        # è·å–é…ç½®
        base_url = model_config.get('base_url', 'http://127.0.0.1:11434')
        models_dir = model_config.get('models_dir', '/workspace/ComfyUI/models/LLM')
        model_source = model_config.get('model_source', 'Remote (Nexa Service)')
        system_prompt = model_config.get('system_prompt', '')
        
        # è·å–å¼•æ“
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        if not engine.is_service_available():
            error_msg = f"âŒ Nexa SDK service is not available at {base_url}"
            print(error_msg)
            print("   Please make sure the service is running.")
            return (error_msg, "", "")
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
        if preset_model == "Custom (è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹)":
            if not custom_model:
                error_msg = "âŒ Please specify a custom model"
                print(error_msg)
                return (error_msg, "", "")
            model = parse_model_input(custom_model)
            print(f"ğŸ“ Using custom model: {model}")
        else:
            model = preset_model
            print(f"ğŸ“‹ Using preset model: {model}")
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨ä¸‹è½½ï¼Œç¡®ä¿æ¨¡å‹å¯ç”¨
        if auto_download and model_source == "Remote (Nexa Service)":
            print(f"ğŸ” Checking model availability...")
            engine.ensure_model_available(model, auto_download=True)
        
        # Nexa SDK åªæ”¯æŒé€šè¿‡ 'nexa pull' ä¸‹è½½çš„æ¨¡å‹
        # æ¨¡å‹æ ¼å¼: author/model-name:quant
        model_id = model
        print(f"ğŸŒ Using Nexa SDK model: {model_id}")
        print(f"ğŸ’¡ Make sure you've run: nexa pull {model_id}")
        
        # å¤„ç†æ€è€ƒæ§åˆ¶
        if not enable_thinking and system_prompt:
            # å¦‚æœç¦ç”¨æ€è€ƒï¼Œæ·»åŠ  no_think åˆ°ç³»ç»Ÿæç¤ºè¯
            if 'no_think' not in system_prompt.lower():
                system_prompt = f"{system_prompt} no_think"
        elif not enable_thinking and not system_prompt:
            system_prompt = "no_think"
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        # 1. ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # 2. å¯¹è¯å†å²ï¼ˆå¦‚æœæä¾›ï¼‰
        if conversation_history:
            try:
                import json
                history = json.loads(conversation_history)
                if isinstance(history, list):
                    messages.extend(history)
            except:
                # å¦‚æœä¸æ˜¯ JSON æ ¼å¼ï¼Œä½œä¸ºæ™®é€šæ–‡æœ¬æ·»åŠ 
                messages.append({"role": "user", "content": conversation_history})
        
        # 3. å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": prompt})
        
        print(f"ğŸ¤– Generating text with Nexa SDK...")
        print(f"   Model: {model_id}")
        print(f"   Source: {model_source}")
        print(f"   Auto-download: {'âœ… Enabled' if auto_download else 'âŒ Disabled'}")
        print(f"   Messages: {len(messages)} messages")
        if not enable_thinking:
            print(f"   ğŸš« Thinking disabled")
        
        try:
            # å‡†å¤‡å‚æ•°
            params = {
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": top_p,
                "auto_download": auto_download,
            }
            
            # åªåœ¨éé›¶æ—¶æ·»åŠ  top_k å’Œ repetition_penalty
            if top_k > 0:
                params["top_k"] = top_k
            if repetition_penalty > 1.0:
                params["repetition_penalty"] = repetition_penalty
            
            # è°ƒç”¨ API
            response = engine.chat_completion(
                model=model_id,
                messages=messages,
                **params
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            raw_output = response['choices'][0]['message']['content']
            
            # æå–æ€è€ƒå†…å®¹
            final_output, thinking = self._extract_thinking(raw_output, enable_thinking)
            
            # æ¸…ç†è¾“å‡ºï¼šç§»é™¤å¯èƒ½çš„è§’è‰²å‰ç¼€
            final_output = final_output.strip()
            for prefix in ["assistant:", "Assistant:", "ASSISTANT:"]:
                if final_output.startswith(prefix):
                    final_output = final_output[len(prefix):].strip()
                    break
            
            if enable_thinking and thinking:
                print(f"   ğŸ’­ Thinking process extracted ({len(thinking)} chars)")
            
            print(f"   âœ… Generated {len(final_output)} characters")
            
            # æ ¼å¼åŒ–åŸå§‹å“åº”
            import json
            raw_response = json.dumps(response, indent=2, ensure_ascii=False)
            
            return (final_output, thinking, raw_response)
        
        except Exception as e:
            error_msg = f"âŒ Generation failed: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return (error_msg, "", str(e))


class NexaServiceStatus:
    """Nexa SDK æœåŠ¡çŠ¶æ€æ£€æŸ¥èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å– LLM æ¨¡å‹ç›®å½•
        if HAS_PATH_CONFIG:
            default_models_dir = PathConfig.get_llm_models_path()
        else:
            import folder_paths
            default_models_dir = os.path.join(folder_paths.models_dir, "LLM", "GGUF")
            os.makedirs(default_models_dir, exist_ok=True)
        
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "Nexa SDK æœåŠ¡åœ°å€ï¼ˆå¯é…ç½®ï¼‰"
                }),
                "models_dir": ("STRING", {
                    "default": default_models_dir,
                    "tooltip": "æœ¬åœ°æ¨¡å‹ç›®å½•"
                }),
                "refresh": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("status", "remote_models", "local_models")
    FUNCTION = "check_status"
    CATEGORY = "GGUF-VisionLM/Nexa"
    OUTPUT_NODE = True
    
    def check_status(self, base_url: str, models_dir: str, refresh: bool = False):
        """æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
        
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        is_available = engine.is_service_available()
        
        status_lines = []
        status_lines.append(f"Nexa SDK Service: {base_url}")
        status_lines.append(f"Models Directory: {models_dir}")
        status_lines.append("")
        
        if is_available:
            # è·å–è¿œç¨‹æ¨¡å‹åˆ—è¡¨
            remote_models = engine.get_available_models(force_refresh=refresh)
            
            status_lines.append(f"âœ… Service is AVAILABLE")
            status_lines.append(f"Found {len(remote_models)} remote model(s)")
            
            remote_models_str = "\n".join([f"  - {model}" for model in remote_models]) if remote_models else "  (none)"
        else:
            status_lines.append(f"âŒ Service is NOT AVAILABLE")
            status_lines.append("Please make sure the service is running.")
            remote_models_str = "Service unavailable"
        
        # è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨
        local_models = engine.get_local_models()
        status_lines.append(f"Found {len(local_models)} local model(s)")
        
        local_models_str = "\n".join([f"  - {model}" for model in local_models]) if local_models else "  (none)"
        
        status = "\n".join(status_lines)
        
        print(status)
        print("\nRemote models:")
        print(remote_models_str)
        print("\nLocal models:")
        print(local_models_str)
        
        return (status, remote_models_str, local_models_str)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "NexaModelSelector": NexaModelSelector,
    "NexaTextGeneration": NexaTextGeneration,
    "NexaServiceStatus": NexaServiceStatus,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "NexaModelSelector": "ğŸ”· Nexa Model Selector",
    "NexaTextGeneration": "ğŸ”· Nexa Text Generation",
    "NexaServiceStatus": "ğŸ”· Nexa Service Status",
}
