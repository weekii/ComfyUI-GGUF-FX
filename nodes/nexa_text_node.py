"""
Nexa SDK Text Node - ä½¿ç”¨ Nexa SDK æœåŠ¡çš„æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹
æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„ç®¡ç†ï¼Œä¸ ComfyUI çš„ /models/LLM ç›®å½•é›†æˆ
"""

import re
import os
from typing import Tuple

# å°è¯•å¯¼å…¥è·¯å¾„é…ç½®
try:
    from ..config.paths import PathConfig
    HAS_PATH_CONFIG = True
except:
    HAS_PATH_CONFIG = False
    print("âš ï¸  PathConfig not available, using default paths")

from ..core.inference.nexa_engine import get_nexa_engine


class NexaModelSelector:
    """Nexa SDK æ¨¡å‹é€‰æ‹©å™¨"""
    
    @classmethod
    def INPUT_TYPES(cls):
        # é»˜è®¤ API ç«¯ç‚¹
        default_base_url = "http://127.0.0.1:11434"
        
        # è·å– LLM æ¨¡å‹ç›®å½•
        if HAS_PATH_CONFIG:
            default_models_dir = PathConfig.get_llm_models_path()
        else:
            default_models_dir = "/workspace/ComfyUI/models/LLM"
        
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": default_base_url,
                    "tooltip": "Nexa SDK æœåŠ¡åœ°å€ï¼ˆå¯é…ç½®ï¼‰"
                }),
                "models_dir": ("STRING", {
                    "default": default_models_dir,
                    "tooltip": "æœ¬åœ°æ¨¡å‹ç›®å½•ï¼ˆGGUF æ–‡ä»¶å­˜æ”¾ä½ç½®ï¼‰"
                }),
                "model_source": (["Remote (Nexa Service)", "Local (GGUF File)"], {
                    "default": "Remote (Nexa Service)",
                    "tooltip": "æ¨¡å‹æ¥æºï¼šè¿œç¨‹æœåŠ¡æˆ–æœ¬åœ°æ–‡ä»¶"
                }),
                "refresh_models": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
                }),
            },
            "optional": {
                "system_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("NEXA_MODEL", "STRING")
    RETURN_NAMES = ("model_config", "available_models")
    FUNCTION = "select_model"
    CATEGORY = "GGUF-VisionLM/Nexa"
    OUTPUT_NODE = True
    
    def select_model(
        self, 
        base_url: str, 
        models_dir: str,
        model_source: str,
        refresh_models: bool = False,
        system_prompt: str = ""
    ):
        """é€‰æ‹©æ¨¡å‹å¹¶è¿”å›é…ç½®"""
        
        # åˆ›å»ºæˆ–è·å–å¼•æ“
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        is_available = engine.is_service_available()
        
        if not is_available:
            error_msg = f"âš ï¸  Nexa SDK service is not available at {base_url}"
            print(error_msg)
            print("   Please make sure the service is running.")
            
            # å³ä½¿æœåŠ¡ä¸å¯ç”¨ï¼Œä¹Ÿè¿”å›é…ç½®ï¼ˆç”¨äºæœ¬åœ°æ¨¡å‹ï¼‰
            config = {
                "base_url": base_url,
                "models_dir": models_dir,
                "model_source": model_source,
                "system_prompt": system_prompt,
                "engine_type": "nexa",
                "service_available": False
            }
            return (config, error_msg)
        
        # è·å–å¯ç”¨æ¨¡å‹
        available_models_list = []
        
        if model_source == "Remote (Nexa Service)":
            # è¿œç¨‹æ¨¡å‹
            remote_models = engine.get_available_models(force_refresh=refresh_models)
            available_models_list.extend([f"[Remote] {m}" for m in remote_models])
        else:
            # æœ¬åœ°æ¨¡å‹
            local_models = engine.get_local_models()
            available_models_list.extend([f"[Local] {m}" for m in local_models])
        
        # æ ¼å¼åŒ–è¾“å‡º
        if available_models_list:
            models_text = "\n".join(available_models_list)
            print(f"âœ… Found {len(available_models_list)} models")
        else:
            models_text = "âš ï¸  No models found"
            print(models_text)
        
        # åˆ›å»ºé…ç½®
        config = {
            "base_url": base_url,
            "models_dir": models_dir,
            "model_source": model_source,
            "system_prompt": system_prompt,
            "engine_type": "nexa",
            "service_available": True,
            "available_models": available_models_list
        }
        
        print(f"âœ… Nexa SDK configured")
        print(f"   Service URL: {base_url}")
        print(f"   Models Dir: {models_dir}")
        print(f"   Source: {model_source}")
        
        return (config, models_text)


class NexaTextGeneration:
    """Nexa SDK æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_config": ("NEXA_MODEL", {
                    "tooltip": "Nexa æ¨¡å‹é…ç½®ï¼ˆæ¥è‡ª Model Selectorï¼‰"
                }),
                "model": ("STRING", {
                    "default": "",
                    "tooltip": "æ¨¡å‹åç§°ï¼ˆè¿œç¨‹æ¨¡å‹ ID æˆ–æœ¬åœ° .gguf æ–‡ä»¶åï¼‰"
                }),
                "prompt": ("STRING", {
                    "default": "Hello, how are you?",
                    "multiline": True,
                    "tooltip": "è¾“å…¥æç¤ºè¯"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 8192,
                    "step": 1,
                    "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p é‡‡æ ·"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "Top-k é‡‡æ ·ï¼ˆ0 è¡¨ç¤ºç¦ç”¨ï¼‰"
                }),
                "repetition_penalty": ("FLOAT", {
                    "default": 1.1,
                    "min": 1.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "é‡å¤æƒ©ç½š"
                }),
                "enable_thinking": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆæ”¯æŒ DeepSeek-R1, Qwen3-Thinking ç­‰æ¨¡å‹ï¼‰"
                }),
            },
            "optional": {
                "conversation_history": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "å¯¹è¯å†å²ï¼ˆJSON æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("context", "thinking", "raw_response")
    FUNCTION = "generate"
    CATEGORY = "GGUF-VisionLM/Nexa"
    OUTPUT_NODE = True
    
    @staticmethod
    def _extract_thinking(text: str, enable_thinking: bool) -> Tuple[str, str]:
        """
        ä»è¾“å‡ºä¸­æå–æ€è€ƒå†…å®¹
        
        æ”¯æŒå¤šç§æ ¼å¼:
        1. <think>...</think> (DeepSeek-R1)
        2. <thinking>...</thinking>
        3. [THINKING]...[/THINKING]
        
        Returns:
            (final_output, thinking_content)
        """
        if not enable_thinking:
            return text, ""
        
        # æ¨¡å¼ 1: <think>...</think>
        think_pattern = r'<think>(.*?)</think>'
        matches = re.findall(think_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            # ç§»é™¤æ€è€ƒæ ‡ç­¾ï¼Œä¿ç•™æœ€ç»ˆç­”æ¡ˆ
            final_output = re.sub(think_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ¨¡å¼ 2: <thinking>...</thinking>
        thinking_pattern = r'<thinking>(.*?)</thinking>'
        matches = re.findall(thinking_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(thinking_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ¨¡å¼ 3: [THINKING]...[/THINKING]
        bracket_pattern = r'\[THINKING\](.*?)\[/THINKING\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(bracket_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ²¡æœ‰æ‰¾åˆ°æ€è€ƒæ ‡è®°ï¼Œè¿”å›åŸæ–‡
        return text, ""
    
    def generate(
        self,
        model_config,
        model: str,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 40,
        repetition_penalty: float = 1.1,
        enable_thinking: bool = False,
        conversation_history: str = ""
    ):
        """ç”Ÿæˆæ–‡æœ¬"""
        
        # è·å–é…ç½®
        base_url = model_config.get('base_url', 'http://127.0.0.1:11434')
        models_dir = model_config.get('models_dir', '/workspace/ComfyUI/models/LLM')
        model_source = model_config.get('model_source', 'Remote (Nexa Service)')
        system_prompt = model_config.get('system_prompt', '')
        
        # è·å–å¼•æ“
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        if not engine.is_service_available():
            error_msg = f"âŒ Nexa SDK service is not available at {base_url}"
            print(error_msg)
            print("   Please make sure the service is running.")
            return (error_msg, "", "")
        
        # éªŒè¯æ¨¡å‹åç§°
        if not model:
            error_msg = "âŒ Please specify a model name"
            print(error_msg)
            return (error_msg, "", "")
        
        # å¤„ç†æ¨¡å‹è·¯å¾„
        if model_source == "Local (GGUF File)":
            # æœ¬åœ°æ¨¡å‹ï¼šç¡®ä¿æ˜¯ .gguf æ–‡ä»¶
            if not model.endswith('.gguf'):
                model = f"{model}.gguf"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_path = engine.get_model_path(model)
            if not os.path.exists(model_path):
                error_msg = f"âŒ Local model not found: {model_path}"
                print(error_msg)
                print(f"   Available models in {models_dir}:")
                for m in engine.get_local_models():
                    print(f"      - {m}")
                return (error_msg, "", "")
            
            print(f"ğŸ“ Using local model: {model_path}")
            model_id = model  # ä½¿ç”¨æ–‡ä»¶åï¼Œå¼•æ“ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
        else:
            # è¿œç¨‹æ¨¡å‹ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹ ID
            model_id = model
            print(f"ğŸŒ Using remote model: {model_id}")
        
        # å¤„ç†æ€è€ƒæ§åˆ¶
        if not enable_thinking and system_prompt:
            # å¦‚æœç¦ç”¨æ€è€ƒï¼Œæ·»åŠ  no_think åˆ°ç³»ç»Ÿæç¤ºè¯
            if 'no_think' not in system_prompt.lower():
                system_prompt = f"{system_prompt} no_think"
        elif not enable_thinking and not system_prompt:
            system_prompt = "no_think"
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        # 1. ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # 2. å¯¹è¯å†å²ï¼ˆå¦‚æœæä¾›ï¼‰
        if conversation_history:
            try:
                import json
                history = json.loads(conversation_history)
                if isinstance(history, list):
                    messages.extend(history)
            except:
                # å¦‚æœä¸æ˜¯ JSON æ ¼å¼ï¼Œä½œä¸ºæ™®é€šæ–‡æœ¬æ·»åŠ 
                messages.append({"role": "user", "content": conversation_history})
        
        # 3. å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": prompt})
        
        print(f"ğŸ¤– Generating text with Nexa SDK...")
        print(f"   Model: {model_id}")
        print(f"   Source: {model_source}")
        print(f"   Messages: {len(messages)} messages")
        if not enable_thinking:
            print(f"   ğŸš« Thinking disabled")
        
        try:
            # å‡†å¤‡å‚æ•°
            params = {
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": top_p,
            }
            
            # åªåœ¨éé›¶æ—¶æ·»åŠ  top_k å’Œ repetition_penalty
            if top_k > 0:
                params["top_k"] = top_k
            if repetition_penalty > 1.0:
                params["repetition_penalty"] = repetition_penalty
            
            # è°ƒç”¨ API
            response = engine.chat_completion(
                model=model_id,
                messages=messages,
                **params
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            raw_output = response['choices'][0]['message']['content']
            
            # æå–æ€è€ƒå†…å®¹
            final_output, thinking = self._extract_thinking(raw_output, enable_thinking)
            
            if enable_thinking and thinking:
                print(f"   ğŸ’­ Thinking process extracted ({len(thinking)} chars)")
            
            # æ ¼å¼åŒ–åŸå§‹å“åº”
            import json
            raw_response = json.dumps(response, indent=2, ensure_ascii=False)
            
            return (final_output, thinking, raw_response)
        
        except Exception as e:
            error_msg = f"âŒ Generation failed: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return (error_msg, "", str(e))


class NexaServiceStatus:
    """Nexa SDK æœåŠ¡çŠ¶æ€æ£€æŸ¥èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å– LLM æ¨¡å‹ç›®å½•
        if HAS_PATH_CONFIG:
            default_models_dir = PathConfig.get_llm_models_path()
        else:
            default_models_dir = "/workspace/ComfyUI/models/LLM"
        
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "Nexa SDK æœåŠ¡åœ°å€ï¼ˆå¯é…ç½®ï¼‰"
                }),
                "models_dir": ("STRING", {
                    "default": default_models_dir,
                    "tooltip": "æœ¬åœ°æ¨¡å‹ç›®å½•"
                }),
                "refresh": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("status", "remote_models", "local_models")
    FUNCTION = "check_status"
    CATEGORY = "GGUF-VisionLM/Nexa"
    OUTPUT_NODE = True
    
    def check_status(self, base_url: str, models_dir: str, refresh: bool = False):
        """æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
        
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        is_available = engine.is_service_available()
        
        status_lines = []
        status_lines.append(f"Nexa SDK Service: {base_url}")
        status_lines.append(f"Models Directory: {models_dir}")
        status_lines.append("")
        
        if is_available:
            # è·å–è¿œç¨‹æ¨¡å‹åˆ—è¡¨
            remote_models = engine.get_available_models(force_refresh=refresh)
            
            status_lines.append(f"âœ… Service is AVAILABLE")
            status_lines.append(f"Found {len(remote_models)} remote model(s)")
            
            remote_models_str = "\n".join([f"  - {model}" for model in remote_models]) if remote_models else "  (none)"
        else:
            status_lines.append(f"âŒ Service is NOT AVAILABLE")
            status_lines.append("Please make sure the service is running.")
            remote_models_str = "Service unavailable"
        
        # è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨
        local_models = engine.get_local_models()
        status_lines.append(f"Found {len(local_models)} local model(s)")
        
        local_models_str = "\n".join([f"  - {model}" for model in local_models]) if local_models else "  (none)"
        
        status = "\n".join(status_lines)
        
        print(status)
        print("\nRemote models:")
        print(remote_models_str)
        print("\nLocal models:")
        print(local_models_str)
        
        return (status, remote_models_str, local_models_str)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "NexaModelSelector": NexaModelSelector,
    "NexaTextGeneration": NexaTextGeneration,
    "NexaServiceStatus": NexaServiceStatus,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "NexaModelSelector": "ğŸ”· Nexa Model Selector",
    "NexaTextGeneration": "ğŸ”· Nexa Text Generation",
    "NexaServiceStatus": "ğŸ”· Nexa Service Status",
}
