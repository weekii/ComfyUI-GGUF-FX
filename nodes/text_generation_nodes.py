"""
æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹ - é‡æ„ç‰ˆ
æ”¯æŒæœ¬åœ° GGUF å’Œè¿œç¨‹ API ä¸¤ç§æ¨¡å¼
"""

import os
import sys
import re
from pathlib import Path
from typing import Tuple
import requests

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

from core.model_loader import ModelLoader
from core.inference_engine import InferenceEngine
from core.inference.unified_api_engine import get_unified_api_engine
from core.cache_manager import CacheManager
from core.registry_manager import RegistryManager
from utils.file_downloader import FileDownloader
from models.text_model_presets import TextModelPresets


class LocalTextModelLoader:
    """æœ¬åœ° GGUF æ–‡æœ¬æ¨¡å‹åŠ è½½å™¨"""
    
    # å…¨å±€å®ä¾‹
    _model_loader = None
    _cache_manager = None
    _registry = None
    
    @classmethod
    def _get_instances(cls):
        """è·å–å…¨å±€å®ä¾‹"""
        if cls._model_loader is None:
            cls._model_loader = ModelLoader()
        if cls._cache_manager is None:
            cls._cache_manager = CacheManager()
        if cls._registry is None:
            cls._registry = RegistryManager()
        return cls._model_loader, cls._cache_manager, cls._registry
    
    @classmethod
    def INPUT_TYPES(cls):
        loader, cache, registry = cls._get_instances()
        
        # è·å–æœ¬åœ°æ¨¡å‹
        all_local_models = loader.list_models()
        
        # è¿‡æ»¤æœ¬åœ°æ¨¡å‹ï¼šåªæ˜¾ç¤ºæ–‡æœ¬ç”Ÿæˆç±»å‹çš„æ¨¡å‹
        local_models = []
        
        # è§†è§‰æ¨¡å‹å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºæ’é™¤ï¼‰
        vision_keywords = [
            'llava', 'vision', 'vl', 'multimodal', 'mm', 
            'clip', 'qwen-vl', 'qwen2-vl', 'minicpm-v',
            'phi-3-vision', 'internvl', 'cogvlm'
        ]
        
        for model_file in all_local_models:
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«è§†è§‰æ¨¡å‹å…³é”®è¯
            model_lower = model_file.lower()
            is_vision_model = any(keyword in model_lower for keyword in vision_keywords)
            
            if is_vision_model:
                continue  # è·³è¿‡è§†è§‰æ¨¡å‹
            
            # æ£€æŸ¥ registry ä¸­çš„æ¨¡å‹ä¿¡æ¯
            model_info = registry.find_model_by_filename(model_file)
            # å¦‚æœæ‰¾åˆ°æ¨¡å‹ä¿¡æ¯ä¸”æ˜¯æ–‡æœ¬ç”Ÿæˆç±»å‹ï¼Œæˆ–è€…æ‰¾ä¸åˆ°ä¿¡æ¯ï¼ˆæœªçŸ¥æ¨¡å‹ï¼Œä¿ç•™ï¼‰
            if model_info is None or model_info.get('business_type') == 'text_generation':
                local_models.append(model_file)
        
        # è·å–å¯ä¸‹è½½çš„æ–‡æœ¬æ¨¡å‹
        downloadable = registry.get_downloadable_models(business_type='text_generation')
        downloadable_names = [name for name, _ in downloadable]
        
        # åˆå¹¶åˆ—è¡¨
        all_models = local_models + downloadable_names
        
        if not all_models:
            all_models = ["No models found"]
        
        return {
            "required": {
                "model": (all_models, {
                    "default": all_models[0] if all_models else "No models found",
                    "tooltip": "é€‰æ‹©æœ¬åœ° GGUF æ–‡æœ¬æ¨¡å‹"
                }),
                "n_ctx": ("INT", {
                    "default": 8192,
                    "min": 512,
                    "max": 128000,
                    "step": 512,
                    "tooltip": "ä¸Šä¸‹æ–‡çª—å£å¤§å°"
                }),
                "device": (["Auto", "GPU", "CPU"], {
                    "default": "Auto",
                    "tooltip": "è¿è¡Œè®¾å¤‡ (Auto=è‡ªåŠ¨æ£€æµ‹, GPU=å…¨éƒ¨GPU, CPU=ä»…CPU)"
                }),
            },
            "optional": {
                "system_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("TEXT_MODEL",)
    RETURN_NAMES = ("model_config",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸ¤– GGUF-Fusion/Text"
    
    def load_model(self, model, n_ctx=8192, device="Auto", system_prompt=""):
        """åŠ è½½æœ¬åœ° GGUF æ¨¡å‹"""
        loader, cache, registry = self._get_instances()
        
        print(f"\n{'='*80}")
        print(f" ğŸ–¥ï¸  Local Text Model Loader")
        print(f"{'='*80}")
        
        # æ ¹æ®è®¾å¤‡é€‰é¡¹è®¾ç½® n_gpu_layers
        if device == "Auto":
            try:
                import torch
                n_gpu_layers = -1 if torch.cuda.is_available() else 0
                print(f"ğŸ” Auto device: {'GPU' if n_gpu_layers == -1 else 'CPU'}")
            except:
                n_gpu_layers = -1  # é»˜è®¤å°è¯• GPU
        elif device == "GPU":
            n_gpu_layers = -1
            print(f"ğŸ® Using GPU (all layers)")
        else:  # CPU
            n_gpu_layers = 0
            print(f"ğŸ’» Using CPU only")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
        if model.startswith("[â¬‡ï¸"):
            print(f"ğŸ“¥ Model needs to be downloaded: {model}")
            download_info = registry.get_model_download_info(model)
            
            if download_info:
                downloader = FileDownloader()
                model_dir = loader.model_dirs[0]
                
                downloaded_path = downloader.download_from_huggingface(
                    repo_id=download_info['repo'],
                    filename=download_info['filename'],
                    dest_dir=model_dir
                )
                
                if downloaded_path:
                    model = download_info['filename']
                    cache.clear("new model downloaded")
                else:
                    error_msg = f"Failed to download model: {model}"
                    print(f"âŒ {error_msg}")
                    return ({"error": error_msg},)
            else:
                error_msg = f"Cannot find download info for: {model}"
                print(f"âŒ {error_msg}")
                return ({"error": error_msg},)
        
        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        model_path = loader.find_model(model)
        if not model_path:
            error_msg = f"Model not found: {model}"
            print(f"âŒ {error_msg}")
            return ({"error": error_msg},)
        
        # åº”ç”¨é¢„è®¾é…ç½®
        preset = TextModelPresets.get_preset(model)
        if preset:
            print(f"ğŸ“‹ Applying preset for {model}")
            if n_ctx == 8192:  # å¦‚æœæ˜¯é»˜è®¤å€¼ï¼Œä½¿ç”¨é¢„è®¾
                n_ctx = preset.get('n_ctx', n_ctx)
        
        # åˆ›å»ºé…ç½®
        config = {
            "mode": "local",
            "model_path": model_path,
            "model_name": model,
            "n_ctx": n_ctx,
            "n_gpu_layers": n_gpu_layers,
            "system_prompt": system_prompt
        }
        
        print(f"âœ… Local model configured")
        print(f"   Model: {model}")
        print(f"   Path: {model_path}")
        print(f"   Context: {n_ctx}")
        print(f"   Device: {device}")
        print(f"{'='*80}\n")
        
        return (config,)


class RemoteTextModelSelector:
    """è¿œç¨‹ API æ–‡æœ¬æ¨¡å‹é€‰æ‹©å™¨"""
    
    @staticmethod
    def get_ollama_models(base_url="http://127.0.0.1:11434"):
        """è·å– Ollama æ¨¡å‹åˆ—è¡¨"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„ç«¯å£
            ports = [11434, 11435]
            for port in ports:
                try:
                    url = f"http://127.0.0.1:{port}/api/tags"
                    response = requests.get(url, timeout=2)
                    if response.status_code == 200:
                        data = response.json()
                        models = [model['name'] for model in data.get('models', [])]
                        if models:
                            return models
                except:
                    continue
            return ["No models found"]
        except:
            return ["No models found"]
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å– Ollama è¿œç¨‹æ¨¡å‹åˆ—è¡¨
        ollama_models = cls.get_ollama_models()
        
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "multiline": False,
                    "tooltip": "API æœåŠ¡åœ°å€"
                }),
                "api_type": (["Ollama", "Nexa SDK", "OpenAI Compatible"], {
                    "default": "Ollama",
                    "tooltip": "API ç±»å‹ï¼ˆæ¨èä½¿ç”¨ Ollamaï¼‰"
                }),
                "model": (ollama_models, {
                    "default": ollama_models[0] if ollama_models else "No models found",
                    "tooltip": "è¿œç¨‹æ¨¡å‹åç§°ï¼ˆä» Ollama è‡ªåŠ¨è·å–ï¼‰"
                }),
            },
            "optional": {
                "system_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("TEXT_MODEL",)
    RETURN_NAMES = ("model_config",)
    FUNCTION = "select_model"
    CATEGORY = "ğŸ¤– GGUF-Fusion/Text"
    
    def select_model(self, base_url, api_type, model, system_prompt=""):
        """é€‰æ‹©è¿œç¨‹ API æ¨¡å‹"""
        print(f"\n{'='*80}")
        print(f" ğŸŒ Remote Text Model Selector")
        print(f"{'='*80}")
        
        # API ç±»å‹æ˜ å°„
        api_type_map = {
            "Ollama": "ollama",
            "Nexa SDK": "nexa",
            "OpenAI Compatible": "openai"
        }
        api_type_key = api_type_map.get(api_type, "ollama")
        
        # è·å– API å¼•æ“
        engine = get_unified_api_engine(base_url, api_type_key)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        if not engine.is_service_available():
            error_msg = f"âš ï¸  {api_type} service is not available at {base_url}"
            print(error_msg)
            print(f"   Please make sure the service is running.")
            
            config = {
                "mode": "remote",
                "base_url": base_url,
                "api_type": api_type_key,
                "service_available": False,
                "system_prompt": system_prompt,
                "error": error_msg
            }
            return (config,)
        
        # è·å–å¯ç”¨æ¨¡å‹
        available_models = engine.get_available_models(force_refresh=False)
        
        if available_models:
            print(f"âœ… Found {len(available_models)} models from {api_type}")
            for i, m in enumerate(available_models[:5], 1):
                print(f"   {i}. {m}")
            if len(available_models) > 5:
                print(f"   ... and {len(available_models) - 5} more")
        else:
            print(f"âš ï¸  No models found")
        
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        if model and model != "No models found":
            selected_model = model
        elif available_models:
            selected_model = available_models[0]
        else:
            selected_model = ""
        
        config = {
            "mode": "remote",
            "base_url": base_url,
            "api_type": api_type_key,
            "model_name": selected_model,
            "available_models": available_models,
            "service_available": True,
            "system_prompt": system_prompt
        }
        
        print(f"âœ… Remote API configured")
        print(f"   Type: {api_type}")
        print(f"   URL: {base_url}")
        print(f"   Model: {selected_model}")
        print(f"{'='*80}\n")
        
        return (config,)


class TextGeneration:
    """ç»Ÿä¸€çš„æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹ - è‡ªåŠ¨è¯†åˆ«æœ¬åœ°/è¿œç¨‹æ¨¡å¼"""
    
    # å…¨å±€æ¨ç†å¼•æ“
    _inference_engine = None
    
    @classmethod
    def _get_engine(cls):
        """è·å–æ¨ç†å¼•æ“"""
        if cls._inference_engine is None:
            cls._inference_engine = InferenceEngine()
        return cls._inference_engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_config": ("TEXT_MODEL", {
                    "tooltip": "æ¨¡å‹é…ç½®ï¼ˆæ¥è‡ª Local/Remote Model Loaderï¼‰"
                }),
                "prompt": ("STRING", {
                    "default": "Hello, how are you?",
                    "multiline": True,
                    "tooltip": "è¾“å…¥æç¤ºè¯"
                }),
                "max_tokens": ("INT", {
                    "default": 256,
                    "min": 1,
                    "max": 8192,
                    "step": 1,
                    "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p é‡‡æ ·"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "Top-k é‡‡æ ·"
                }),
                "repetition_penalty": ("FLOAT", {
                    "default": 1.1,
                    "min": 1.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "é‡å¤æƒ©ç½š"
                }),
                "enable_thinking": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆæ”¯æŒ DeepSeek-R1, Qwen3-Thinking ç­‰æ¨¡å‹ï¼‰"
                }),
            },
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("output", "thinking")
    FUNCTION = "generate"
    CATEGORY = "ğŸ¤– GGUF-Fusion/Text"
    OUTPUT_NODE = True
    
    @staticmethod
    def _extract_thinking(text: str, enable_thinking: bool) -> Tuple[str, str]:
        """
        ä»è¾“å‡ºä¸­æå–æ€è€ƒå†…å®¹
        
        æ”¯æŒå¤šç§æ ¼å¼:
        - <think>...</think> (DeepSeek-R1, Qwen3)
        - <thinking>...</thinking>
        - [THINKING]...[/THINKING]
        
        æ³¨æ„ï¼šå³ä½¿ç¦ç”¨ thinkingï¼Œä¹Ÿä¼šç§»é™¤ç©ºçš„æ€è€ƒæ ‡ç­¾å’Œå¤šä½™ç©ºè¡Œ
        """
        thinking = ""
        final_output = text
        
        # å°è¯•æå– <think> æ ‡ç­¾
        think_pattern = r'<think>(.*?)</think>'
        matches = re.findall(think_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(think_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking if enable_thinking else ""
        
        # å°è¯•æå– <thinking> æ ‡ç­¾
        thinking_pattern = r'<thinking>(.*?)</thinking>'
        matches = re.findall(thinking_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(thinking_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking if enable_thinking else ""
        
        # å°è¯•æå– [THINKING] æ ‡ç­¾
        bracket_pattern = r'\[THINKING\](.*?)\[/THINKING\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(bracket_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking if enable_thinking else ""
        
        # æ²¡æœ‰æ‰¾åˆ°æ€è€ƒæ ‡ç­¾ï¼Œä½†ä»éœ€æ¸…ç†ç©ºæ ‡ç­¾å’Œå¤šä½™ç©ºè¡Œ
        final_output = re.sub(r'<think>\s*</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        final_output = re.sub(r'<thinking>\s*</thinking>', '', final_output, flags=re.DOTALL | re.IGNORECASE)
        final_output = re.sub(r'\[THINKING\]\s*\[/THINKING\]', '', final_output, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆ3ä¸ªæˆ–æ›´å¤šè¿ç»­æ¢è¡Œç¬¦ï¼‰
        final_output = re.sub(r'\n{3,}', '\n\n', final_output)
        final_output = final_output.strip()
        
        return final_output, ""
    
    def generate(self, model_config, prompt, max_tokens=256, temperature=0.7, top_p=0.9, top_k=40, repetition_penalty=1.1, enable_thinking=False):
        """ç”Ÿæˆæ–‡æœ¬"""
        print("\n" + "="*80)
        print(" ğŸ¤– Text Generation")
        print("="*80)
        
        # æ£€æŸ¥é…ç½®é”™è¯¯
        if "error" in model_config:
            error_msg = model_config["error"]
            print(f"âŒ {error_msg}")
            return (error_msg, "")
        
        mode = model_config.get("mode", "local")
        system_prompt = model_config.get("system_prompt", "")
        
        # å¤„ç†æ€è€ƒæ§åˆ¶
        if not enable_thinking and system_prompt:
            if 'no_think' not in system_prompt.lower():
                system_prompt = f"{system_prompt} no_think"
                print(f"  ğŸš« Thinking disabled (added no_think)")
        elif not enable_thinking and not system_prompt:
            system_prompt = "no_think"
            print(f"  ğŸš« Thinking disabled")
        
        if mode == "local":
            # æœ¬åœ° GGUF æ¨¡å¼
            return self._generate_local(model_config, prompt, system_prompt, max_tokens, temperature, top_p, top_k, repetition_penalty, enable_thinking)
        else:
            # è¿œç¨‹ API æ¨¡å¼
            return self._generate_remote(model_config, prompt, system_prompt, max_tokens, temperature, top_p, top_k, repetition_penalty, enable_thinking)
    
    def _generate_local(self, model_config, prompt, system_prompt, max_tokens, temperature, top_p, top_k, repetition_penalty, enable_thinking):
        """æœ¬åœ° GGUF ç”Ÿæˆ"""
        engine = self._get_engine()
        model_path = model_config["model_path"]
        
        print(f"ğŸ–¥ï¸  Local GGUF Generation")
        print(f"   Model: {model_config['model_name']}")
        print(f"   Path: {model_path}")
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
        if not engine.is_model_loaded(model_path):
            print(f"\nâ³ Loading model...")
            success = engine.load_model(
                model_path=model_path,
                n_ctx=model_config.get('n_ctx', 8192),
                n_gpu_layers=model_config.get('n_gpu_layers', -1),
                verbose=False
            )
            if not success:
                error_msg = "âŒ Failed to load model"
                print(error_msg)
                return (error_msg, "")
            print(f"âœ… Model loaded")
        
        # æ„å»ºå®Œæ•´çš„ prompt
        full_prompt_parts = []
        
        if system_prompt:
            full_prompt_parts.append(f"System: {system_prompt}")
        
        full_prompt_parts.append(f"User: {prompt}")
        full_prompt_parts.append("Assistant:")
        
        full_prompt = "\n\n".join(full_prompt_parts)
        
        print(f"\nğŸ’¬ Generating...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        
        # è®¾ç½® stop åºåˆ—
        stop_sequences = ["User:", "System:", "\n\n\n", "\n\n##", "\n\nNote:", "\n\nThis "]
        
        try:
            raw_output = engine.generate_text(
                model_path=model_path,
                prompt=full_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repeat_penalty=repetition_penalty,
                stop=stop_sequences
            )
            
            # æå–æ€è€ƒå†…å®¹
            final_output, thinking = self._extract_thinking(raw_output, enable_thinking)
            
            # ç§»é™¤å¯èƒ½çš„ "Assistant:" å‰ç¼€
            if final_output.lower().startswith("assistant:"):
                final_output = final_output[10:].strip()
            
            final_output = final_output.strip()
            
            if enable_thinking and thinking:
                print(f"   ğŸ’­ Thinking extracted ({len(thinking)} chars)")
            
            print(f"   âœ… Generated {len(final_output)} characters")
            print("="*80 + "\n")
            
            return (final_output, thinking)
        
        except Exception as e:
            error_msg = f"âŒ Generation failed: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return (error_msg, "")
    
    def _generate_remote(self, model_config, prompt, system_prompt, max_tokens, temperature, top_p, top_k, repetition_penalty, enable_thinking):
        """è¿œç¨‹ API ç”Ÿæˆ"""
        base_url = model_config["base_url"]
        api_type = model_config["api_type"]
        model_name = model_config.get("model_name", "")
        
        if not model_config.get("service_available", False):
            error_msg = f"âŒ {api_type} service is not available"
            print(error_msg)
            return (error_msg, "")
        
        if not model_name:
            error_msg = "âŒ No model specified"
            print(error_msg)
            return (error_msg, "")
        
        print(f"ğŸŒ Remote API Generation")
        print(f"   Type: {api_type}")
        print(f"   URL: {base_url}")
        print(f"   Model: {model_name}")
        
        # è·å– API å¼•æ“
        engine = get_unified_api_engine(base_url, api_type)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        print(f"\nğŸ’¬ Generating...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        print(f"   Messages: {len(messages)}")
        
        try:
            response = engine.chat_completion(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            raw_output = response['choices'][0]['message']['content']
            
            # æå–æ€è€ƒå†…å®¹
            final_output, thinking = self._extract_thinking(raw_output, enable_thinking)
            
            # ç§»é™¤å¯èƒ½çš„ "Assistant:" å‰ç¼€
            for prefix in ["assistant:", "Assistant:", "ASSISTANT:"]:
                if final_output.startswith(prefix):
                    final_output = final_output[len(prefix):].strip()
                    break
            
            final_output = final_output.strip()
            
            if enable_thinking and thinking:
                print(f"   ğŸ’­ Thinking extracted ({len(thinking)} chars)")
            
            print(f"   âœ… Generated {len(final_output)} characters")
            print("="*80 + "\n")
            
            return (final_output, thinking)
        
        except Exception as e:
            error_msg = f"âŒ Generation failed: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return (error_msg, "")


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "LocalTextModelLoader": LocalTextModelLoader,
    "RemoteTextModelSelector": RemoteTextModelSelector,
    "TextGeneration": TextGeneration,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LocalTextModelLoader": "ğŸ¤– Local Text Model Loader",
    "RemoteTextModelSelector": "ğŸŒ Remote Text Model Selector",
    "TextGeneration": "ğŸ¤– Text Generation",
}
