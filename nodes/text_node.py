"""
Text Generation Node - æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹
"""

import os
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.model_loader import ModelLoader
    from core.inference_engine import InferenceEngine
    from core.cache_manager import CacheManager
    from utils.registry import RegistryManager
    from utils.downloader import FileDownloader
    from models.text_models import TextModelConfig, TextModelPresets
except ImportError as e:
    print(f"[ComfyUI-GGUF-VisionLM] Import error in text_node: {e}")
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..core.model_loader import ModelLoader
    from ..core.inference_engine import InferenceEngine
    from ..core.cache_manager import CacheManager
    from ..utils.registry import RegistryManager
    from ..utils.downloader import FileDownloader
    from ..models.text_models import TextModelConfig, TextModelPresets


class TextModelLoader:
    """æ–‡æœ¬æ¨¡å‹åŠ è½½å™¨èŠ‚ç‚¹"""
    
    # å…¨å±€å®ä¾‹
    _model_loader = None
    _cache_manager = None
    _registry = None
    
    @classmethod
    def _get_instances(cls):
        """è·å–å…¨å±€å®ä¾‹"""
        if cls._model_loader is None:
            cls._model_loader = ModelLoader()
        if cls._cache_manager is None:
            cls._cache_manager = CacheManager()
        if cls._registry is None:
            cls._registry = RegistryManager()
        return cls._model_loader, cls._cache_manager, cls._registry
    
    @classmethod
    def INPUT_TYPES(cls):
        loader, cache, registry = cls._get_instances()
        
        # è·å–æœ¬åœ°æ¨¡å‹
        all_local_models = loader.list_models()
        
        # è¿‡æ»¤æœ¬åœ°æ¨¡å‹ï¼šåªæ˜¾ç¤ºæ–‡æœ¬ç”Ÿæˆç±»å‹çš„æ¨¡å‹
        local_models = []
        for model_file in all_local_models:
            model_info = registry.find_model_by_filename(model_file)
            # å¦‚æœæ‰¾åˆ°æ¨¡å‹ä¿¡æ¯ä¸”æ˜¯æ–‡æœ¬ç”Ÿæˆç±»å‹ï¼Œæˆ–è€…æ‰¾ä¸åˆ°ä¿¡æ¯ï¼ˆæœªçŸ¥æ¨¡å‹ï¼Œä¿ç•™ï¼‰
            if model_info is None or model_info.get('business_type') == 'text_generation':
                local_models.append(model_file)
        
        # è·å–å¯ä¸‹è½½çš„æ–‡æœ¬æ¨¡å‹
        downloadable = registry.get_downloadable_models(business_type='text_generation')
        downloadable_names = [name for name, _ in downloadable]
        
        # åˆå¹¶åˆ—è¡¨
        all_models = local_models + downloadable_names
        
        if not all_models:
            all_models = ["No models found"]
        
        return {
            "required": {
                "model": (all_models, {
                    "default": all_models[0] if all_models else "No models found",
                    "tooltip": "é€‰æ‹©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹"
                }),
                "n_ctx": ("INT", {
                    "default": 8192,
                    "min": 512,
                    "max": 128000,
                    "step": 512,
                    "tooltip": "ä¸Šä¸‹æ–‡çª—å£å¤§å°"
                }),
                "device": (["Auto", "GPU", "CPU"], {
                    "default": "Auto",
                    "tooltip": "è¿è¡Œè®¾å¤‡ (Auto=è‡ªåŠ¨æ£€æµ‹, GPU=å…¨éƒ¨GPU, CPU=ä»…CPU)"
                }),
            },
            "optional": {
                "system_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("TEXT_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "GGUF-VisionLM/Text"
    
    def load_model(self, model, n_ctx=8192, device="Auto", system_prompt=""):
        """åŠ è½½æ–‡æœ¬æ¨¡å‹"""
        loader, cache, registry = self._get_instances()
        
        # æ ¹æ®è®¾å¤‡é€‰é¡¹è®¾ç½® n_gpu_layers
        if device == "Auto":
            # è‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœæœ‰ GPU åˆ™å…¨éƒ¨ä½¿ç”¨ï¼Œå¦åˆ™ CPU
            try:
                import torch
                n_gpu_layers = -1 if torch.cuda.is_available() else 0
                print(f"ğŸ” Auto device: {'GPU' if n_gpu_layers == -1 else 'CPU'}")
            except:
                n_gpu_layers = -1  # é»˜è®¤å°è¯• GPU
        elif device == "GPU":
            n_gpu_layers = -1
            print(f"ğŸ® Using GPU (all layers)")
        else:  # CPU
            n_gpu_layers = 0
            print(f"ğŸ’» Using CPU only")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
        if model.startswith("[â¬‡ï¸"):
            print(f"ğŸ“¥ Model needs to be downloaded: {model}")
            download_info = registry.get_model_download_info(model)
            
            if download_info:
                downloader = FileDownloader()
                model_dir = loader.model_dirs[0]
                
                downloaded_path = downloader.download_from_huggingface(
                    repo_id=download_info['repo'],
                    filename=download_info['filename'],
                    dest_dir=model_dir
                )
                
                if downloaded_path:
                    model = download_info['filename']
                    cache.clear("new model downloaded")
                else:
                    raise RuntimeError(f"Failed to download model: {model}")
            else:
                raise ValueError(f"Cannot find download info for: {model}")
        
        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        model_path = loader.find_model(model)
        if not model_path:
            raise FileNotFoundError(f"Model not found: {model}")
        
        # åº”ç”¨é¢„è®¾é…ç½®
        preset = TextModelPresets.get_preset(model)
        if preset:
            print(f"ğŸ“‹ Applying preset for {model}")
            if n_ctx == 8192:  # å¦‚æœæ˜¯é»˜è®¤å€¼ï¼Œä½¿ç”¨é¢„è®¾
                n_ctx = preset.get('n_ctx', n_ctx)
        
        # åˆ›å»ºé…ç½®
        config = TextModelConfig(
            model_name=model,
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            system_prompt=system_prompt if system_prompt else None
        )
        
        # éªŒè¯é…ç½®
        validation = config.validate()
        if not validation['valid']:
            raise ValueError(f"Invalid config: {validation['errors']}")
        
        print(f"âœ… Text model loaded: {model}")
        
        return (config.to_dict(),)


class TextGenerationNode:
    """æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹"""
    
    # å…¨å±€æ¨ç†å¼•æ“
    _inference_engine = None
    
    @classmethod
    def _get_engine(cls):
        """è·å–æ¨ç†å¼•æ“"""
        if cls._inference_engine is None:
            cls._inference_engine = InferenceEngine()
        return cls._inference_engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("TEXT_MODEL", {
                    "tooltip": "æ–‡æœ¬æ¨¡å‹é…ç½®"
                }),
                "prompt": ("STRING", {
                    "default": "Hello, how are you?",
                    "multiline": True,
                    "tooltip": "è¾“å…¥æç¤ºè¯"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 8192,
                    "step": 1,
                    "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p é‡‡æ ·"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "Top-k é‡‡æ ·"
                }),
                "repeat_penalty": ("FLOAT", {
                    "default": 1.1,
                    "min": 1.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "é‡å¤æƒ©ç½š"
                }),
                "enable_thinking": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆæ”¯æŒ DeepSeek-R1, Qwen3-Thinking ç­‰æ¨¡å‹ï¼‰"
                }),
            },
            "optional": {
                "conversation_history": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("context", "thinking")
    FUNCTION = "generate"
    CATEGORY = "GGUF-VisionLM/Text"
    OUTPUT_NODE = True
    
    @staticmethod
    def _extract_thinking(text: str, enable_thinking: bool) -> tuple:
        """
        ä»è¾“å‡ºä¸­æå–æ€è€ƒå†…å®¹
        
        æ”¯æŒå¤šç§æ ¼å¼:
        1. <think>...</think> (DeepSeek-R1)
        2. <thinking>...</thinking>
        3. [THINKING]...[/THINKING]
        
        Returns:
            (final_output, thinking_content)
        """
        if not enable_thinking:
            return text, ""
        
        import re
        
        # æ¨¡å¼ 1: <think>...</think>
        think_pattern = r'<think>(.*?)</think>'
        matches = re.findall(think_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            # ç§»é™¤æ€è€ƒæ ‡ç­¾ï¼Œä¿ç•™æœ€ç»ˆç­”æ¡ˆ
            final_output = re.sub(think_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ¨¡å¼ 2: <thinking>...</thinking>
        thinking_pattern = r'<thinking>(.*?)</thinking>'
        matches = re.findall(thinking_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(thinking_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ¨¡å¼ 3: [THINKING]...[/THINKING]
        bracket_pattern = r'\[THINKING\](.*?)\[/THINKING\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            thinking = '\n\n'.join(matches)
            final_output = re.sub(bracket_pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
            return final_output, thinking
        
        # æ²¡æœ‰æ‰¾åˆ°æ€è€ƒæ ‡è®°ï¼Œè¿”å›åŸæ–‡
        return text, ""
    
    def generate(self, model, prompt, max_tokens=512, temperature=0.7, top_p=0.9, top_k=40, repeat_penalty=1.1, enable_thinking=False, conversation_history=""):
        """ç”Ÿæˆæ–‡æœ¬"""
        engine = self._get_engine()
        
        model_path = model['model_path']
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
        if not engine.is_model_loaded(model_path):
            print(f" Loading model into memory...")
            success = engine.load_model(
                model_path=model_path,
                n_ctx=model.get('n_ctx', 8192),
                n_gpu_layers=model.get('n_gpu_layers', -1),
                verbose=model.get('verbose', False)
            )
            
            if not success:
                raise RuntimeError(f"Failed to load model: {model_path}")
        
        # å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œæ€è€ƒæ§åˆ¶
        system_prompt_text = model.get('system_prompt', '')
        
        # å¦‚æœç¦ç”¨æ€è€ƒï¼Œæ·»åŠ  no_think åˆ°ç³»ç»Ÿæç¤ºè¯
        if not enable_thinking:
            if system_prompt_text:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ no_think
                if 'no_think' not in system_prompt_text.lower():
                    system_prompt_text = f"{system_prompt_text} no_think"
            else:
                # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæç¤ºè¯ä½†ç¦ç”¨æ€è€ƒï¼Œåˆ›å»ºä¸€ä¸ª
                system_prompt_text = "no_think"
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        full_prompt_parts = []
        
        # 1. ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if system_prompt_text:
            full_prompt_parts.append(f"System: {system_prompt_text}")
        
        # 2. å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if conversation_history:
            full_prompt_parts.append(conversation_history)
        
        # 3. å½“å‰ç”¨æˆ·è¾“å…¥
        full_prompt_parts.append(f"User: {prompt}")
        full_prompt_parts.append("Assistant:")
        
        full_prompt = "\n\n".join(full_prompt_parts)
        
        print(f"ğŸ¤– Generating text...")
        print(f"ğŸ“ Prompt preview: {full_prompt[:150]}...")
        if not enable_thinking:
            print(f"ğŸš« Thinking disabled (no_think in system prompt)")
        
        # ç”Ÿæˆæ–‡æœ¬
        try:
            raw_output = engine.generate_text(
                model_path=model_path,
                prompt=full_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repeat_penalty=repeat_penalty
            )
            
            # æå–æ€è€ƒå†…å®¹
            final_output, thinking = self._extract_thinking(raw_output, enable_thinking)
            
            if enable_thinking and thinking:
                print(f" Thinking process extracted ({len(thinking)} chars)")
            
            return (final_output, thinking)
        
        except Exception as e:
            error_msg = f"Generation failed: {str(e)}"
            print(f" {error_msg}")
            return (error_msg, "")


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "TextModelLoader": TextModelLoader,
    "TextGenerationNode": TextGenerationNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "TextModelLoader": "ğŸ“ Text Model Loader",
    "TextGenerationNode": "ğŸ¤– Text Generation",
}
