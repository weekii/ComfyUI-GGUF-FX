"""
Vision Language Node - è§†è§‰è¯­è¨€æ¨¡å‹èŠ‚ç‚¹
"""

import os
import sys
import uuid
import numpy as np
from pathlib import Path
from PIL import Image
import folder_paths

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.model_loader import ModelLoader
    from core.inference_engine import InferenceEngine
    from core.cache_manager import CacheManager
    from utils.registry import RegistryManager
    from utils.downloader import FileDownloader
    from models.vision_models import VisionModelConfig, VisionModelPresets
    from utils.device_optimizer import DeviceOptimizer
    from utils.mmproj_validator import MMProjValidator
except ImportError as e:
    print(f"[ComfyUI-GGUF-VisionLM] Import error in vision_node: {e}")
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..core.model_loader import ModelLoader
    from ..core.inference_engine import InferenceEngine
    from ..core.cache_manager import CacheManager
    from ..utils.registry import RegistryManager
    from ..utils.downloader import FileDownloader
    from ..models.vision_models import VisionModelConfig, VisionModelPresets
    from ..utils.device_optimizer import DeviceOptimizer
    from ..utils.mmproj_finder import MMProjFinder


class VisionModelLoader:
    """è§†è§‰è¯­è¨€æ¨¡å‹åŠ è½½å™¨èŠ‚ç‚¹"""
    
    # å…¨å±€å®ä¾‹
    _model_loader = None
    _cache_manager = None
    _registry = None
    _device_optimizer = None
    
    @classmethod
    def _get_instances(cls):
        """è·å–å…¨å±€å®ä¾‹"""
        if cls._model_loader is None:
            cls._model_loader = ModelLoader()
        if cls._cache_manager is None:
            cls._cache_manager = CacheManager()
        if cls._registry is None:
            cls._registry = RegistryManager()
        if cls._device_optimizer is None:
            cls._device_optimizer = DeviceOptimizer()
        return cls._model_loader, cls._cache_manager, cls._registry, cls._device_optimizer
    
    @classmethod
    def INPUT_TYPES(cls):
        loader, cache, registry, optimizer = cls._get_instances()
        
        # è·å–æœ¬åœ°æ¨¡å‹
        all_local_models = loader.list_models()
        
        # è¿‡æ»¤æœ¬åœ°æ¨¡å‹ï¼šåªæ˜¾ç¤ºè§†è§‰è¯­è¨€ç±»å‹çš„æ¨¡å‹
        local_models = []
        for model_file in all_local_models:
            model_info = registry.find_model_by_filename(model_file)
            # å¦‚æœæ‰¾åˆ°æ¨¡å‹ä¿¡æ¯ä¸”æ˜¯å›¾åƒ/è§†é¢‘åˆ†æç±»å‹ï¼Œæˆ–è€…æ‰¾ä¸åˆ°ä¿¡æ¯ï¼ˆæœªçŸ¥æ¨¡å‹ï¼Œä¿ç•™ï¼‰
            if model_info is None or model_info.get('business_type') in ['image_analysis', 'video_analysis']:
                local_models.append(model_file)
        
        # è·å–ä¸åŒç±»å‹çš„å¯ä¸‹è½½æ¨¡å‹
        image_models = registry.get_downloadable_models(business_type='image_analysis')
        video_models = registry.get_downloadable_models(business_type='video_analysis')
        
        # æ·»åŠ ç±»å‹æ ‡ç­¾
        categorized_models = []
        
        # æ·»åŠ åˆ†ç»„æ ‡é¢˜
        if image_models:
            categorized_models.append("--- ğŸ–¼ï¸ å›¾åƒåˆ†ææ¨¡å‹ ---")
            categorized_models.extend([name for name, _ in image_models])
        
        if video_models:
            categorized_models.append("--- ğŸ¥ è§†é¢‘åˆ†ææ¨¡å‹ ---")
            categorized_models.extend([name for name, _ in video_models])
        
        if local_models:
            categorized_models.append("--- ğŸ’¾ æœ¬åœ°æ¨¡å‹ ---")
            categorized_models.extend(local_models)
        
        if not categorized_models:
            categorized_models = ["No models found"]
        
        return {
            "required": {
                "model": (categorized_models, {
                    "default": categorized_models[0] if categorized_models else "No models found",
                    "tooltip": "ğŸ¤– é€‰æ‹©è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼‰"
                }),
                "n_ctx": ("INT", {
                    "default": 8192,
                    "min": 512,
                    "max": 32768,
                    "step": 512,
                    "tooltip": "ğŸ¤– ä¸Šä¸‹æ–‡çª—å£å¤§å°"
                }),
                "device": (["Auto", "GPU", "CPU"], {
                    "default": "ğŸ¤– Auto",
                    "tooltip": "ğŸ¤– è¿è¡Œè®¾å¤‡ (Auto=è‡ªåŠ¨æ£€æµ‹, GPU=å…¨éƒ¨GPU, CPU=ä»…CPU)"
                }),
            },
            "optional": {
                "mmproj_file": ("STRING", {
                    "default": "",
                    "tooltip": "ğŸ¤– æ‰‹åŠ¨æŒ‡å®š mmproj æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("VISION_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸ¤– GGUF-LLM/Vision"
    
    def load_model(self, model, n_ctx=8192, device="Auto", mmproj_file=""):
        """åŠ è½½è§†è§‰è¯­è¨€æ¨¡å‹"""
        loader, cache, registry, optimizer = self._get_instances()
        
        # æ£€æŸ¥ llama-cpp-python å®‰è£…çŠ¶æ€
        llama_status = optimizer.check_llama_cpp_installation()
        if not llama_status['installed']:
            error_msg = "âŒ llama-cpp-python not installed.\n"
            error_msg += "Install with:\n"
            error_msg += "  pip install llama-cpp-python\n"
            error_msg += "\nFor CUDA support, see: /home/README_LLAMA_CPP_INSTALL.md"
            raise RuntimeError(error_msg)
        
        if llama_status['issues']:
            for issue in llama_status['issues']:
                print(f"âš ï¸  {issue}")
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        device_summary = optimizer.get_device_summary()
        print(f"\n{device_summary}\n")
        
        # æ ¹æ®è®¾å¤‡é€‰é¡¹è®¾ç½®å‚æ•°
        if device == "Auto":
            # ä½¿ç”¨æ™ºèƒ½ä¼˜åŒ–
            optimized_params = optimizer.get_optimized_params(model_size_gb=7.0)
            n_gpu_layers = optimized_params['n_gpu_layers']
            n_batch = optimized_params.get('n_batch', 512)
            
            print(f"ğŸ¯ Auto-optimized: {optimized_params['device_info']}")
            print(f"   GPU layers: {n_gpu_layers}")
            print(f"   Batch size: {n_batch}")
        elif device == "GPU":
            n_gpu_layers = -1
            n_batch = 512
            print(f"ğŸ® Using GPU (all layers)")
        else:  # CPU
            n_gpu_layers = 0
            n_batch = 128
            print(f"ğŸ’» Using CPU only")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ç»„æ ‡é¢˜
        if model.startswith("---"):
            raise ValueError("è¯·é€‰æ‹©ä¸€ä¸ªå…·ä½“çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯åˆ†ç»„æ ‡é¢˜")
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
        if model.startswith("[â¬‡ï¸"):
            print(f"ğŸ“¥ Model needs to be downloaded: {model}")
            download_info = registry.get_model_download_info(model)
            
            if download_info:
                downloader = FileDownloader()
                model_dir = loader.model_dirs[0]
                
                # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
                downloaded_path = downloader.download_from_huggingface(
                    repo_id=download_info['repo'],
                    filename=download_info['filename'],
                    dest_dir=model_dir
                )
                
                if not downloaded_path:
                    raise RuntimeError(f"Failed to download model: {model}")
                
                # ä¸‹è½½ mmproj æ–‡ä»¶
                if download_info.get('mmproj'):
                    # ä½¿ç”¨ mmproj_repoï¼ˆå¦‚æœæŒ‡å®šï¼‰æˆ–é»˜è®¤ä½¿ç”¨æ¨¡å‹ä»“åº“
                    mmproj_repo = download_info.get('mmproj_repo', download_info['repo'])
                    mmproj_downloaded = downloader.download_from_huggingface(
                        repo_id=mmproj_repo,
                        filename=download_info['mmproj'],
                        dest_dir=model_dir
                    )
                    if mmproj_downloaded:
                        mmproj_file = download_info['mmproj']
                        print(f"âœ… Downloaded mmproj from {mmproj_repo}")
                
                model = download_info['filename']
                cache.clear("new model downloaded")
            else:
                raise ValueError(f"Cannot find download info for: {model}")
        
        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        model_path = loader.find_model(model)
        if not model_path:
            raise FileNotFoundError(f"Model not found: {model}")
        
        # æŸ¥æ‰¾ mmproj æ–‡ä»¶
        mmproj_path = None
        if mmproj_file:
            # æ‰‹åŠ¨æŒ‡å®š
            mmproj_path = loader.find_mmproj(model, mmproj_file)
            if not mmproj_path:
                raise FileNotFoundError(f"mmproj file not found: {mmproj_file}")
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾ - ç›´æ¥ä½¿ç”¨æ™ºèƒ½æŸ¥æ‰¾å™¨
            print(f"ğŸ” Auto-searching for mmproj file...")
            mmproj_path = loader.find_mmproj(model)  # ä¸ä¼  mmproj_nameï¼Œè®©å®ƒè‡ªåŠ¨æŸ¥æ‰¾
            
            if not mmproj_path:
                # å°è¯•é€šè¿‡ registry æŸ¥æ‰¾
                mmproj_name = registry.smart_match_mmproj(model)
                if mmproj_name:
                    mmproj_path = loader.find_mmproj(model, mmproj_name)
                
                if not mmproj_path:
                    print(f"âš ï¸  mmproj not found locally, attempting auto-download...")
                    # å°è¯•è‡ªåŠ¨ä¸‹è½½
                    model_info = registry.find_model_by_filename(model)
                    if model_info and model_info.get('mmproj'):
                        downloader = FileDownloader()
                        model_dir = os.path.dirname(model_path)
                        mmproj_path = downloader.download_from_huggingface(
                            repo_id=model_info['repo'],
                            filename=model_info['mmproj'],
                            dest_dir=model_dir
                        )
        
        if not mmproj_path:
            # ä½¿ç”¨æ™ºèƒ½æŸ¥æ‰¾å™¨å’ŒéªŒè¯å™¨
            from ..utils.mmproj_finder import MMProjFinder
            from ..utils.mmproj_validator import MMProjValidator
            
            finder = MMProjFinder([os.path.dirname(model_path)])
            validator = MMProjValidator()
            
            # è·å–å»ºè®®
            suggestions = validator.suggest_mmproj_for_model(model)
            available = finder.list_all_mmproj_files(os.path.dirname(model_path))
            
            error_msg = f"âŒ Could not find mmproj file for {model}.\n\n"
            
            # æ˜¾ç¤ºä¸»è¦å»ºè®®
            error_msg += f"ğŸ’¡ Recommended mmproj filename:\n"
            error_msg += f"   {suggestions['primary']}\n\n"
            
            if suggestions['notes']:
                error_msg += f"ğŸ“ Note: {suggestions['notes']}\n\n"
            
            # æ˜¾ç¤ºå¯ç”¨æ–‡ä»¶å¹¶æ£€æŸ¥å…¼å®¹æ€§
            if available:
                error_msg += f"ğŸ“ Available mmproj files in model directory:\n"
                for mmproj_path_item in available:
                    mmproj_name = os.path.basename(mmproj_path_item)
                    
                    # æ£€æŸ¥å…¼å®¹æ€§
                    compat = validator.check_compatibility(model, mmproj_name)
                    
                    if compat['confidence'] == 'high':
                        error_msg += f"   âœ… {mmproj_name} (æ¨èä½¿ç”¨)\n"
                    elif compat['confidence'] == 'medium':
                        error_msg += f"   âš ï¸  {mmproj_name} (å¯èƒ½å…¼å®¹)\n"
                    else:
                        error_msg += f"   âŒ {mmproj_name} (å¯èƒ½ä¸å…¼å®¹)\n"
                    
                    # æ˜¾ç¤ºè­¦å‘Š
                    if compat['warnings']:
                        for warning in compat['warnings']:
                            error_msg += f"      âš ï¸  {warning}\n"
                
                error_msg += "\n"
            
            error_msg += "âš ï¸  é‡è¦æç¤º:\n"
            error_msg += "   - mmproj æ–‡ä»¶å¿…é¡»ä¸æ¨¡å‹çš„è§†è§‰ç¼–ç å™¨åŒ¹é…\n"
            error_msg += "   - ä¸åŒç‰ˆæœ¬çš„æ¨¡å‹å¯èƒ½éœ€è¦ä¸åŒçš„ mmproj æ–‡ä»¶\n"
            error_msg += "   - ä½¿ç”¨ä¸åŒ¹é…çš„ mmproj ä¼šå¯¼è‡´å¼ é‡é”™è¯¯\n\n"
            
            error_msg += "è§£å†³æ–¹æ¡ˆ:\n"
            error_msg += "1. ä¸‹è½½ä¸æ¨¡å‹åŒ¹é…çš„ mmproj æ–‡ä»¶\n"
            error_msg += "2. å¦‚æœæœ‰æ¨èçš„æ–‡ä»¶ï¼Œé‡å‘½åä¸ºæ¨èçš„æ–‡ä»¶å\n"
            error_msg += "3. åœ¨èŠ‚ç‚¹ä¸­æ‰‹åŠ¨æŒ‡å®š mmproj_file å‚æ•°\n"
            
            raise FileNotFoundError(error_msg)
        
        # åº”ç”¨é¢„è®¾é…ç½®
        preset = VisionModelPresets.get_preset(model)
        if preset:
            print(f"ğŸ“‹ Applying preset for {model}")
            if n_ctx == 8192:  # å¦‚æœæ˜¯é»˜è®¤å€¼ï¼Œä½¿ç”¨é¢„è®¾
                n_ctx = preset.get('n_ctx', n_ctx)
        
        # åˆ›å»ºé…ç½®
        config = VisionModelConfig(
            model_name=model,
            model_path=model_path,
            mmproj_path=mmproj_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers
        )
        
        # éªŒè¯é…ç½®
        validation = config.validate()
        if not validation['valid']:
            raise ValueError(f"Invalid config: {validation['errors']}")
        
        print(f"âœ… Vision model loaded: {model}")
        print(f"ğŸ“ Using mmproj: {os.path.basename(mmproj_path)}")
        
        return (config.to_dict(),)


class VisionLanguageNode:
    """è§†è§‰è¯­è¨€ç”ŸæˆèŠ‚ç‚¹"""
    
    # å…¨å±€æ¨ç†å¼•æ“
    _inference_engine = None
    
    @classmethod
    def _get_engine(cls):
        """è·å–æ¨ç†å¼•æ“"""
        if cls._inference_engine is None:
            cls._inference_engine = InferenceEngine()
        return cls._inference_engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("VISION_MODEL", {
                    "tooltip": "ğŸ¤– è§†è§‰è¯­è¨€æ¨¡å‹é…ç½®"
                }),
                "prompt": ("STRING", {
                    "default": "ğŸ¤– Describe this image in detail.",
                    "multiline": True,
                    "tooltip": "ğŸ¤– ç”¨æˆ·æç¤ºè¯"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 4096,
                    "step": 1,
                    "tooltip": "ğŸ¤– æœ€å¤§ç”Ÿæˆ token æ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ğŸ¤– æ¸©åº¦å‚æ•°"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "ğŸ¤– Top-p é‡‡æ ·"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "ğŸ¤– Top-k é‡‡æ ·"
                }),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xFFFFFFFFFFFFFFFF,
                    "tooltip": "ğŸ¤– éšæœºç§å­"
                }),
            },
            "optional": {
                "image": ("IMAGE", {
                    "tooltip": "ğŸ¤– è¾“å…¥å›¾åƒï¼ˆä¸è§†é¢‘äºŒé€‰ä¸€ï¼‰"
                }),
                "video": ("IMAGE", {
                    "tooltip": "ğŸ¤– è¾“å…¥è§†é¢‘å¸§åºåˆ—ï¼ˆä¸å›¾åƒäºŒé€‰ä¸€ï¼‰"
                }),
                "system_prompt": ("STRING", {
                    "default": "ğŸ¤– You are a helpful assistant that describes images and videos accurately and in detail.",
                    "multiline": True,
                    "tooltip": "ğŸ¤– ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯è‡ªå®šä¹‰æ¨¡å‹è¡Œä¸ºï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("context",)
    FUNCTION = "describe_image"
    CATEGORY = "ğŸ¤– GGUF-LLM/Vision"
    OUTPUT_NODE = True
    
    def describe_image(self, model, prompt, max_tokens=512, 
                      temperature=0.7, top_p=0.9, top_k=40, seed=0,
                      image=None, video=None, system_prompt=None):
        """ç”Ÿæˆå›¾åƒ/è§†é¢‘æè¿°"""
        try:
            from llama_cpp import Llama
            from llama_cpp.llama_chat_format import Qwen25VLChatHandler
            
            # éªŒè¯è¾“å…¥ï¼šå¿…é¡»æä¾›å›¾åƒæˆ–è§†é¢‘ä¹‹ä¸€
            if image is None and video is None:
                raise ValueError("å¿…é¡»æä¾› image æˆ– video è¾“å…¥ä¹‹ä¸€")
            if image is not None and video is not None:
                raise ValueError("ä¸èƒ½åŒæ—¶æä¾› image å’Œ video è¾“å…¥ï¼Œè¯·åªé€‰æ‹©ä¸€ä¸ª")
            
            engine = self._get_engine()
            model_path = model['model_path']
            mmproj_path = model['mmproj_path']
            
            # ç¡®å®šè¾“å…¥ç±»å‹
            is_video = video is not None
            input_data = video if is_video else image
            
            print(f"ğŸ“Š è¾“å…¥ç±»å‹: {'è§†é¢‘' if is_video else 'å›¾åƒ'}")
            if is_video:
                print(f"ğŸ¬ è§†é¢‘å¸§æ•°: {input_data.shape[0]}")
            
            # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
            if not engine.is_model_loaded(model_path):
                print(f"ğŸ”„ Loading vision model into memory...")
                print(f"ğŸ“ Model: {os.path.basename(model_path)}")
                print(f"ğŸ“ mmproj: {os.path.basename(mmproj_path)}")
                
                chat_handler = Qwen25VLChatHandler(clip_model_path=mmproj_path)
                llm = Llama(
                    model_path=model_path,
                    chat_handler=chat_handler,
                    n_ctx=model.get('n_ctx', 8192),
                    n_gpu_layers=model.get('n_gpu_layers', -1),
                    verbose=model.get('verbose', False),
                    seed=seed
                )
                
                engine.loaded_models[model_path] = llm
                print(f"âœ… Vision model loaded successfully")
            else:
                llm = engine.loaded_models[model_path]
            
            # å¤„ç†å›¾åƒæˆ–è§†é¢‘å¸§
            if is_video:
                # è§†é¢‘ï¼šä¿å­˜å¤šä¸ªå¸§
                image_paths = self._save_video_frames(input_data, seed)
            else:
                # å›¾åƒï¼šä¿å­˜å•å¸§
                image_paths = [self._save_temp_image(input_data, seed)]
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = []
            
            # æ·»åŠ å›¾åƒ/è§†é¢‘å¸§
            for img_path in image_paths:
                content.append({
                    "type": "ğŸ¤– image_url",
                    "image_url": {"url": f"file://{img_path}"}
                })
            
            # æ·»åŠ ç”¨æˆ·æç¤ºè¯
            content.append({
                "type": "ğŸ¤– text",
                "text": prompt
            })
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
                print(f"ğŸ“‹ ç³»ç»Ÿæç¤ºè¯: {system_prompt[:50]}...")
            
            messages.append({"role": "user", "content": content})
            
            print(f"ğŸ¤– Generating {'video' if is_video else 'image'} description...")
            print(f"ğŸ“ ç”¨æˆ·æç¤ºè¯: {prompt[:50]}...")
            
            # ç”Ÿæˆæè¿°
            response = llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                stream=False
            )
            
            output_text = response["choices"][0]["message"]["content"]
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for img_path in image_paths:
                try:
                    os.remove(img_path)
                except:
                    pass
            
            print(f"âœ… Generated description ({len(output_text)} chars)")
            return (str(output_text),)
        
        except ImportError as e:
            error_msg = "âŒ llama-cpp-python not installed. Install with: pip install llama-cpp-python"
            print(error_msg)
            return (error_msg,)
        
        except Exception as e:
            import traceback
            error_msg = f"âŒ Error: {str(e)}"
            print(f"âŒ Detailed error:\n{traceback.format_exc()}")
            return (error_msg,)
    
    def _save_temp_image(self, image, seed):
        """ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶"""
        unique_id = uuid.uuid4().hex
        image_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}_{unique_id}.png"
        image_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢ tensor åˆ° PIL Image
        img_array = image.cpu().numpy()
        if img_array.ndim == 4:
            img_array = img_array[0]  # å–ç¬¬ä¸€å¼ å›¾åƒ
        img_array = np.clip(255.0 * img_array, 0, 255).astype(np.uint8)
        img = Image.fromarray(img_array)
        img.save(str(image_path))
        
        return str(image_path.resolve())
    
    def _save_video_frames(self, video, seed, max_frames=8):
        """ä¿å­˜è§†é¢‘å¸§åˆ°ä¸´æ—¶æ–‡ä»¶"""
        unique_id = uuid.uuid4().hex
        temp_dir = Path(folder_paths.temp_directory) / f"temp_video_{seed}_{unique_id}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢ tensor åˆ° PIL Images
        video_array = video.cpu().numpy()
        num_frames = video_array.shape[0]
        
        print(f"ğŸ¬ å¤„ç†è§†é¢‘: {num_frames} å¸§")
        
        # é‡‡æ ·å¸§ï¼ˆå¦‚æœå¸§æ•°å¤ªå¤šï¼‰
        if num_frames > max_frames:
            indices = np.linspace(0, num_frames - 1, max_frames, dtype=int)
            video_array = video_array[indices]
            print(f"ğŸ“Š é‡‡æ ·åˆ° {max_frames} å¸§")
        
        image_paths = []
        for i, frame in enumerate(video_array):
            img_array = np.clip(255.0 * frame, 0, 255).astype(np.uint8)
            img = Image.fromarray(img_array)
            
            frame_path = temp_dir / f"frame_{i:04d}.png"
            img.save(str(frame_path))
            image_paths.append(str(frame_path.resolve()))
        
        print(f"âœ… ä¿å­˜äº† {len(image_paths)} ä¸ªè§†é¢‘å¸§")
        return image_paths


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "VisionModelLoader": VisionModelLoader,
    "VisionLanguageNode": VisionLanguageNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VisionModelLoader": "ğŸ¤– Vision Model Loader",
    "VisionLanguageNode": "ğŸ¤– Vision Language Generation",
}
