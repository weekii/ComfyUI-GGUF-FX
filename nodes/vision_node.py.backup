"""
Vision Language Node - 视觉语言模型节点
"""

import os
import sys
import uuid
import numpy as np
from pathlib import Path
from PIL import Image
import folder_paths

# 添加父目录到路径
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.model_loader import ModelLoader
    from core.inference_engine import InferenceEngine
    from core.cache_manager import CacheManager
    from utils.registry import RegistryManager
    from utils.downloader import FileDownloader
    from models.vision_models import VisionModelConfig, VisionModelPresets
except ImportError as e:
    print(f"[ComfyUI-GGUF-VisionLM] Import error in vision_node: {e}")
    # 尝试相对导入
    from ..core.model_loader import ModelLoader
    from ..core.inference_engine import InferenceEngine
    from ..core.cache_manager import CacheManager
    from ..utils.registry import RegistryManager
    from ..utils.downloader import FileDownloader
    from ..models.vision_models import VisionModelConfig, VisionModelPresets


class VisionModelLoader:
    """视觉语言模型加载器节点"""
    
    # 全局实例
    _model_loader = None
    _cache_manager = None
    _registry = None
    
    @classmethod
    def _get_instances(cls):
        """获取全局实例"""
        if cls._model_loader is None:
            cls._model_loader = ModelLoader()
        if cls._cache_manager is None:
            cls._cache_manager = CacheManager()
        if cls._registry is None:
            cls._registry = RegistryManager()
        return cls._model_loader, cls._cache_manager, cls._registry
    
    @classmethod
    def INPUT_TYPES(cls):
        loader, cache, registry = cls._get_instances()
        
        # 获取本地模型
        all_local_models = loader.list_models()
        
        # 过滤本地模型：只显示视觉语言类型的模型
        local_models = []
        for model_file in all_local_models:
            model_info = registry.find_model_by_filename(model_file)
            # 如果找到模型信息且是图像/视频分析类型，或者找不到信息（未知模型，保留）
            if model_info is None or model_info.get('business_type') in ['image_analysis', 'video_analysis']:
                local_models.append(model_file)
        
        # 获取不同类型的可下载模型
        image_models = registry.get_downloadable_models(business_type='image_analysis')
        video_models = registry.get_downloadable_models(business_type='video_analysis')
        
        # 添加类型标签
        categorized_models = []
        
        # 添加分组标题
        if image_models:
            categorized_models.append("--- 🖼️ 图像分析模型 ---")
            categorized_models.extend([name for name, _ in image_models])
        
        if video_models:
            categorized_models.append("--- 🎥 视频分析模型 ---")
            categorized_models.extend([name for name, _ in video_models])
        
        if local_models:
            categorized_models.append("--- 💾 本地模型 ---")
            categorized_models.extend(local_models)
        
        if not categorized_models:
            categorized_models = ["No models found"]
        
        return {
            "required": {
                "model": (categorized_models, {
                    "default": categorized_models[0] if categorized_models else "No models found",
                    "tooltip": "选择视觉语言模型（按类型分组）"
                }),
                "n_ctx": ("INT", {
                    "default": 8192,
                    "min": 512,
                    "max": 32768,
                    "step": 512,
                    "tooltip": "上下文窗口大小"
                }),
                "device": (["Auto", "GPU", "CPU"], {
                    "default": "Auto",
                    "tooltip": "运行设备 (Auto=自动检测, GPU=全部GPU, CPU=仅CPU)"
                }),
            },
            "optional": {
                "mmproj_file": ("STRING", {
                    "default": "",
                    "tooltip": "手动指定 mmproj 文件（可选）"
                }),
            }
        }
    
    RETURN_TYPES = ("VISION_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "GGUF-VisionLM/Vision"
    
    def load_model(self, model, n_ctx=8192, device="Auto", mmproj_file=""):
        """加载视觉语言模型"""
        loader, cache, registry = self._get_instances()
        
        # 根据设备选项设置 n_gpu_layers
        if device == "Auto":
            # 自动检测：如果有 GPU 则全部使用，否则 CPU
            try:
                import torch
                n_gpu_layers = -1 if torch.cuda.is_available() else 0
                print(f"🔍 Auto device: {'GPU' if n_gpu_layers == -1 else 'CPU'}")
            except:
                n_gpu_layers = -1  # 默认尝试 GPU
        elif device == "GPU":
            n_gpu_layers = -1
            print(f"🎮 Using GPU (all layers)")
        else:  # CPU
            n_gpu_layers = 0
            print(f"💻 Using CPU only")
        
        # 检查是否是分组标题
        if model.startswith("---"):
            raise ValueError("请选择一个具体的模型，而不是分组标题")
        
        print(f"📦 加载模型: {model}")
        
        # 检查是否需要下载
        if model.startswith("[⬇️"):
            print(f"📥 Model needs to be downloaded: {model}")
            download_info = registry.get_model_download_info(model)
            
            if download_info:
                downloader = FileDownloader()
                model_dir = loader.model_dirs[0]
                
                # 下载模型文件
                downloaded_path = downloader.download_from_huggingface(
                    repo_id=download_info['repo'],
                    filename=download_info['filename'],
                    dest_dir=model_dir
                )
                
                if not downloaded_path:
                    raise RuntimeError(f"Failed to download model: {model}")
                
                # 下载 mmproj 文件
                if download_info.get('mmproj'):
                    # 使用 mmproj_repo（如果指定）或默认使用模型仓库
                    mmproj_repo = download_info.get('mmproj_repo', download_info['repo'])
                    mmproj_downloaded = downloader.download_from_huggingface(
                        repo_id=mmproj_repo,
                        filename=download_info['mmproj'],
                        dest_dir=model_dir
                    )
                    if mmproj_downloaded:
                        mmproj_file = download_info['mmproj']
                        print(f"✅ Downloaded mmproj from {mmproj_repo}")
                
                model = download_info['filename']
                cache.clear("new model downloaded")
            else:
                raise ValueError(f"Cannot find download info for: {model}")
        
        # 查找模型路径
        model_path = loader.find_model(model)
        if not model_path:
            raise FileNotFoundError(f"Model not found: {model}")
        
        # 查找 mmproj 文件
        mmproj_path = None
        if mmproj_file:
            # 手动指定
            mmproj_path = loader.find_mmproj(model, mmproj_file)
            if not mmproj_path:
                raise FileNotFoundError(f"mmproj file not found: {mmproj_file}")
        else:
            # 自动查找
            mmproj_name = registry.smart_match_mmproj(model)
            if mmproj_name:
                mmproj_path = loader.find_mmproj(model, mmproj_name)
                
                if not mmproj_path:
                    print(f"⚠️  mmproj not found locally, attempting auto-download...")
                    # 尝试自动下载
                    model_info = registry.find_model_by_filename(model)
                    if model_info and model_info.get('mmproj'):
                        downloader = FileDownloader()
                        model_dir = os.path.dirname(model_path)
                        mmproj_path = downloader.download_from_huggingface(
                            repo_id=model_info['repo'],
                            filename=model_info['mmproj'],
                            dest_dir=model_dir
                        )
        
        if not mmproj_path:
            raise FileNotFoundError(
                f"Could not find mmproj file for {model}.\n"
                f"Please either:\n"
                f"1. Download the mmproj file to the same directory as the model\n"
                f"2. Specify mmproj_file manually\n"
            )
        
        # 应用预设配置
        preset = VisionModelPresets.get_preset(model)
        if preset:
            print(f"📋 Applying preset for {model}")
            if n_ctx == 8192:  # 如果是默认值，使用预设
                n_ctx = preset.get('n_ctx', n_ctx)
        
        # 创建配置
        config = VisionModelConfig(
            model_name=model,
            model_path=model_path,
            mmproj_path=mmproj_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers
        )
        
        # 验证配置
        validation = config.validate()
        if not validation['valid']:
            raise ValueError(f"Invalid config: {validation['errors']}")
        
        print(f"✅ Vision model loaded: {model}")
        print(f"📁 Using mmproj: {os.path.basename(mmproj_path)}")
        
        return (config.to_dict(),)


class VisionLanguageNode:
    """视觉语言生成节点"""
    
    # 全局推理引擎
    _inference_engine = None
    
    @classmethod
    def _get_engine(cls):
        """获取推理引擎"""
        if cls._inference_engine is None:
            cls._inference_engine = InferenceEngine()
        return cls._inference_engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("VISION_MODEL", {
                    "tooltip": "视觉语言模型配置"
                }),
                "prompt": ("STRING", {
                    "default": "Describe this image in detail.",
                    "multiline": True,
                    "tooltip": "用户提示词"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 4096,
                    "step": 1,
                    "tooltip": "最大生成 token 数"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "温度参数"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p 采样"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "Top-k 采样"
                }),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xFFFFFFFFFFFFFFFF,
                    "tooltip": "随机种子"
                }),
            },
            "optional": {
                "image": ("IMAGE", {
                    "tooltip": "输入图像（与视频二选一）"
                }),
                "video": ("IMAGE", {
                    "tooltip": "输入视频帧序列（与图像二选一）"
                }),
                "system_prompt": ("STRING", {
                    "default": "You are a helpful assistant that describes images and videos accurately and in detail.",
                    "multiline": True,
                    "tooltip": "系统提示词（可自定义模型行为）"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("description",)
    FUNCTION = "describe_image"
    CATEGORY = "GGUF-VisionLM/Vision"
    OUTPUT_NODE = True
    
    def describe_image(self, model, prompt, max_tokens=512, 
                      temperature=0.7, top_p=0.9, top_k=40, seed=0,
                      image=None, video=None, system_prompt=None):
        """生成图像/视频描述"""
        try:
            from llama_cpp import Llama
            from llama_cpp.llama_chat_format import Qwen25VLChatHandler
            
            # 验证输入：必须提供图像或视频之一
            if image is None and video is None:
                raise ValueError("必须提供 image 或 video 输入之一")
            if image is not None and video is not None:
                raise ValueError("不能同时提供 image 和 video 输入，请只选择一个")
            
            engine = self._get_engine()
            model_path = model['model_path']
            mmproj_path = model['mmproj_path']
            
            # 确定输入类型
            is_video = video is not None
            input_data = video if is_video else image
            
            print(f"📊 输入类型: {'视频' if is_video else '图像'}")
            if is_video:
                print(f"🎬 视频帧数: {input_data.shape[0]}")
            
            # 加载模型（如果未加载）
            if not engine.is_model_loaded(model_path):
                print(f"🔄 Loading vision model into memory...")
                print(f"📁 Model: {os.path.basename(model_path)}")
                print(f"📁 mmproj: {os.path.basename(mmproj_path)}")
                
                chat_handler = Qwen25VLChatHandler(clip_model_path=mmproj_path)
                llm = Llama(
                    model_path=model_path,
                    chat_handler=chat_handler,
                    n_ctx=model.get('n_ctx', 8192),
                    n_gpu_layers=model.get('n_gpu_layers', -1),
                    verbose=model.get('verbose', False),
                    seed=seed
                )
                
                engine.loaded_models[model_path] = llm
                print(f"✅ Vision model loaded successfully")
            else:
                llm = engine.loaded_models[model_path]
            
            # 处理图像或视频帧
            if is_video:
                # 视频：保存多个帧
                image_paths = self._save_video_frames(input_data, seed)
            else:
                # 图像：保存单帧
                image_paths = [self._save_temp_image(input_data, seed)]
            
            # 构建消息内容
            content = []
            
            # 添加图像/视频帧
            for img_path in image_paths:
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"file://{img_path}"}
                })
            
            # 添加用户提示词
            content.append({
                "type": "text",
                "text": prompt
            })
            
            # 构建消息列表
            messages = []
            
            # 添加系统提示词（如果提供）
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
                print(f"📋 系统提示词: {system_prompt[:50]}...")
            
            messages.append({"role": "user", "content": content})
            
            print(f"🤖 Generating {'video' if is_video else 'image'} description...")
            print(f"📝 用户提示词: {prompt[:50]}...")
            
            # 生成描述
            response = llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                stream=False
            )
            
            output_text = response["choices"][0]["message"]["content"]
            
            # 清理临时文件
            for img_path in image_paths:
                try:
                    os.remove(img_path)
                except:
                    pass
            
            print(f"✅ Generated description ({len(output_text)} chars)")
            return (str(output_text),)
        
        except ImportError as e:
            error_msg = "❌ llama-cpp-python not installed. Install with: pip install llama-cpp-python"
            print(error_msg)
            return (error_msg,)
        
        except Exception as e:
            import traceback
            error_msg = f"❌ Error: {str(e)}"
            print(f"❌ Detailed error:\n{traceback.format_exc()}")
            return (error_msg,)
    
    def _save_temp_image(self, image, seed):
        """保存图像到临时文件"""
        unique_id = uuid.uuid4().hex
        image_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}_{unique_id}.png"
        image_path.parent.mkdir(parents=True, exist_ok=True)
        
        # 转换 tensor 到 PIL Image
        img_array = image.cpu().numpy()
        if img_array.ndim == 4:
            img_array = img_array[0]  # 取第一张图像
        img_array = np.clip(255.0 * img_array, 0, 255).astype(np.uint8)
        img = Image.fromarray(img_array)
        img.save(str(image_path))
        
        return str(image_path.resolve())
    
    def _save_video_frames(self, video, seed, max_frames=8):
        """保存视频帧到临时文件"""
        unique_id = uuid.uuid4().hex
        temp_dir = Path(folder_paths.temp_directory) / f"temp_video_{seed}_{unique_id}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        # 转换 tensor 到 PIL Images
        video_array = video.cpu().numpy()
        num_frames = video_array.shape[0]
        
        print(f"🎬 处理视频: {num_frames} 帧")
        
        # 采样帧（如果帧数太多）
        if num_frames > max_frames:
            indices = np.linspace(0, num_frames - 1, max_frames, dtype=int)
            video_array = video_array[indices]
            print(f"📊 采样到 {max_frames} 帧")
        
        image_paths = []
        for i, frame in enumerate(video_array):
            img_array = np.clip(255.0 * frame, 0, 255).astype(np.uint8)
            img = Image.fromarray(img_array)
            
            frame_path = temp_dir / f"frame_{i:04d}.png"
            img.save(str(frame_path))
            image_paths.append(str(frame_path.resolve()))
        
        print(f"✅ 保存了 {len(image_paths)} 个视频帧")
        return image_paths


# 节点注册
NODE_CLASS_MAPPINGS = {
    "VisionModelLoader": VisionModelLoader,
    "VisionLanguageNode": VisionLanguageNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VisionModelLoader": "👁️ Vision Model Loader",
    "VisionLanguageNode": "🖼️ Vision Language Generation",
}
