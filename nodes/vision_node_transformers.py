"""
Vision Node (Transformers Mode) - Transformers æ¨¡å¼çš„è§†è§‰è¯­è¨€æ¨¡å‹èŠ‚ç‚¹
æ”¯æŒ Qwen3-VL ç­‰å®Œæ•´çš„ Transformers æ¨¡å‹ï¼ˆä½¿ç”¨æœ€æ–° APIï¼‰
"""

import os
import sys
import torch
from pathlib import Path
from PIL import Image
from torchvision.transforms import ToPILImage
import folder_paths

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.inference.transformers_engine import TransformersInferenceEngine
    from utils.system_prompts import SystemPromptsManager
    from config.node_definitions import (
        SEED_INPUT,
        TEMPERATURE_INPUT,
        TOP_P_INPUT,
        TOP_K_INPUT,
        REPETITION_PENALTY_INPUT,
        PROMPT_INPUT,
        SYSTEM_PROMPT_INPUT,
        TRANSFORMERS_QUANTIZATION_INPUT,
        TRANSFORMERS_ATTENTION_INPUT,
        TRANSFORMERS_PIXELS_INPUT,
        KEEP_MODEL_LOADED_INPUT,
        TEXT_OUTPUT,
        TRANSFORMERS_MODEL_OUTPUT,
        merge_inputs
    )
except ImportError:
    from ..core.inference.transformers_engine import TransformersInferenceEngine
    from ..utils.system_prompts import SystemPromptsManager
    from ..config.node_definitions import (
        SEED_INPUT,
        TEMPERATURE_INPUT,
        TOP_P_INPUT,
        TOP_K_INPUT,
        REPETITION_PENALTY_INPUT,
        PROMPT_INPUT,
        SYSTEM_PROMPT_INPUT,
        TRANSFORMERS_QUANTIZATION_INPUT,
        TRANSFORMERS_ATTENTION_INPUT,
        TRANSFORMERS_PIXELS_INPUT,
        KEEP_MODEL_LOADED_INPUT,
        TEXT_OUTPUT,
        TRANSFORMERS_MODEL_OUTPUT,
        merge_inputs
    )


class VisionModelLoaderTransformers:
    """Transformers æ¨¡å¼çš„è§†è§‰æ¨¡å‹åŠ è½½å™¨"""
    
    # å…¨å±€å¼•æ“å®ä¾‹
    _engine = None
    
    @classmethod
    def _get_engine(cls):
        """è·å–å…¨å±€å¼•æ“å®ä¾‹"""
        if cls._engine is None:
            cls._engine = TransformersInferenceEngine()
        return cls._engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": merge_inputs(
                {
                    "model": (
                        [
                            "Huihui-Qwen3-VL-4B-Instruct-abliterated",
                            "Huihui-Qwen3-VL-8B-Instruct-abliterated",
                            "Qwen3-VL-4B-Instruct-FP8",
                            "Qwen3-VL-4B-Thinking-FP8",
                            "Qwen3-VL-8B-Instruct-FP8",
                            "Qwen3-VL-8B-Thinking-FP8",
                            "Qwen3-VL-4B-Instruct",
                            "Qwen3-VL-4B-Thinking",
                            "Qwen3-VL-8B-Instruct",
                            "Qwen3-VL-8B-Thinking"
                        ],
                        {
                            "default": "Huihui-Qwen3-VL-4B-Instruct-abliterated",
                            "tooltip": "é€‰æ‹© Qwen3-VL æ¨¡å‹"
                        }
                    ),
                },
                TRANSFORMERS_QUANTIZATION_INPUT,
                TRANSFORMERS_ATTENTION_INPUT,
                KEEP_MODEL_LOADED_INPUT,
                TRANSFORMERS_PIXELS_INPUT
            )
        }
    
    RETURN_TYPES = TRANSFORMERS_MODEL_OUTPUT["types"]
    RETURN_NAMES = TRANSFORMERS_MODEL_OUTPUT["names"]
    FUNCTION = "load_model"
    CATEGORY = "ğŸ¤– GGUF-Fusion/Transformers"
    
    def load_model(
        self,
        model,
        quantization,
        attention,
        keep_model_loaded,
        min_pixels,
        max_pixels
    ):
        """åŠ è½½ Transformers æ¨¡å‹"""
        
        # ç¡®å®šæ¨¡å‹ ID
        if model == "Huihui-Qwen3-VL-8B-Instruct-abliterated":
            model_id = "huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"
        else:
            model_id = f"qwen/{model}"
        
        # æ„å»ºé…ç½®
        config = {
            "model_name": model,
            "model_id": model_id,
            "quantization": quantization,
            "attention": attention,
            "min_pixels": min_pixels,
            "max_pixels": max_pixels,
            "keep_loaded": keep_model_loaded,
        }
        
        # åŠ è½½æ¨¡å‹
        engine = self._get_engine()
        success = engine.load_model(config)
        
        if not success:
            raise RuntimeError(f"Failed to load model: {model}")
        
        print(f"âœ… Transformers model loaded: {model}")
        
        return (config,)


class VisionLanguageNodeTransformers:
    """Transformers æ¨¡å¼çš„è§†è§‰è¯­è¨€èŠ‚ç‚¹ï¼ˆQwen3-VL ä¼˜åŒ–ï¼‰"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": merge_inputs(
                {
                    "model_config": ("TRANSFORMERS_MODEL",),
                },
                PROMPT_INPUT,
                TEMPERATURE_INPUT,
                TOP_P_INPUT,
                TOP_K_INPUT,
                REPETITION_PENALTY_INPUT,
                {
                    "max_tokens": (
                        "INT",
                        {
                            "default": 2048,
                            "min": 128,
                            "max": 256000,
                            "step": 1,
                            "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆQwen3-VL æ¨è: 16384ï¼‰"
                        }
                    ),
                },
                SEED_INPUT
            ),
            "optional": merge_inputs(
                {
                    "image": ("IMAGE",),
                },
                SYSTEM_PROMPT_INPUT
            )
        }
    
    RETURN_TYPES = TEXT_OUTPUT["types"]
    RETURN_NAMES = TEXT_OUTPUT["names"]
    FUNCTION = "generate"
    CATEGORY = "ğŸ¤– GGUF-Fusion/Transformers"
    OUTPUT_NODE = True
    
    def generate(
        self,
        model_config,
        prompt,
        temperature,
        top_p,
        top_k,
        repetition_penalty,
        max_tokens,
        seed,
        image=None,
        system_prompt=""
    ):
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨ Qwen3-VL æ–° APIï¼‰"""
        
        engine = VisionModelLoaderTransformers._get_engine()
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if engine.model is None or engine.processor is None:
            print("âš ï¸  Model not loaded, loading now...")
            success = engine.load_model(model_config)
            if not success:
                raise RuntimeError(f"Failed to load model: {model_config.get('model_name', 'unknown')}")
        
        # å‡†å¤‡å›¾åƒ
        temp_path = None
        if image is not None:
            pil_image = ToPILImage()(image[0].permute(2, 0, 1))
            temp_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}.png"
            pil_image.save(temp_path)
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆQwen3-VL æ ¼å¼ï¼‰
        messages = []
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
        user_content = []
        
        if temp_path:
            user_content.append({
                "type": "image",
                "image": str(temp_path)
            })
        
        # å°†ç³»ç»Ÿæç¤ºè¯åˆå¹¶åˆ°ç”¨æˆ·æ–‡æœ¬ä¸­
        if system_prompt and system_prompt.strip():
            final_text = f"{system_prompt.strip()}\n\n{prompt}"
        else:
            # ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤ºè¯
            default_prompt = SystemPromptsManager.get_preset("default")
            final_text = f"{default_prompt}\n\n{prompt}"
        
        user_content.append({
            "type": "text",
            "text": final_text
        })
        
        messages.append({
            "role": "user",
            "content": user_content
        })
        
        # æ‰§è¡Œæ¨ç†ï¼ˆä½¿ç”¨ Qwen3-VL æ¨èå‚æ•°ï¼‰
        try:
            result = engine.inference(
                messages=messages,
                temperature=temperature,
                max_new_tokens=max_tokens,
                seed=seed,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty
            )
            
            print(f"âœ… Generated text ({len(result)} chars)")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_path and temp_path.exists():
                temp_path.unlink()
            
            # å¦‚æœä¸ä¿æŒåŠ è½½ï¼Œå¸è½½æ¨¡å‹
            if not model_config.get("keep_loaded", False):
                engine.unload()
            
            return (result,)
            
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            import traceback
            traceback.print_exc()
            raise


# å¯¼å‡ºèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VisionModelLoaderTransformers": VisionModelLoaderTransformers,
    "VisionLanguageNodeTransformers": VisionLanguageNodeTransformers,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VisionModelLoaderTransformers": "ğŸ¤– Vision Model Loader (Transformers)",
    "VisionLanguageNodeTransformers": "ğŸ¤– Vision Language (Transformers)",
}
