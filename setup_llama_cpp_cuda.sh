#!/bin/bash

################################################################################
# llama-cpp-python CUDA å®‰è£…è„šæœ¬
# é€‚ç”¨äº Vast.ai å’Œå…¶ä»– GPU ä¸»æœº
# æ”¯æŒè‡ªåŠ¨æ£€æµ‹ CUDA ç‰ˆæœ¬å’Œ GPU å‹å·
################################################################################

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ‰“å°å‡½æ•°
print_header() {
    echo -e "${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

################################################################################
# 1. æ£€æµ‹ç¯å¢ƒ
################################################################################

print_header "æ£€æµ‹ç³»ç»Ÿç¯å¢ƒ"

# æ£€æµ‹ Python ç‰ˆæœ¬
if command -v python3 &> /dev/null; then
    PYTHON_VERSION=$(python3 --version | awk '{print $2}')
    print_success "Python ç‰ˆæœ¬: $PYTHON_VERSION"
else
    print_error "Python3 æœªå®‰è£…"
    exit 1
fi

# æ£€æµ‹è™šæ‹Ÿç¯å¢ƒ
if [ -n "$VIRTUAL_ENV" ]; then
    print_success "è™šæ‹Ÿç¯å¢ƒ: $VIRTUAL_ENV"
    PYTHON_CMD="python3"
    PIP_CMD="pip"
elif [ -d "/venv/main" ]; then
    print_info "æ£€æµ‹åˆ° /venv/mainï¼Œå°†æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ"
    source /venv/main/bin/activate
    PYTHON_CMD="python3"
    PIP_CMD="pip"
else
    print_warning "æœªæ£€æµ‹åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œä½¿ç”¨ç³»ç»Ÿ Python"
    PYTHON_CMD="python3"
    PIP_CMD="pip3"
fi

# æ£€æµ‹ CUDA
if command -v nvidia-smi &> /dev/null; then
    print_success "NVIDIA é©±åŠ¨å·²å®‰è£…"
    
    # è·å– CUDA ç‰ˆæœ¬
    CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | awk '{print $9}')
    print_info "CUDA ç‰ˆæœ¬: $CUDA_VERSION"
    
    # è·å– GPU å‹å·
    GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
    print_info "GPU: $GPU_NAME"
    
    # æ£€æµ‹è®¡ç®—èƒ½åŠ›
    if [[ "$GPU_NAME" == *"H100"* ]]; then
        COMPUTE_CAP="90"
        print_info "æ£€æµ‹åˆ° H100ï¼Œè®¡ç®—èƒ½åŠ›: 9.0"
    elif [[ "$GPU_NAME" == *"A100"* ]]; then
        COMPUTE_CAP="80"
        print_info "æ£€æµ‹åˆ° A100ï¼Œè®¡ç®—èƒ½åŠ›: 8.0"
    elif [[ "$GPU_NAME" == *"RTX 4090"* ]] || [[ "$GPU_NAME" == *"RTX 4080"* ]]; then
        COMPUTE_CAP="89"
        print_info "æ£€æµ‹åˆ° RTX 40 ç³»åˆ—ï¼Œè®¡ç®—èƒ½åŠ›: 8.9"
    elif [[ "$GPU_NAME" == *"RTX 3090"* ]] || [[ "$GPU_NAME" == *"RTX 3080"* ]]; then
        COMPUTE_CAP="86"
        print_info "æ£€æµ‹åˆ° RTX 30 ç³»åˆ—ï¼Œè®¡ç®—èƒ½åŠ›: 8.6"
    else
        COMPUTE_CAP="80"
        print_warning "æœªè¯†åˆ«çš„ GPUï¼Œä½¿ç”¨é»˜è®¤è®¡ç®—èƒ½åŠ›: 8.0"
    fi
else
    print_error "æœªæ£€æµ‹åˆ° NVIDIA GPU"
    exit 1
fi

# æ£€æµ‹ CUDA è·¯å¾„
if [ -d "/usr/local/cuda" ]; then
    CUDA_HOME="/usr/local/cuda"
    print_success "CUDA è·¯å¾„: $CUDA_HOME"
elif [ -d "/usr/local/cuda-12.6" ]; then
    CUDA_HOME="/usr/local/cuda-12.6"
    print_success "CUDA è·¯å¾„: $CUDA_HOME"
elif [ -d "/usr/local/cuda-12" ]; then
    CUDA_HOME="/usr/local/cuda-12"
    print_success "CUDA è·¯å¾„: $CUDA_HOME"
else
    print_error "æœªæ‰¾åˆ° CUDA å®‰è£…è·¯å¾„"
    exit 1
fi

################################################################################
# 2. æ£€æŸ¥ç°æœ‰å®‰è£…
################################################################################

print_header "æ£€æŸ¥ç°æœ‰ llama-cpp-python å®‰è£…"

if $PYTHON_CMD -c "import llama_cpp" 2>/dev/null; then
    CURRENT_VERSION=$($PYTHON_CMD -c "import llama_cpp; print(llama_cpp.__version__ if hasattr(llama_cpp, '__version__') else '0.3.x')" 2>/dev/null || echo "æœªçŸ¥")
    print_info "å½“å‰ç‰ˆæœ¬: $CURRENT_VERSION"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ CUDA æ”¯æŒ
    HAS_CUDA=$($PYTHON_CMD << 'EOF'
import llama_cpp
import os
lib_path = os.path.join(os.path.dirname(llama_cpp.__file__), 'lib')
if os.path.exists(lib_path):
    for f in os.listdir(lib_path):
        if 'cuda' in f.lower() and f.endswith('.so'):
            print("yes")
            exit(0)
print("no")
EOF
)
    
    if [ "$HAS_CUDA" = "yes" ]; then
        print_success "å·²æœ‰ CUDA æ”¯æŒ"
        read -p "æ˜¯å¦é‡æ–°å®‰è£…? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            print_info "ä¿æŒç°æœ‰å®‰è£…"
            exit 0
        fi
    else
        print_warning "å½“å‰ç‰ˆæœ¬æ—  CUDA æ”¯æŒï¼Œéœ€è¦é‡æ–°å®‰è£…"
    fi
else
    print_info "llama-cpp-python æœªå®‰è£…"
fi

################################################################################
# 3. å¸è½½ç°æœ‰ç‰ˆæœ¬
################################################################################

print_header "å¸è½½ç°æœ‰ç‰ˆæœ¬"

$PIP_CMD uninstall -y llama-cpp-python 2>/dev/null || true
print_success "å¸è½½å®Œæˆ"

################################################################################
# 4. å®‰è£… CUDA ç‰ˆæœ¬
################################################################################

print_header "å®‰è£… llama-cpp-python (CUDA æ”¯æŒ)"

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_HOME=$CUDA_HOME
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

print_info "ç¼–è¯‘å‚æ•°:"
print_info "  CUDA_HOME: $CUDA_HOME"
print_info "  è®¡ç®—èƒ½åŠ›: $COMPUTE_CAP"

# ä»æºç ç¼–è¯‘å®‰è£…
print_info "å¼€å§‹ç¼–è¯‘ï¼ˆè¿™å¯èƒ½éœ€è¦ 5-10 åˆ†é’Ÿï¼‰..."

CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=$COMPUTE_CAP" \
$PIP_CMD install llama-cpp-python --no-cache-dir --verbose 2>&1 | tee /tmp/llama_cpp_install.log

if [ ${PIPESTATUS[0]} -eq 0 ]; then
    print_success "å®‰è£…æˆåŠŸ"
else
    print_error "å®‰è£…å¤±è´¥ï¼ŒæŸ¥çœ‹æ—¥å¿—: /tmp/llama_cpp_install.log"
    exit 1
fi

################################################################################
# 5. éªŒè¯å®‰è£…
################################################################################

print_header "éªŒè¯ CUDA æ”¯æŒ"

VERIFICATION=$($PYTHON_CMD << 'EOF'
import llama_cpp
import os

lib_path = os.path.join(os.path.dirname(llama_cpp.__file__), 'lib')
cuda_found = False
cuda_size = 0

if os.path.exists(lib_path):
    for f in os.listdir(lib_path):
        if 'cuda' in f.lower() and f.endswith('.so'):
            cuda_found = True
            cuda_size = os.path.getsize(os.path.join(lib_path, f)) / (1024*1024)
            print(f"CUDAåº“: {f}")
            print(f"å¤§å°: {cuda_size:.1f} MB")

if cuda_found:
    print("çŠ¶æ€: æˆåŠŸ")
else:
    print("çŠ¶æ€: å¤±è´¥")
EOF
)

echo "$VERIFICATION"

if echo "$VERIFICATION" | grep -q "çŠ¶æ€: æˆåŠŸ"; then
    print_success "CUDA æ”¯æŒå·²å¯ç”¨"
else
    print_error "CUDA æ”¯æŒæœªå¯ç”¨"
    exit 1
fi

################################################################################
# 6. ä¿®å¤ numpy ç‰ˆæœ¬
################################################################################

print_header "ä¿®å¤ä¾èµ–ç‰ˆæœ¬"

print_info "å®‰è£…å…¼å®¹çš„ numpy ç‰ˆæœ¬..."
$PIP_CMD install 'numpy<2.0,>=1.20' --force-reinstall -q

print_success "ä¾èµ–ä¿®å¤å®Œæˆ"

################################################################################
# 7. åˆ›å»ºæµ‹è¯•è„šæœ¬
################################################################################

print_header "åˆ›å»ºæµ‹è¯•è„šæœ¬"

cat > /tmp/test_llama_cpp_cuda.py << 'EOF'
#!/usr/bin/env python3
"""
æµ‹è¯• llama-cpp-python CUDA æ”¯æŒ
"""

import sys
import os

print("=" * 60)
print("llama-cpp-python CUDA æµ‹è¯•")
print("=" * 60)

# 1. å¯¼å…¥æ£€æŸ¥
try:
    from llama_cpp import Llama
    print("\nâœ… llama-cpp-python å·²å®‰è£…")
except ImportError as e:
    print(f"\nâŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# 2. æ£€æŸ¥ CUDA åº“
import llama_cpp
lib_path = os.path.join(os.path.dirname(llama_cpp.__file__), 'lib')

print(f"\nåº“è·¯å¾„: {lib_path}")

if os.path.exists(lib_path):
    cuda_libs = [f for f in os.listdir(lib_path) if 'cuda' in f.lower() and f.endswith('.so')]
    
    if cuda_libs:
        print("\nâœ… CUDA åº“:")
        for lib in cuda_libs:
            size = os.path.getsize(os.path.join(lib_path, lib)) / (1024*1024)
            print(f"   - {lib} ({size:.1f} MB)")
    else:
        print("\nâŒ æœªæ‰¾åˆ° CUDA åº“")
        sys.exit(1)
else:
    print("\nâŒ lib ç›®å½•ä¸å­˜åœ¨")
    sys.exit(1)

# 3. æ£€æŸ¥ PyTorch CUDAï¼ˆå¯é€‰ï¼‰
try:
    import torch
    print(f"\nâœ… PyTorch CUDA: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
except ImportError:
    print("\nâš ï¸  PyTorch æœªå®‰è£…ï¼ˆå¯é€‰ï¼‰")

print("\n" + "=" * 60)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 60)
print("\nä¸‹ä¸€æ­¥:")
print("1. åœ¨ ComfyUI ä¸­é€‰æ‹© device: GPU")
print("2. è¿è¡Œ nvidia-smi ç›‘æ§ GPU ä½¿ç”¨")
print("3. äº«å— GPU åŠ é€Ÿï¼")
print("=" * 60)
EOF

chmod +x /tmp/test_llama_cpp_cuda.py

print_success "æµ‹è¯•è„šæœ¬å·²åˆ›å»º: /tmp/test_llama_cpp_cuda.py"

################################################################################
# 8. è¿è¡Œæµ‹è¯•
################################################################################

print_header "è¿è¡Œæµ‹è¯•"

$PYTHON_CMD /tmp/test_llama_cpp_cuda.py

################################################################################
# 9. å®Œæˆ
################################################################################

print_header "å®‰è£…å®Œæˆ"

print_success "llama-cpp-python CUDA æ”¯æŒå·²å®‰è£…"
print_info "ç¯å¢ƒä¿¡æ¯:"
print_info "  Python: $PYTHON_VERSION"
print_info "  CUDA: $CUDA_VERSION"
print_info "  GPU: $GPU_NAME"
print_info "  è®¡ç®—èƒ½åŠ›: $COMPUTE_CAP"

echo ""
print_info "ä½¿ç”¨æ–¹æ³•:"
echo "  1. åœ¨ ComfyUI Text Model Loader ä¸­é€‰æ‹© device: GPU"
echo "  2. è¿è¡Œ 'watch -n 1 nvidia-smi' ç›‘æ§ GPU"
echo "  3. é¢„æœŸæ€§èƒ½æå‡: 20-60å€"

echo ""
print_info "æµ‹è¯•è„šæœ¬: /tmp/test_llama_cpp_cuda.py"
print_info "å®‰è£…æ—¥å¿—: /tmp/llama_cpp_install.log"

echo ""
print_success "ğŸ‰ æ‰€æœ‰è®¾ç½®å®Œæˆï¼"
